### nav-buttons



Math 118 is in many ways an unusual course: it's a blend of linear algebra and multivariable calculus, two topics that usually aren't combined, and the multivariable calculus portion comes from a topic that usually isn't split in such a manner. It's also a terminal course in the math department, and it's used primarily as preparation for higher-level courses in economics and statistics. All this means that the course is a unique perspective on two somewhat distinct areas of math that share one main thing in common: they concern functions of multiple variables. In the first half of this course, we'll explore how the ideas and techniques of calculus I --- specifically optimization --- apply to functions that take more than a single variable as input. In the second half, we'll apply our experience with higher-dimensional space to cover an introduction to linear algebra, exploring how formalizing and structuring methods to solve systems of linear equations results in a surprisingly rich and fully-featured theory that applies to far more than those systems. But before we get started with either of these, let's take our first section to familiarize ourselves with 3D coordinates, a language common to both linear algebra and multivariable calculus.

We're used to functions of a single variable (which we often just call *functions*) of the form $y = f(x)$. They take in a real number $x$ and output a real number $y$, often according to some rule like $f(x) = x^2$. When we graph functions like these, we plot pairs $(x, y)$, with one $y$-coordinate for every $x$-coordinate we can plug in.

Plenty of real-world functions take in more than one input though --- a fluid has viscosity that is a function of both temperature and pressure, a company producing multiple products that require overlapping resources has profit that is a function of the amounts of each product produced, and maybe most directly, height above sea level is a function of latitude and longitude.

We could define a brand-new notion of a function of two or more variables and proceed to redevelop many of the same results we already know to be true about single-variable functions, but there's a better way, and a good lesson to learn about math more broadly: whenever reasonable, it pays to leverage notions and language that's already established rather than making something new from scratch. To that end, let's think of a function that takes two inputs in a slightly different way: as a function that takes in a *single* pair of real numbers $(x, y)$ and outputs a real number $z$. That way, most of what we already know about single-variable functions will carry over to these new ones.

To distinguish and denote these functions, though, we're in desperate need of notation and language! Let's define some terms so that it's easier to talk about these functions and others.

### def "the real numbers"

	The set of all real numbers (i.e. all decimals, both rational and irrational) is denoted $#R#$, and the set of ordered lists of $n$ real numbers is denoted $#R#^n$ (read "R-n").

###

For example, the Cartesian plane is the set of all ordered pairs of real numbers, and so we denote it $#R#^2$. This is where graphs of single-variable functions live, and it's also the domain of functions of two variables!

### def "domain and codomain"

	Let $f$ be a function that takes inputs from a set $A$ and produces outputs in a set $B$. We say that $A$ is the **domain** of $f$ and that $B$ is the **codomain** of $f$, and we write $f : A \to B$.

###

The codomain of a function is distinct from its **range**, which is the set of all outputs of the function that actually occur. For example, the function $f : #R# \to #R#$ defined by $f(x) = 2x^4$ has a codomain of $#R#$ as it's written (which just means its outputs are real numbers), but it has a range of $[0, \infty)$, since all real number inputs to $f$ produce positive outputs.

Our functions of interest, particularly in the second half of the course, will be functions $f : #R#^n \to #R#$: they take in a list of $n$ real numbers and produce a single one. While we could write something like $f((x, y)) = x + y$ to indicate that a function $f$ takes in a pair $(x, y)$ in $#R#^2$ and sends it to $x + y$, we'll usually omit the awkward double parentheses and just write $f(x, y) = x + y$.



## Geometry in $#R#^3$

Let's start by getting our bearings in $#R#^3$ specifically --- while we'll mostly be working in $#R#^n$ for general $n$, we'll develop much of our intuition in $#R#^3$, since that's the highest space we can reasonably picture (we live in a $3$-dimensional universe, after all), and nearly everything true about $#R#^3$ that's of use to us generalizes directly to every $#R#^n$.

Let's start getting our bearings in $#R#^3$ by taking equations whose graphs we're already familiar with in $#R#^2$ and plotting them in 3D. Open the Desmos sidebar in the graph below and check the Extend to 3D checkbox for each equation in turn. You can also <span class="click-tap"><span>click</span><span>tap</span></span> the colored circles next to the equations to hide and show them.

### desmos extendTo3d

The results are similar --- plotting a 2D function in 3D results in an object like a paper sheet, potentially warped a bit due to the function. Fundamentally, this is because plotting a graph of any equation just means drawing all of the points where that equation is true, and any equation of the form $y = f(x)$ doesn't involve $z$, so any point $(x, y)$ where $y = f(x)$ will result in us plotting a whole column of points $(x, y, z)$ for *all* values of $z$.

The plane defined by $x = 0$ is the unique plane that contains the $y$- and $z$-axes, and we call it the **$\mathbf{yz}$-plane**. Similarly, we call $y = 0$ the **$\mathbf{xz}$-plane**, and $z = 0$ the **$\mathbf{xy}$-plane** --- which is fittingly enough the term we already use for $#R#$ when we're using it to graph things! Collectively, we call these three planes the **coordinate planes**.

### exc "graphing in $#R#^3$"

	Sketch a graph of $y = x^2$ in $#R#^3$.

	### solution

	Like the previous graphs, this one looks like the graph of $y = x^2$, but extended vertically parallel to the $z$-axis.

	### desmos extendTo3d2

###

One of the learning objectives for this course is being able to sketch basic 3D graphs, but don't worry if that seems like a daunting task --- I'm not here to be overly picky about perspective and detail!

We'll talk more about how to find the equations of more general planes in the next section, but for now, let's turn to something a little bit simpler: distance. We can compute the distance between any two points $(x_1, y_1)$ and $(x_2, y_2)$ in $#R#^2$ by using the Pythagorean theorem: it's

$$
	d((x_1, y_1), (x_2, y_2)) = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}.
$$

### desmos distance

Once again, instead of trying to reprove something like the Pythagorean theorem in 3D, we're best off just using it verbatim to extend the distance result to $#R#^3$. To travel from the blue point $(x_1, y_1, z_1)$ in the graph above to the red point $(x_2, y_2, z_2)$, we can first find the distance to the purple point $(x_2, y_2, z_1)$. That's contained in the plane $z = z_1$, and so the distance formula from $#R#^2$ applies: the distance is

$$
	d' = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}.
$$

Now the remainder of the distance is the vertical line from the purple point to the red one, and that distance is just $|z_2 - z_1|$. The resulting gray triangle that we form is a right triangle, and the Pythagorean theorem tells us that its hypotenuse has length

$$
	d = \sqrt{d^2 + |z_2 - z_1|^2} = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2 + (z_2 - z_1)^2}.
$$

### exc "spheres and cylinders"

	What is an equation for a sphere of radius $1$ centered at the origin (i.e. $(0, 0, 0)$) in $#R#^3$? What about a sphere of radius $r$ centered at $(a, b, c)$? How about a vertical cylinder of radius $1$ parallel to the $z$-axis whose center axis contains the origin? Hint: for all of these, start with an equation for the unit circle in $#R#^2$.

	### solution

	Just like the unit circle in $#R#^2$ is given by the equation $x^2 + y^2 = 1$, or in other words the set of all points whose distance from $(0, 0)$ is equal to $1$, the unit sphere in $#R#^3$ is given by the set of all points whose distance to the origin is $1$. Using the distance formula we just saw, that means we want $\sqrt{x^2 + y^2 + z^2} = 1$, so $x^2 + y^2 + z^2 = 1^2$. Extending that idea to a sphere centered at $(a, b, c)$ and with radius $r$ means applying the distance formula again: this time,

	$$
		(x - a)^2 + (y - b)^2 + (z - c)^2 = r^2.
	$$

	For the cylinder, there are a few different approaches. We could treat it as the set of points in $#R#^3$ whose distance to the *$z$-axis* is $1$, meaning it's defined by the equation $x^2 + y^2 = 1$, or alternatively we could think of it as the extension of the unit circle in the $xy$-plane to $#R#^3$ --- also $x^2 + y^2 = 1$.

	### desmos sphereAndCylinder

###



## Intro to Vectors

A natural way to think about points in $#R#^2$ or $#R#^3$ is as locations: a point $p = (a, b, c)$ specifies a specific location in 3D space. But most physical applications of math, particularly physics, often need a closely related concept: that of a *change* in location. Objects that represent a change in location --- called **vectors** --- are similar but in many ways more versatile than points, and we'll have a *lot* to say about them throughout the semester! Let's get started with a definition and some examples.

### def "vector"

	A **vector** is a quantity that represents a difference in location (which we sometimes call **displacement**) between two points in space; equivalently, it's a quantity with magnitude (the distance between the two points) and direction (the direction of travel from the starting point to the ending point). Notably, only the *difference* in location matters --- the actual start and end points are irrelevant. We typically indicate vectors with a small arrow over a variable name, as in $\vec{v}$.

###

For example, the displacement $\vec{v}$ from Los Angeles to Seattle is a vector, but that vector doesn't have anything to do with Los Angeles or Seattle specifically. In fact, $\vec{v}$ is *also* equal to the displacement from Seattle to somewhere in the Yukon Territories (when treating those locations as being on a 2D map, at least).

The typical way to represent vectors is with arrows, which indicate both their magnitude and direction. As we just saw, though, the tail of the arrow doesn't affect the value of the vector, so in the below example, dragging the red point never changes the vector; only dragging the blue point does.

### desmos vectors

While vectors are fundamentally different from points, there's definitely a close similarity between them. In that previous graph, every position of the blue point corresponds to a vector, and vice versa. Specifically, the vector can be determined either by a magnitude and direction, or by an amount to move parallel to the $x$-axis, then an amount to move parallel to the $y$-axis (the green dashed lines in the graph). In fact, if we place the red point at the origin, then the blue point's coordinates *tell* us those amounts. This is very often an easier way to work with vectors, especially when we need a more algebraic presentation of them, so let's write down some notation for it.

### def "components of a vector and the zero vector"

	Let $\vec{v}$ be a vector in $#R#^2$. To write $\vec{v}$ in **component notation**, we denote it $\vec{v} = \left< v_1, v_2 \right>$, where $v_1$ and $v_2$ are the real numbers indicating how much $\vec{v}$ moves parallel to the $x$- and $y$-axes, respectively. Similarly, a vector $\vec{w}$ in $#R#^3$ can be written in component notation as $\vec{w} = \left< w_1, w_2, w_3 \right>$, where $w_1$, $w_2$, and $w_3$ are the amounts $\vec{w}$ moves parallel to the $x$-, $y$-, and $z$-axes.

	We also define the **zero vector** to be the vector with zero magnitude, which is unique as long as we know what dimension of space we're working in. In practice, that's clear from context, and so we use the same symbol --- $\vec{0}$ --- to represent it, regardless of the space we're in. In $#R#^2$, $\vec{0} = \left< 0, 0 \right>$, in $#R#^3$, $\vec{0} = \left< 0, 0, 0 \right>$, and so on.

###

Most of our vectors will be in $#R#^2$ and $#R#^3$, but we can just well write $\left< 2 \right>$ for the vector with length $2$ pointing in the positive direction in $#R#$, or $\left< -1, -2, -3, -4 \right>$ for a vector in $#R#^4$ that we can't very well visualize. To avoid our notation becoming clunky and obscuring what we're trying to communicate, we'll stick with $#R#^3$ specifically when presenting most of the results and terminology about vectors, including for this next one.

### def "magnitude of a vector and unit vectors"

	Let $\vec{v} = \left< v_1, v_2, v_3 \right>$ be a vector. The **magnitude** of $\vec{v}$ is

	$$
		\left| \left| \vec{v} \right| \right| = \sqrt{v_1^2 + v_2^2 + v_3^2}.
	$$

	In other words, it's the length of $\vec{v}$ when it's drawn as an arrow. A **unit vector** is a vector $\vec{u}$ with $\left| \left| \vec{u} \right| \right| = 1$.

###

Since vectors are quantities with magnitude and direction, we naturally would want notation for the direction of a vector, too. However, no such standard notation exists. While we can just use a standard angle measure counterclockwise from the positive $x$-axis in $#R#^2$, we then need a combination of angles in $#R#^3$, akin to latitude and longitude. To be clear, such a system exists, and this is a solvable problem in any $#R#^n$. However, it ends up not being useful enough to justify its formalization, at least not at the very start of learning about vectors.

### exc "vectors"
	
	1. Sketch the vectors $\vec{v} = \left< 1, -2 \right>$ and $\vec{w} = \left< 1, -2, 3 \right>$ and find $\left| \left| \vec{v} \right| \right|$ and $\left| \left| \vec{w} \right| \right|$.

	2. What does the set of all unit vectors look like in $#R#^2$? What about in $#R#^3$?

	3. How many unit vectors are there with a first component equal to $\frac{1}{2}$ in $#R#^2$? What about in $#R#^3$?

	### solution

	1. We can draw $\vec{v}$ anywhere in $#R#^2$, as long as it's pointing one unit to the right and $2$ units down --- all of the following vectors are equal to $\vec{v}$!

	### desmos equalVectors

	We can do the same thing with $\vec{w}$, but since 3D graphs get crowded more easily than 2D ones, let's just draw $\vec{w}$ starting at the origin.

	### desmos threeDVector

	The magnitudes are $\left| \left| \vec{v} \right| \right| = \sqrt{1^2 + (-2)^2} = 5$ and $\left| \left| \vec{w} \right| \right| = \sqrt{14}$, and with them both drawn out, we can confirm that that matches our intuition.

	2. The set of all unit vectors in $#R#^2$ is the set of all $\left< v_1, v_2 \right>$ with $\sqrt{v_1^2 + v_2^2} = 1$, so if we draw all the vectors starting from the origin, then it's a circle! Similarly, the set of all unit vectors in $#R#^3$ is a sphere.

	3. If the first component is equal to $\frac{1}{2}$ in $#R#^2$, then we can solve

	$$
		\sqrt{\left( \frac{1}{2} \right)^2 + v_2^2} &= 1

		\frac{1}{4} + v_2^2 &= 1

		v_2 &= \pm \sqrt{\frac{3}{4}}

		&= \pm \frac{\sqrt{3}}{2}.
	$$

	Therefore, the only two vectors are $\left< \frac{1}{2}, \frac{\sqrt{3}}{2} \right>$ and $\left< \frac{1}{2}, -\frac{\sqrt{3}}{2} \right>$.

	### desmos unitVectors2d

	In $#R#^3$, that same equation results in

	$$
		\frac{1}{4} + v_2^2 + v_3^2 &= 1

		v_2^2 + v_3^2 &= \frac{3}{4},
	$$

	which is a little more complicated: it means that the **2D** vector $\left< v_2, v_3 \right>$ has magnitude $\frac{\sqrt{3}}{2}$. If we still center all our vectors at the origin, then these vectors appear as a circle on the unit sphere at $x = \frac{1}{2}$. We can also think of it as the intersection of the plane $x = \frac{1}{2}$ with the unit sphere!

	### desmos unitVectors3d

###

Going back to our flight example, the notion of vectors as displacements works nicely if we want to add them together: the displacement from Los Angeles to Seattle *combined* with the displacement from Seattle to New York should equal the displacement from Los Angeles to New York. This is exactly the notion of adding vectors geometrically: we place the vectors so that first one ends where the second one starts, and then draw the sum of the two as the vector that stretches from the start of the first to the end of the second.

### desmos vectorAddition

Here, the sum of the red and blue vectors is the purple one --- and notice that we can add the vectors in either order to get the same result! That's a concise visual justification that **vector addition is commutative**.

Vector addition also plays nicely with components! If $\vec{v} = \left< v_1, v_2, v_3 \right>$ and $\vec{w} = \left< w_1, w_2, w_3 \right>$, then the combination $\vec{v} + \vec{w}$ moves a total of $v_1 + w_1$ in the $x$-direction and so on, meaning

$$
	\vec{v} + \vec{w} = \left< v_1 + w_1, v_2 + w_2, v_3 + w_3 \right>.
$$

With the knowledge of how to add vectors, we can also make sense of **scalar multiplication**: how to multiply a vector by a constant number (i.e. a **scalar**, which here means not a vector). For example, any vector $\vec{v}$ should satisfy $\vec{v} + \vec{v} = 2\vec{v}$, where $2\vec{v}$ is an operation we'll need to define. But $\vec{v} + \vec{v}$ is just the result of placing $\vec{v}$ at the end of itself, meaning it's a vector in the same direction as $\vec{v}$, but twice as long. More generally, we can define scalar multiplication as follows:

### def "scalar multiplication of vectors"

	Let $\vec{v}$ be a vector and let $c > 0$ be a real number. Then $c\vec{v}$ is the vector in the same direction as $\vec{v}$, but $c$ times as long --- that is, $\left| \left| c\vec{v} \right| \right| = c \cdot \left| \left| \vec{v} \right| \right|$.

	If $c < 0$, then $c\vec{v}$ is the vector in the **opposite** direction as $\vec{v}$, and $|c|$ times as long. Finally, if $c = 0$, then $c\vec{v} = 0\vec{v} = \vec{0}$. In general, $\left| \left| c\vec{v} \right| \right| = |c| \cdot \left| \left| \vec{v} \right| \right|$.

###

### exc "finding unit vectors"

	Let $\vec{v}$ be any vector other than the zero vector. What is a formula in terms of $\vec{v}$ for a *unit vector* in the same direction as $\vec{v}$? Why doesn't this work for the zero vector?

	### solution

	The vectors in the same direction as $\vec{v}$ are the vectors $c\vec{v}$, so if we want one that's a unit vector, we need $\left| \left| c\vec{v} \right| \right| = |c| \cdot \left| \left| \vec{v} \right| \right| = 1$, so $|c| = \frac{1}{\left| \left| \vec{v} \right| \right|}$. Therefore, $c = \pm \frac{1}{\left| \left| \vec{v} \right| \right|}$, and the negative version would point in the opposite direction as $\vec{v}$, so we want the positive one. Since the zero vector has zero magnitude, we'd be dividing by zero! This is the algebraic reflection of the zero vector not having a direction.

###

While it might seem like vector multiplication is just a definition away, there are unfortunately unsolvable difficulties with trying to define $\vec{v}\vec{w}$ by multiplying corresponding components. For one, any vector with a zero in any component wouldn't have a multiplicative inverse. We'll look into alternatives that will sort of let us multiply vectors in the next section, but for now, we're going to leave vector multiplication completely undefined.

Let's take a moment to summarize some properties of vector addition and scalar multiplication, and then we'll define some useful terminology.

### prop "properties of vector arithmetic"

	For any vectors $\vec{u}$, $\vec{v}$ and $\vec{w}$,

	1. $\vec{u} + \left( \vec{v} + \vec{w} \right) = \left(  \vec{u} + \vec{v} \right) + \vec{w}$ (i.e. vector addition is associative).

	2. $\vec{v} + \vec{w} = \vec{w} + \vec{v}$ (i.e. vector addition is commutative).

	3. $\vec{v} + \vec{0} = \vec{v}$.

	4. $0\vec{v} = \vec{0}$.

	5. $\vec{v} - \vec{v} = \vec{0}$.

	6. $c\left( \vec{v} + \vec{w} \right) = c\vec{v} + c\vec{w}$ and $(c + d)\vec{v} = c\vec{v} + d\vec{v}$ for any numbers $c$ and $d$ (i.e. scalar multiplication of vectors is distributive).

	7. $\left| \left| c\vec{v} \right| \right| = |c| \cdot \left| \left| \vec{v} \right| \right|$ for any number $c$.

	8. The only vector $\vec{v}$ with $\left| \left| \vec{v} \right| \right| = 0$ is $\vec{v} = \vec{0}$.

###

When we a vector $\vec{v}$ in $#R#^2$ as $\left< v_1, v_2 \right>$, we can think of it as the sum $\vec{v} = \left< v_1, 0 \right> + \left< 0, v_2 \right>$, or equivalently, $\vec{v} = v_1 \left< 1, 0 \right> + v_2\left< 0, 1 \right>$. That decomposition will prove useful, and we'll give those two vectors names to better handle them.

### def "standard basis vectors"

	In $#R#^2$, we define $\vec{i} = \left< 1, 0 \right>$ and $\vec{j} = \left< 0, 1 \right>$, and in $#R#^3$, $\vec{i} = \left< 1, 0, 0 \right>$, $\vec{j} = \left< 0, 1, 0 \right>$, and $\vec{k} = \left< 0, 0, 1 \right>$.

###

We can see the decomposition in a more geometric way now, as a sequence of many vector additions:

### desmos componentVectors

That's it for our first section! In the next one, we'll discuss ways that we can combine vectors geometrically, via two different types of multiplication.



### nav-buttons