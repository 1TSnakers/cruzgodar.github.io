<head>
	<title>Section 9: Taylor Series</title>
	
	<meta property="og:type" content="website">
	
	<meta property="og:title" content="Section 9: Taylor Series"/>
	
	<meta property="og:url" content="https://cruzgodar.com/teaching/uo/253/notes/9-taylor-series/">

	<meta property="og:image" content="https://cruzgodar.com/teaching/uo/253/notes/9-taylor-series/cover.jpg"/>
	
	<style>body {opacity: 0;}</style>
</head>

<div style="height: 5vh"></div>

<noscript>
	<p class="body-text" style="text-align: center">JavaScript is required to use this site and many others. Consider enabling it.</p>
</noscript>



!header Section 9: Taylor Series

!nav-buttons



!begin-text-block

	By now, we&#x2019;ve built up a robust theory of series. We know when and where they converge, and in a few special cases, we know what they converge to. In this section, we&#x2019;ll extend those few special cases to almost every power series we&#x2019;ve seen, and by the end, we&#x2019;ll be able to pass from functions to power series and back almost effortlessly.
	
	Let&#x2019;s return to an old friend: $\frac{1}{1 - x}$. We already know that when $|x| < 1$, it&#x2019;s equal to $\sum_{n = 0}^\infty x^n$, but the way we found that was by directly taking a limit of partial sums and expressing $1 + x + x^2 + \cdots + x^k$ as $\frac{1 - x^{k + 1}}{1 - x}$. That method was specific to this particular series and doesn&#x2019;t really help with any others. Instead, let&#x2019;s try to find series like that in general, without any fancy tricks.
	
	Take a power series $\sum_{n = 0}^\infty c_n (x - a)^n$ centered at $x = a$, and suppose that the series converges to a function $f(x)$ on some interval $(a - R, a + R)$. With the logic we used at the end of the previous section, we can extract the derivatives of $f$ from the coefficients of the series. Specifically, we have
	
	$$
		\begin{align*}
			f(a) &= c_0\\
			f'(a) &= c_1\\
			f''(a) &= 2 \cdot c_2\\
			f'''(a) &= 3 \cdot 2 \cdot c_3\\
			f''''(a) &= 4 \cdot 3 \cdot 2 \cdot c_4\\
			& \ \ \vdots\\
			f^{(n)}(a) &= n!c_n.
		\end{align*}
	$$
	
	From this perspective, if we know a power series representation of a function $f$, then we can find any derivative of $f$ from the coefficients. But we really want to go the other way: we want to <em>find</em> series representations from a function. To get at that direction, let&#x2019;s solve all of those equations for the $c_n$:
	
	$$
		\begin{align*}
			c_0 &= \frac{f^{(0)}(a)}{0!} = f(a)\\
			c_1 &= \frac{f^{(1)}(a)}{1!} = f'(a)\\
			c_2 &= \frac{f^{(2)}(a)}{2!} = \frac{f''(a)}{2}\\
			c_3 &= \frac{f^{(3)}(a)}{3!} = \frac{f'''(a)}{3!}\\
			c_0 &= \frac{f^{(4)}(a)}{4!}\\
			& \ \ \vdots\\
			c_n &= \frac{f^{(n)}(a)}{n!}.
		\end{align*}
	$$
	
	What we&#x2019;ve come to is the idea that underpins the entire section, and to a large extent the course: if $f$ has a power series representation centered at $x = a$, then there is only one possibility for what it can be, since power series representations are unique, and we&#x2019;ve just written down what that one possibility is. We&#x2019;ll be talking quite a lot about these things, and they deserve their own name.

!end-text-block



!begin-def

	Taylor series
	
	Let $f(x)$ be a function that is infinitely differentiable. The <dfn>Taylor series</dfn> expansion of $f$ centered at $a$ is the power series
	
	$$
		\sum_{n = 0}^\infty \frac{f^{(n)}}{n!} (x - a)^n.
	$$
	
	A Taylor series centered at $0$ is called a <dfn>Maclaurin series</dfn>. The partial sums of a Taylor or Maclaurin series are called the <dfn>Taylor</dfn> or <dfn>Maclaurin polynomials</dfn> for $f$ centered at $a$.

!end-def



!begin-text-block

	Critically, this definition says nothing about convergence: the Taylor series for a function </strong>does not necessarily converge to that function</strong>. We&#x2019;ll see examples of this a little later, but the takeaway is that we need to verify convergence to show that a Taylor series actually does what it&#x2019;s supposed to.

!end-text-block



!begin-example

	finding a Maclaurin series
	
	Find the Maclaurin series for $\sin(x)$ and determine its interval of convergence. Find the first six Maclaurin polynomials.
	
	The Maclaurin series is just the Taylor series centered at $0$, so it is $\sum_{n = 0}^\infty c_n x^n$, where
	
	$$
		c_n = \frac{1}{n!} \frac{d^n}{dx^n} [\sin(x)]|_{x = 0}.
	$$
	
	Okay, so what is that massive derivative? Let&#x2019;s list out a few derivatives of $\sin(x)$ to remind ourselves of the pattern.
	
	$$
		\begin{align*}
			\frac{d}{dx}[\sin(x)] &= \cos(x)\\
			\frac{d^2}{dx^2}[\sin(x)] &= -\sin(x)\\
			\frac{d^3}{dx^3}[\sin(x)] &= -\cos(x)\\
			\frac{d^4}{dx^4}[\sin(x)] &= \sin(x).
		\end{align*}
	$$	
	
	When we plug in $x = 0$, $\sin(0) = 0$ while $\cos(0) = 1$. All of the even derivatives have a $\sin$ in them, so they all vanish. The remaining terms alternate in sign as the derivative flips from $\cos(x)$ to $-\cos(x)$. In total, we wind up with
	
	$$
		x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \cdots.
	$$
	
	It&#x2019;s pretty awkward to write those coefficients as a series if we try to include the missing terms. Instead, let&#x2019;s write this as an alternating series by changing the power on $x$:
	
	$$
		\sum_{n = 0}^\infty (-1)^n \frac{x^{2n + 1}}{(2n + 1)!}.
	$$
	
	The $2n + 1$ is just a nice way of expressing all the odd numbers.
	
	Next, let&#x2019;s determine where this series converges. The ratio test is often the best bet for Taylor series, since they&#x2019;re full of factorials and exponents. Evaluating it on this series, we have
	
	$$
		\begin{align*}
			\lim_{n \to \infty} \left| \frac{(-1)^{n + 1} \frac{x^{2(n + 1) + 1}}{(2(n + 1) + 1)!}}{(-1)^n \frac{x^{2n + 1}}{(2n + 1)!}} \right| &= \lim_{n \to \infty} \left| -\frac{x^{2(n + 1) + 1}(2n + 1)!}{x^{2n + 1}(2(n + 1) + 1)!} \right|\\
			&= \lim_{n \to \infty} \left| \frac{x^{2n + 3}(2n + 1)!}{x^{2n + 1}(2n + 3)!} \right|\\
			&= \lim_{n \to \infty} \left| \frac{x^2}{(2n + 3)(2n + 2)} \right|\\
			&= \lim_{n \to \infty} \left| \frac{x^2}{(2n + 3)(2n + 2)} \right|\\
			&= 0.
		\end{align*}
	$$
	
	Therefore, the series converges everywhere.
	
	The Maclaurin polynomials are just partial sums of this series. Denoting the first six polynomials as $p_0, p_1, ..., p_5$, every $p_n$ should have a highest term of $x^n$. Therefore, we have
	
	$$
		\begin{align*}
			p_0 &= 0\\
			p_1 &= x\\
			p_2 &= x\\
			p_3 &= x - \frac{x^3}{3!}\\
			p_4 &= x - \frac{x^3}{3!}\\
			p_5 &= x - \frac{x^3}{3!} + \frac{x^5}{5!}.
		\end{align*}
	$$
	
	Plotting these polynomials shows that they really do seem to converge to $\sin$: plotted in green is $p_N$, and increasing $N$ increases the interval where $p_N$ is almost identical to $\sin$. You can also change $a$ to see the effect recentering has.
	
	!iframe https://www.desmos.com/calculator/dtuvs1pfgn

!end-example



!begin-exercise

	finding Taylor series
	
	Find the following Taylor series. For each, determine the interval of convergence and find the first three Taylor polynomials.
	
	1. A Taylor series for $\ln(x)$ centered at $x = 1$.
	
	2. A Taylor series for $x^2 + 4x + 4$ centered at $x = -2$.
	
	3. A Maclaurin series for $e^x$.

!end-exercise



!section

!text s Convergence of Taylor Series

<br>



!begin-text-block

	Now that we have a candidate for general series representations, the natural next thing to figure out is when they converge to the functions they&#x2019;re derived from. Just as we measured our approximation of series with remainders, we&#x2019;ll measure a Taylor series&#x2019;s approximation of a function with a <dfn>remainder functions</dfn>. Given a Taylor series
	
	$$
		\sum_{n = 0}^\infty \frac{f^{(n)}}{n!} (x - a)^n
	$$
	
	centered at $a$, let&#x2019;s define the $N$th remainder function as
	
	$$
		R_N(x) = f(x) - \sum_{n = 0}^N \frac{f^{(n)}}{n!} (x - a)^n.
	$$
	
	In order for the Taylor series to approximate $f$, we need $\lim_{N \to \infty} R_N(x) = 0$, and to know when that&#x2019;s true, we have to find a formula for $R_N(x)$. Figuring that formula out takes some effort, but the result will be simple enough in the end. A critical part will be the use of the Mean Value theorem &mdash; let&#x2019;s state that now to remind ourselves how it works.

!end-text-block



!begin-thm

	The Mean Value Theorem
	
	Let $f$ be a function differentiable on an open interval $(a, b)$. Then there is a point $c$ with $a < c < b$ with
	
	$$
		f'(c) = \frac{f(b) - f(a)}{b - a}.
	$$

!end-thm



!begin-text-block

	The idea behind the MVT is that the fraction on the right is the slope of the line between $(a, f(a))$ and $(b, f(b))$, and there is no way to draw a fucntion between those points without a tangent line somewhere having the same slope as this secant line. We&#x2019;ll be looking at a case where $f(a) = f(b)$, so the conclusion will be that the derivative must be zero somewhere in between.
	
	The function we&#x2019;ll be applying the MVT to is somewhat complicated. Fix a value of $x \neq a$ and define a function $g$ by
	
	$$
		g(t) = f(x) - \left( \sum_{n = 0}^N \left( \frac{f^{(n)}(t)}{n!} (x - t)^n \right) + R_N(x) \frac{(x - t)^{N + 1}}{(x - a)^{N + 1}} \right).
	$$
	
	This is a huge function, but it&#x2019;s not quite as complicated as it might seem. Inside the parentheses on the right is the start of a Taylor series for $f$ centered at $t$. If we added on its remainder, we&#x2019;d have a complete Taylor series, but instead we add on $R_N$, which is the remainder of the series centered at $a$, and we multiply it by $\frac{(x - t)^{n + 1}}{(x - a)^{n + 1}}$, which will help us a little later.
	
	There&#x2019;s unfortunately not that much motivation for why $g$ is defined this way, and the ends will have to justify the means. First of all, since $f$ is infinitely differentiable, so is $g$ &mdash; it&#x2019;s just a bunch of derivatives of $f$ and polynomials in $t$. Also, we know that $g$ is zero for $t = x$ and $t = a$:
	
	$$
		\begin{align*}
			g(x) &= f(x) - \left( \sum_{n = 0}^N \left( \frac{f^{(n)}(x)}{n!} (x - x)^n \right) + R_N(x) \frac{(x - x)^{N + 1}}{(x - a)^{N + 1}} \right).
			&= f(x) - \frac{f^{(0)}(x)}{0!}\\
			&= f(x) - f(x)\\
			&= 0.
		\end{align*}
	$$
	
	$$
		\begin{align*}
			g(a) &= f(x) - \left( \sum_{n = 0}^N \left( \frac{f^{(n)}(a)}{n!} (x - a)^n \right) + R_N(x) \frac{(x - a)^{N + 1}}{(x - a)^{N + 1}} \right).
			&= f(x) - \left( \sum_{n = 0}^N \left( \frac{f^{(n)}(a)}{n!} (x - a)^n \right) + R_N(x) \right)\\
			&= f(x) - \sum_{n = 0}^N \left( \frac{f^{(n)}(a)}{n!} (x - a)^n \right) - R_N(x)\\
			&= R_N(x) - R_N(x)\\
			&= 0.
		\end{align*}
	$$
	
	In this sense, $g$ is measuring how well a Taylor series centered at $t$ approximates $f$, except we staple on the remainder from a Taylor series centered at $a$ no matter what $t$ is.
	
	By the Mean Value theorem, there must be a point $c$ in between $x$ and $a$ where $g'(c) = 0$. So what is $g'(c)$? Let&#x2019;s differentiate and find out. Remember, we&#x2019;re differentiating with respect to $t$, not $x$, so terms with just $x$ are treated as constants.
	
	$$
		\begin{align*}
			g'(t) &= \frac{d}{dt} \left[ f(x) - \left( f(t) + \sum_{n = 1}^N \left( \frac{f^{(n)}(t)}{n!} (x - t)^n \right) + R_N(x) \frac{(x - t)^{N + 1}}{(x - a)^{N + 1}} \right) \right]\\
			&= -f'(t) - \sum_{n = 1}^N \left( \frac{f^{(n + 1)}(t)}{n!} (x - t)^n + \frac{f^{(n)}(t)}{n!} n(x - t)^{n-1}(-1) \right) - R_N(x) \frac{(N + 1)(x - t)^N(-1)}{(x - a)^{N + 1}}\\
			&= -f'(t) - \sum_{n = 1}^N \left( \frac{f^{(n + 1)}(t)}{n!} (x - t)^n - \frac{f^{(n)}(t)}{(n-1)!} (x - t)^{n-1} \right) + R_N(x) \frac{(N + 1)(x - t)^N}{(x - a)^{N + 1}}.
		\end{align*}
	$$
	
	Incredibly, that sum telescopes! The $n = 1$ term survives on the right, and the $n = N$ survives on the left. In total, we have
	
	$$
		\begin{align*}
			g'(t) &= -f'(t) - \left( \frac{f^{(N + 1)}(t)}{N!} (x - t)^N - \frac{f'(t)}{(0)!} (x - t)^{0} \right) + R_N(x) \frac{(N + 1)(x - t)^N}{(x - a)^{N + 1}}\\
			&= -f'(t) + f'(t) - \frac{f^{(N + 1)}(t)}{N!} (x - t)^N + R_N(x) \frac{(N + 1)(x - t)^N}{(x - a)^{N + 1}}\\
			&= - \frac{f^{(N + 1)}(t)}{N!} (x - t)^N + R_N(x) \frac{(N + 1)(x - t)^N}{(x - a)^{N + 1}}.
		\end{align*}
	$$
	
	Since $g'(c) = 0$, we have that for some $c$,
	
	$$
		0 = - \frac{f^{(N + 1)}(c)}{N!} (x - c)^N + R_N(x) \frac{(N + 1)(x - c)^N}{(x - a)^{N + 1}}.
	$$
	
	Now we can solve for $R_N(x)$ to find that
	
	$$
		\begin{align*}
			R_N(x) &= \frac{\frac{f^{(N + 1)}(c)}{N!} (x - c)^N}{\frac{(N + 1)(x - c)^N}{(x - a)^{N + 1}}}\\
			&= \frac{f^{(N + 1)}(c)}{N!} (x - c)^N \frac{(x - a)^{N + 1}}{(N + 1)(x - c)^N}\\
			&= \frac{f^{(N + 1)}(c)}{N!(N + 1)} (x - a)^{N + 1}\\
			&= \frac{f^{(N + 1)}(c)}{(N + 1)!} (x - a)^{N + 1}.
		\end{align*}
	$$
	
	Specifically, if $f^{(N + 1)}$ is a bounded function, then that bound will carry over to the remainder. This is the core of the main result about the remainders of Taylor series, which we are <em>finally</em> ready to state.

!end-text-block



!begin-thm
	
	Taylor&#x2019;s Theorem
	
	Let $f$ be a function that is differentiable $N + 1$ times on some interval $I$, and let $a$ be a number in $I$. Let $R_N(x)$ be the $N$th remainder function of the Taylor series for $f$ centered at $a$. Suppose further that $f^{(N + 1)}$ is bounded on $I$: in other words, there is some number $M$ with
	
	$$
		\left| f^{(N + 1)}(x) \right| \leq M
	$$
	
	for all $x$ in $I$. Then
	
	$$
		\left| R_N(x) \right| \leq \frac{M}{(N + 1)!}|x - a|^{N + 1}.
	$$
	
!end-thm



!begin-text-block

	That was a lot of very dense work leading up to a pretty dense result &mdash; let&#x2019;s take a second to unpack how we&#x2019;ll actually be using it in practice. When we have a Taylor series for a function $f(x)$, we can determine how good of an approximation the $N$th Taylor polynomial is by <strong>effectively only looking at the next term</strong>. For example, the quality of the approximation of a Taylor polynomial that ends at the $n = 3$ term of the Taylor series is determined only by the behavior of the $n = 4$ term. To make it work, we need to be able to bound that term, which should make some intuitive sense. Polynomials, whose derivatives are eventually zero, will easily be bounded after enough derivatives, which indicates that Taylor polynomials are very good approximations. On the other hand, functions like $e^x$, whose derivatives never settle to zero, will never be bounded unless we restrict to a small interval of $x$-values, which tells us that Taylor polynomials don&#x2019;t approximate $e^x$ quite so well.
	
	If this section has been throwing you for a bit of a loop so far, now&#x2019;s the time to jump back in. We&#x2019;ll return to our example of $\sin$ to determine the remainders for each of the Taylor polynomials.

!end-text-block



!begin-example

	Taylor&#x2019;s Theorem
	
	Determine a bound for the $N$th remainder function of the Maclaurin series for $\sin(x)$. Using a Maclaurin polynomial of degree $5$, estimate $\sin(1)$ and determine the maximum error.
	
	First, remember that the Maclaurin series for $\sin(x)$ is
	
	$$
		\sum_{n = 0}^\infty (-1)^n \frac{x^{2n + 1}}{(2n + 1)!},
	$$
	
	and in order to determine the remainder, we&#x2019;ll need to bound $\frac{d^{n+1}}{dx^{n+1}} [\sin(x)]$. Every derivative of $\sin(x)$ is either $\pm \sin(x)$ or $\pm \cos(x)$, and so
	
	$$
		\left| \frac{d^{n+1}}{dx^{n+1}} [\sin(x)] \right| \leq 1
	$$
	
	for all $x$. Plugging this into Taylor&#x2019;s remainder formula, we find that
	
	$$
		\begin{align*}
			\left| R_N(x) \right| &\leq \frac{1}{(N + 1)!}|x - a|^{N + 1}\\
			&= \frac{|x|^{N + 1}}{(N + 1)!}.
		\end{align*}
	$$
	
	For example, if we&#x2019;d like to know how well $p_5(x)$ approximates $\sin\left( \frac{\pi}{2} \right)$, we evaluate
	
	$$
		\begin{align*}
			\left| R_5\left( \frac{\pi}{2} \right) \right| &\leq \frac{\left| \frac{\pi}{2} \right|^6}{6!}\\
			&\approx .021.
		\end{align*}
	$$
	
	Therefore, the approximation is off by at most $.021$ in either direction. This is the upper bound &mdash; if we actually evaluate $p_5\left( \frac{\pi}{2} \right)$, we find
	
	$$
		\begin{align*}
			p_5\left( \frac{\pi}{2} \right) &= \frac{\pi}{2} - \frac{\left( \frac{\pi}{2} \right)^3}{3!} + \frac{\left( \frac{\pi}{2} \right)^5}{5!}\\
			&\approx 1.0045.
		\end{align*}
	$$
	
	The actual value of $\sin\left( \frac{\pi}{2} \right)$ is exactly $1$, so in reality, $p_5\left( \frac{\pi}{2} \right)$ is off by only $.0045$.
	
	The second part of the example works similarly to this. If we want to approximate $\sin(1)$ with $p_5$, we just evaluate $p_5(1)$:
	
	$$
		\begin{align*}
			p_5(1) &= 1 - \frac{1^3}{3!} + \frac{1^5}{5!}\\
			&= \frac{101}{120}
			&\approx .8417.
		\end{align*}
	$$
	
	By bounding $|R_5(1)|$, we can tell how good of an approximation this is.
	
	$$
		\begin{align*}
			\left| R_5(1) \right| &\leq \frac{\left| 1 \right|^6}{6!}\\
			&= \frac{1}{120}\\
			&\approx .0083.
		\end{align*}
	$$
	
	This is actually how calculators and computers compute functions like $\sin(x)$, $\sqrt{x}$, and $\ln(x)$ for values of $x$ that don&#x2019;t have nice expressions: under the hood, it&#x2019;s all Taylor series.

!end-example



!nav-buttons



!footer



<script>
	if (typeof Page === "undefined")
	{
		if (window.location.search !== "")
		{
			window.location.replace("/index.html?page=" + encodeURIComponent(window.location.pathname) + "&" + window.location.search.slice(1));
		}
		
		else
		{
			window.location.replace("/index.html?page=" + encodeURIComponent(window.location.pathname));
		}
	}
	
	
	
	Page.settings = 
	{
		"title": "Section 9: Taylor Series",
		
		"math_page": true,
		
		"small_margins_on_ultrawide": true,
		
		"parent_list": "/teaching/uo/253/notes"
	};
	
	Page.load();
</script>