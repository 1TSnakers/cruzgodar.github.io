### nav-buttons

For most of the course up to this point, we've focused on square matrices. There's good reason for that --- square matrices are the ones that have eigenvectors and eigenvalues, and the only ones that are possibly diagonalizable. But the framing of diagonalization as a convenient consequence of eigenvectors does a disservice to how useful a concept it is in isolation. In the last section, we saw one way to produce a diagonal-like decomposition that's still extremely useful in its own right, even when the matrix itself wasn't diagonalizable. That required it to be square, though, since we still found ourselves falling back to notions of eigenvectors and eigenvalues eventually. Many "naturally occurring" matrices --- those that come up in real-world applications --- are very much not square, and though they might not have eigenvectors to speak of, they're still worth our time and attention. Let's consider an $m \times n$ matrix $A$ and see what we can do.

While $A$ might not be square, we've already seen a matrix in section 7 that's guaranteed to be: $A^T A$. It's also diagonalizable with an orthonormal basis of eigenvectors, and in an extremely loose sense, if $A$ had eigenvalues, then the eigenvalues of $A^T$ would be the same, and so the eigenvalues of $A^T A$ are the squares of the eigenvalues of $A$.

Let's see if we can turn that intuitive but technically-bankrupt notion into something actually meaningful. Since $A^T A$ is an $n \times n$ real symmetric matrix, it has eigenvalues $\lambda_1, ..., \lambda_n$ and an orthonormal basis of eigenvectors $\vec{v_1}, ..., \vec{v_n}$. If we place these $\vec{v_i}$ as columns in a matrix $V$ (that's an unusual letter for a matrix, but it's standard notation), then $V$ is unitary, so $V^{-1} = V^T$. Now each 

### nav-buttons