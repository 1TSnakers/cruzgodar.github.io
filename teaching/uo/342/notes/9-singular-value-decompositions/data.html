<header><div id="logo"><a href="/home/" tabindex="-1"><img src="/graphics/general-icons/logo.png" alt="Logo" tabindex="1"></img></a></div><div style="height: 20px"></div><h1 class="heading-text">Section 9: Singular Value Decompositions</h1></header><main><section><div class="text-buttons nav-buttons"><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div><p class="body-text">For most of the course up to this point, we&#x2019;ve focused on square matrices. There&#x2019;s good reason for that &mdash; square matrices are the ones that have eigenvectors and eigenvalues, and the only ones that are possibly diagonalizable. But the framing of diagonalization as a convenient consequence of eigenvectors does a disservice to how useful a concept it is in isolation. In the last section, we saw one way to produce a diagonal-like decomposition that&#x2019;s still extremely useful in its own right, even when the matrix itself wasn&#x2019;t diagonalizable. That required it to be square, though, since we still found ourselves falling back to notions of eigenvectors and eigenvalues eventually. Many &#x201C;naturally occurring&#x201D; matrices &mdash; those that come up in real-world applications &mdash; are very much not square, and though they might not have eigenvectors to speak of, they&#x2019;re still worth our time and attention. Let&#x2019;s consider an <span class="tex-holder inline-math" data-source-tex="m \times n">$m \times n$</span> matrix <span class="tex-holder inline-math" data-source-tex="A">$A$</span> and see what we can do.</p><p class="body-text">While <span class="tex-holder inline-math" data-source-tex="A">$A$</span> might not be square, we&#x2019;ve already seen a matrix in section 7 that&#x2019;s guaranteed to be: <span class="tex-holder inline-math" data-source-tex="A^T A">$A^T A$.</span> It&#x2019;s also diagonalizable with an orthonormal basis of eigenvectors, and in an extremely loose sense, if <span class="tex-holder inline-math" data-source-tex="A">$A$</span> had eigenvalues, then the eigenvalues of <span class="tex-holder inline-math" data-source-tex="A^T">$A^T$</span> would be the same, and so the eigenvalues of <span class="tex-holder inline-math" data-source-tex="A^T A">$A^T A$</span> are the squares of the eigenvalues of <span class="tex-holder inline-math" data-source-tex="A">$A$.</span></p><p class="body-text">Let&#x2019;s see if we can turn that intuitive but technically-bankrupt notion into something actually meaningful. Since <span class="tex-holder inline-math" data-source-tex="A^T A">$A^T A$</span> is an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> real symmetric matrix, it has eigenvalues <span class="tex-holder inline-math" data-source-tex="\lambda_1, ..., \lambda_n">$\lambda_1, ..., \lambda_n$</span> and an orthonormal basis of eigenvectors <span class="tex-holder inline-math" data-source-tex="\vec{v_1}, ..., \vec{v_n}">$\vec{v_1}, ..., \vec{v_n}$.</span> If we place these <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$</span> as columns in a matrix <span class="tex-holder inline-math" data-source-tex="V">$V$</span> (that&#x2019;s an unusual letter to use for a matrix, but it&#x2019;s standard notation), then <span class="tex-holder inline-math" data-source-tex="V">$V$</span> is unitary, so <span class="tex-holder inline-math" data-source-tex="V^{-1} = V^T">$V^{-1} = V^T$.</span> Now for each <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$,</span></p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left| \left| A\vec{v} \right| \right|^2 &= A\vec{v_i} \bullet A\vec{v_i}\\[NEWLINE][TAB]&= \vec{v_i}^T A^T A \vec{v_i}\\[NEWLINE][TAB]&= \vec{v_i}^T \lambda_i \vec{v_i}\\[NEWLINE][TAB]&= \lambda_i.[NEWLINE]\end{align*}">$$\begin{align*}\left| \left| A\vec{v} \right| \right|^2 &= A\vec{v_i} \bullet A\vec{v_i}\\[4px]&= \vec{v_i}^T A^T A \vec{v_i}\\[4px]&= \vec{v_i}^T \lambda_i \vec{v_i}\\[4px]&= \lambda_i.\end{align*}$$</span></p><p class="body-text">That tells us quite a bit: first, that all the eigenvalues of <span class="tex-holder inline-math" data-source-tex="A^T A">$A^T A$</span> are positive, and second, that those quantities that were analogous to the square roots of the eigenvalues are given by the values <span class="tex-holder inline-math" data-source-tex="\left| \left| A\vec{v_i} \right| \right|">$\left| \left| A\vec{v_i} \right| \right|$</span> for each <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$.</span> Let&#x2019;s give those values a name, and then we&#x2019;ll finish the decomposition.</p><div class="notes-def notes-environment"><div class="notes-def-title notes-title">Definition: singular value</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="m \times n">$m \times n$</span> matrix. The <strong>singular values</strong> of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> are the values <span class="tex-holder inline-math" data-source-tex="\sigma_1, ..., \sigma_n">$\sigma_1, ..., \sigma_n$,</span> where</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\sigma_i = \sqrt{\lambda_i},[NEWLINE]$$">$$\begin{align*}\sigma_i = \sqrt{\lambda_i},\end{align*}$$</span></p><p class="body-text">and <span class="tex-holder inline-math" data-source-tex="\lambda_1, ..., \lambda_n">$\lambda_1, ..., \lambda_n$</span> are the eigenvalues of <span class="tex-holder inline-math" data-source-tex="A^T A">$A^T A$.</span> Importantly, we always write the singular values in decreasing order.</p></div><p class="body-text">Let&#x2019;s return to decomposing our matrix <span class="tex-holder inline-math" data-source-tex="A">$A$.</span> The outputs <span class="tex-holder inline-math" data-source-tex="A\vec{v_i}">$A\vec{v_i}$</span> and <span class="tex-holder inline-math" data-source-tex="A\vec{v_j}">$A\vec{v_j}$</span> are orthogonal, since</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left( A\vec{v_i} \right) \bullet \left( A\vec{v_j} \right) &= \vec{v_i}^T A^T A \vec{v_j}\\[NEWLINE][TAB]&= \vec{v_i}^T \lambda_j \vec{v_j}\\[NEWLINE][TAB]&= 0,[NEWLINE]\end{align*}">$$\begin{align*}\left( A\vec{v_i} \right) \bullet \left( A\vec{v_j} \right) &= \vec{v_i}^T A^T A \vec{v_j}\\[4px]&= \vec{v_i}^T \lambda_j \vec{v_j}\\[4px]&= 0,\end{align*}$$</span></p><p class="body-text">and so the vectors <span class="tex-holder inline-math" data-source-tex="A\vec{v_1}, ..., A\vec{v_k}">$A\vec{v_1}, ..., A\vec{v_k}$</span> form an orthogonal basis for <span class="tex-holder inline-math" data-source-tex="\operatorname{image} A">$\operatorname{image} A$,</span> where <span class="tex-holder inline-math" data-source-tex="\sigma_1 \geq \cdots \geq \sigma_k > 0">$\sigma_1 \geq \cdots \geq \sigma_k > 0$</span> are the nonzero singular values of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> (since the remaining vectors are mapped to zero). By extending these to an orthogonal basis for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^m">$\mathbb{R}^m$</span> (if they don&#x2019;t already form one) and normalizing them, we can produce an orthonormal basis <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{u_1}, ..., \vec{u_m} \right\}">$\left\{ \vec{u_1}, ..., \vec{u_m} \right\}$</span> for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^m">$\mathbb{R}^m$,</span> and in total we&#x2019;ve decomposed <span class="tex-holder inline-math" data-source-tex="A">$A$</span> into a product very reminiscent of a diagonalization:</p><div class="notes-thm notes-environment"><div class="notes-thm-title notes-title">Theorem: singular value decomposition</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="m \times n">$m \times n$</span> matrix, and let <span class="tex-holder inline-math" data-source-tex="\sigma_1 \geq \cdots \geq \sigma_k > 0">$\sigma_1 \geq \cdots \geq \sigma_k > 0$</span> be its nonzero singular values. Then</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]A = U\Sigma V^T,[NEWLINE]$$">$$\begin{align*}A = U\Sigma V^T,\end{align*}$$</span></p><p class="body-text">where</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]V = \left[\begin{array}{ccc} \mid& & \mid \\ \vec{v_1}& \cdots& \vec{v_n} \\ \mid& & \mid \end{array}\right][NEWLINE]\end{align*}">$$\begin{align*}V = \left[\begin{array}{ccc} \mid& & \mid \\ \vec{v_1}& \cdots& \vec{v_n} \\ \mid& & \mid \end{array}\right]\end{align*}$$</span></p><p class="body-text">is a unitary <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix whose columns are the eigenvectors of <span class="tex-holder inline-math" data-source-tex="A^T A">$A^T A$,</span></p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\Sigma = \left[\begin{array}{c|c} D & 0 \\ \hline 0 & 0 \end{array}\right] = \left[\begin{array}{ccc|ccc} \sigma_1& \cdots& 0 & 0& \cdots& 0 \\ \vdots& \ddots& \vdots & \vdots& \ddots& \vdots \\ 0& \cdots& \sigma_k & 0& \cdots& 0 \\ \hline 0& \cdots& 0 & 0& \cdots& 0 \\ \vdots& \ddots& \vdots & \vdots& \ddots& \vdots \\ 0& \cdots& 0 & 0& \cdots& 0 \end{array}\right][NEWLINE]\end{align*}">$$\begin{align*}\Sigma = \left[\begin{array}{c|c} D & 0 \\ \hline 0 & 0 \end{array}\right] = \left[\begin{array}{ccc|ccc} \sigma_1& \cdots& 0 & 0& \cdots& 0 \\ \vdots& \ddots& \vdots & \vdots& \ddots& \vdots \\ 0& \cdots& \sigma_k & 0& \cdots& 0 \\ \hline 0& \cdots& 0 & 0& \cdots& 0 \\ \vdots& \ddots& \vdots & \vdots& \ddots& \vdots \\ 0& \cdots& 0 & 0& \cdots& 0 \end{array}\right]\end{align*}$$</span></p><p class="body-text">is an <span class="tex-holder inline-math" data-source-tex="m \times n">$m \times n$</span> matrix containing the nonzero singular values on its diagonal and zeros elsewhere, and</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]U = \left[\begin{array}{ccc} \mid& & \mid \\ \vec{u_1}& \cdots& \vec{u_m} \\ \mid& & \mid \end{array}\right][NEWLINE]\end{align*}">$$\begin{align*}U = \left[\begin{array}{ccc} \mid& & \mid \\ \vec{u_1}& \cdots& \vec{u_m} \\ \mid& & \mid \end{array}\right]\end{align*}$$</span></p><p class="body-text">is an <span class="tex-holder inline-math" data-source-tex="m \times m">$m \times m$</span> unitary matrix whose columns form an orthonormal basis for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^m">$\mathbb{R}^m$,</span> where <span class="tex-holder inline-math" data-source-tex="\vec{u_i}">$\vec{u_i}$</span> is <span class="tex-holder inline-math" data-source-tex="A\vec{v_i}">$A\vec{v_i}$</span> (after normalizing).</p></div><div class="notes-ex notes-environment"><div class="notes-ex-title notes-title">Example: singular value decomposition</div><p class="body-text">Find a singular value decomposition for <span class="tex-holder inline-math" data-source-tex="A = \left[\begin{array}{ccc} 1& 1& 2 \\ 1& -1& 1 \end{array}\right]">$A = \left[\begin{array}{ccc} 1& 1& 2 \\ 1& -1& 1 \end{array}\right]$.</span></p><p class="body-text">We&#x2019;ll start by finding the eigenvectors and eigenvalues for <span class="tex-holder inline-math" data-source-tex="A^T A">$A^T A$.</span> We have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A^T A = \left[\begin{array}{ccc} 2& 0& 3 \\ 0& 2& 1 \\ 3& 1& 5 \end{array}\right],[NEWLINE]\end{align*}">$$\begin{align*}A^T A = \left[\begin{array}{ccc} 2& 0& 3 \\ 0& 2& 1 \\ 3& 1& 5 \end{array}\right],\end{align*}$$</span></p><p class="body-text">which has eigenvalues <span class="tex-holder inline-math" data-source-tex="\lambda_1 = 7">$\lambda_1 = 7$,</span> <span class="tex-holder inline-math" data-source-tex="\lambda_2 = 2">$\lambda_2 = 2$,</span> and <span class="tex-holder inline-math" data-source-tex="\lambda_3 = 0">$\lambda_3 = 0$</span> (arranged in decreasing order), with corresponding eigenvectors</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v_1} = \left[\begin{array}{c} 3 \\ 1 \\ 5 \end{array}\right] \qquad \vec{v_2} = \left[\begin{array}{c} -1 \\ 3 \\ 0 \end{array}\right] \qquad \vec{v_3} = \left[\begin{array}{c} -3 \\ 1 \\ 2 \end{array}\right].[NEWLINE]$$">$$\begin{align*}\vec{v_1} = \left[\begin{array}{c} 3 \\ 1 \\ 5 \end{array}\right] \qquad \vec{v_2} = \left[\begin{array}{c} -1 \\ 3 \\ 0 \end{array}\right] \qquad \vec{v_3} = \left[\begin{array}{c} -3 \\ 1 \\ 2 \end{array}\right].\end{align*}$$</span></p><p class="body-text">The singular values of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> are the square roots of the eigenvalues, so we have <span class="tex-holder inline-math" data-source-tex="\sigma_1 = \sqrt{7}">$\sigma_1 = \sqrt{7}$</span> and <span class="tex-holder inline-math" data-source-tex="\sigma_2 = \sqrt{2}">$\sigma_2 = \sqrt{2}$.</span> We won&#x2019;t need <span class="tex-holder inline-math" data-source-tex="\sigma_3 = 0">$\sigma_3 = 0$,</span> since only the nonzero singular values appear in the decomposition. Our final step is to evaluate <span class="tex-holder inline-math" data-source-tex="A">$A$</span> on the <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$</span> corresponding to nonzero singular values and use them to construct an orthonormal basis if they don&#x2019;t already form one. We have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\vec{u_1} &= A\vec{v_1} = \left[\begin{array}{c} 14 \\ 7 \end{array}\right]\\[NEWLINE][TAB]\vec{u_2} &= A\vec{v_2} = \left[\begin{array}{c} 2 \\ -4 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}\vec{u_1} &= A\vec{v_1} = \left[\begin{array}{c} 14 \\ 7 \end{array}\right]\\[4px]\vec{u_2} &= A\vec{v_2} = \left[\begin{array}{c} 2 \\ -4 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Normalizing our vectors and compiling them into a three matrices, we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]U &= \left[\begin{array}{cc} 2 / \sqrt{5}& 1 / \sqrt{5} \\ 1 / \sqrt{5}& -2 / \sqrt{5}  \end{array}\right]\\[NEWLINE][TAB]\Sigma &= \left[\begin{array}{ccc} \sqrt{7}& 0& 0 \\ 0& \sqrt{2}& 0 \end{array}\right]\\[NEWLINE][TAB]V &= \left[\begin{array}{ccc} 3 / \sqrt{35}& -1 / \sqrt{10}& -3 / \sqrt{14} \\ 1 / \sqrt{35}& 3 / \sqrt{10}& 1 / \sqrt{14} \\ 5 / \sqrt{35}& 0& 2 / \sqrt{14} \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}U &= \left[\begin{array}{cc} 2 / \sqrt{5}& 1 / \sqrt{5} \\ 1 / \sqrt{5}& -2 / \sqrt{5}  \end{array}\right]\\[4px]\Sigma &= \left[\begin{array}{ccc} \sqrt{7}& 0& 0 \\ 0& \sqrt{2}& 0 \end{array}\right]\\[4px]V &= \left[\begin{array}{ccc} 3 / \sqrt{35}& -1 / \sqrt{10}& -3 / \sqrt{14} \\ 1 / \sqrt{35}& 3 / \sqrt{10}& 1 / \sqrt{14} \\ 5 / \sqrt{35}& 0& 2 / \sqrt{14} \end{array}\right].\end{align*}$$</span></p><p class="body-text">Geometrically, the expression <span class="tex-holder inline-math" data-source-tex="A = U \Sigma V^T">$A = U \Sigma V^T$</span> expresses the action of <span class="tex-holder inline-math" data-source-tex="A : \mathbb{R}^3 \to \mathbb{R}^2">$A : \mathbb{R}^3 \to \mathbb{R}^2$</span> in four steps: a rotation / reflection in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^3">$\mathbb{R}^3$,</span> a projection to <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^2">$\mathbb{R}^2$,</span> a scaling of the axes in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^2">$\mathbb{R}^2$,</span> and finally a rotation / reflection in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^2">$\mathbb{R}^2$.</span> Since every matrix has an SVD, that means every linear map is expressible as these four steps, possibly swapping the projection for an inclusion to a higher-dimensional space, or just an identity matrix if the domain and codomain are equal.</p></div><div class="notes-exc notes-environment"><div class="notes-exc-title notes-title">Exercise: singular value decomposition</div><p class="body-text">Find a singular value decomposition for <span class="tex-holder inline-math" data-source-tex="A = \left[\begin{array}{cc} 1& -2 \\ 2& 3 \\ -1& 1 \end{array}\right]">$A = \left[\begin{array}{cc} 1& -2 \\ 2& 3 \\ -1& 1 \end{array}\right]$.</span></p></div></section><h2 class="section-text"> An Application to Image Compression</h2><section><div class="text-buttons nav-buttons"><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div></section></main>