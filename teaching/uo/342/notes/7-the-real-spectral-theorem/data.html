<header><div id="logo"><a href="/home/" tabindex="-1"><img src="/graphics/general-icons/logo.png" alt="Logo" tabindex="1"></img></a></div><div style="height: 20px"></div><h1 class="heading-text">Section 7: The Real Spectral Theorem</h1></header><main><section><div class="text-buttons nav-buttons"><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div><p class="body-text">In the first part of the course, we studied bases of eigenvectors, and in the second part, orthonormal bases. These two notions are somewhat in conflict: while we can always construct an orthonormal basis for a vector space with the Gram-Schmidt process, applying that to a basis of eigenvectors for a diagonalizable matrix generally won&#x2019;t preserve the fact that they&#x2019;re eigenvectors. What we&#x2019;ll explore in this last part of the course is the situation in which we can get the best of both worlds &mdash; an orthonormal basis of eigenvectors. Without any more preliminaries, let&#x2019;s state and prove the theorem that characterizes exactly when this is possible.</p><div class="notes-thm notes-environment"><div class="notes-thm-title notes-title">The Real Spectral Theorem</div><p class="body-text">A matrix <span class="tex-holder inline-math" data-source-tex="A">$A$</span> with real entries has an orthonormal basis of eigenvectors with real eigenvalues if and only if <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is symmetric (i.e. <span class="tex-holder inline-math" data-source-tex="A^T = A">$A^T = A$).</span></p></div><p class="body-text">We&#x2019;ll see the proof of this shortly; to keep it from being overwhelmingly long, we&#x2019;ll split it into a few chunks that we can tackle one at a time.</p><div class="notes-lem notes-environment"><div class="notes-lem-title notes-title">Lemma: real symmetric matrices have real eigenvalues</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> symmetric matrix with real entries. Then all of the eigenvalues of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> are real.</p></div><div class="notes-pf notes-environment"><div class="notes-pf-title notes-title">Proof</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="\vec{v} \in \mathbb{C}^n">$\vec{v} \in \mathbb{C}^n$</span> be an eigenvector with eigenvalue <span class="tex-holder inline-math" data-source-tex="\lambda \in \mathbb{C}">$\lambda \in \mathbb{C}$</span> &mdash; we have to assume both are complex since we haven&#x2019;t yet shown that they have to be real. Multiplying <span class="tex-holder inline-math" data-source-tex="A">$A$</span> on the right by <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> and the left by <span class="tex-holder inline-math" data-source-tex="\overline{\vec{v}}^T">$\overline{\vec{v}}^T$,</span> we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\overline{\vec{v}^T} A \vec{v} &= \overline{\vec{v}^T} \lambda \vec{v}\\[NEWLINE][TAB]&= \lambda \left( \vec{v} \bullet \overline{\vec{v}} \right).[NEWLINE]\end{align*}">$$\begin{align*}\overline{\vec{v}^T} A \vec{v} &= \overline{\vec{v}^T} \lambda \vec{v}\\[4px]&= \lambda \left( \vec{v} \bullet \overline{\vec{v}} \right).\end{align*}$$</span></p><p class="body-text">Now <span class="tex-holder inline-math" data-source-tex="\vec{v} \bullet \overline{\vec{v}}">$\vec{v} \bullet \overline{\vec{v}}$</span> is a real number, since</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\overline{\vec{v} \bullet \overline{\vec{v}}} &= \overline{\vec{v}} \bullet \vec{v}\\[NEWLINE][TAB]&= \vec{v} \bullet \overline{\vec{v}},[NEWLINE]\end{align*}">$$\begin{align*}\overline{\vec{v} \bullet \overline{\vec{v}}} &= \overline{\vec{v}} \bullet \vec{v}\\[4px]&= \vec{v} \bullet \overline{\vec{v}},\end{align*}$$</span></p><p class="body-text">and the same is true for <span class="tex-holder inline-math" data-source-tex="\overline{\vec{v}^T} A \vec{v}">$\overline{\vec{v}^T} A \vec{v}$:</span></p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\overline{\overline{\vec{v}^T} A \vec{v}} &= \vec{v}^T \overline{A} \overline{\vec{v}}\\[NEWLINE][TAB]&= \vec{v}^T A \overline{\vec{v}}\\[NEWLINE][TAB]&= \vec{v}^T A^T \overline{\vec{v}}\\[NEWLINE][TAB]&= \left( \overline{\vec{v}}^T A \vec{v} \right)^T\\[NEWLINE][TAB]&= \overline{\vec{v}}^T A \vec{v}.[NEWLINE]\end{align*}">$$\begin{align*}\overline{\overline{\vec{v}^T} A \vec{v}} &= \vec{v}^T \overline{A} \overline{\vec{v}}\\[4px]&= \vec{v}^T A \overline{\vec{v}}\\[4px]&= \vec{v}^T A^T \overline{\vec{v}}\\[4px]&= \left( \overline{\vec{v}}^T A \vec{v} \right)^T\\[4px]&= \overline{\vec{v}}^T A \vec{v}.\end{align*}$$</span></p><p class="body-text">All that is to say that <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$</span> itself must also be real, since</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\lambda = \frac{\overline{\vec{v}^T} A \vec{v}}{\vec{v} \bullet \overline{\vec{v}}}.[NEWLINE]$$">$$\begin{align*}\lambda = \frac{\overline{\vec{v}^T} A \vec{v}}{\vec{v} \bullet \overline{\vec{v}}}.\end{align*}$$</span></p></div><p class="body-text">This lemma also implies that all the eigenvectors of a real symmetric matrix are real, since row reducing <span class="tex-holder inline-math" data-source-tex="A - \lambda I">$A - \lambda I$</span> for a real-valued matrix <span class="tex-holder inline-math" data-source-tex="A">$A$</span> and a real eigenvalue <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$</span> will only produce real solutions.</p><div class="notes-lem notes-environment"><div class="notes-lem-title notes-title">Lemma: real symmetric matrices have orthogonal eigenspaces</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> symmetric matrix with real entries and let <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{w}">$\vec{w}$</span> be eigenvectors corresponding to distinct eigenvalues <span class="tex-holder inline-math" data-source-tex="\lambda_i">$\lambda_i$</span> and <span class="tex-holder inline-math" data-source-tex="\lambda_j">$\lambda_j$.</span> Then <span class="tex-holder inline-math" data-source-tex="\vec{v} \bullet \vec{w} = 0">$\vec{v} \bullet \vec{w} = 0$.</span></p></div><div class="notes-pf notes-environment"><div class="notes-pf-title notes-title">Proof</div><p class="body-text">We can verify this with a direct computation:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB](\lambda_i - \lambda_j) \left( \vec{v} \bullet \vec{w} \right) &= \lambda_i \left( \vec{v} \bullet \vec{w} \right) - \lambda_j \left( \vec{v} \bullet \vec{w} \right)\\[NEWLINE][TAB]&= \left( \lambda_i \vec{v} \right) \bullet \vec{w} - \vec{v} \bullet \left( \lambda_j \vec{w} \right)\\[NEWLINE][TAB]&= \left( A \vec{v} \right) \bullet \vec{w} - \vec{v} \bullet \left( A \vec{w} \right)\\[NEWLINE][TAB]&= \left( A \vec{v} \right)^T \vec{w} - \vec{v}^T \left( A \vec{w} \right)\\[NEWLINE][TAB]&= \vec{v}^T A^T \vec{w} - \vec{v}^T A \vec{w}\\[NEWLINE][TAB]&= \vec{v}^T A \vec{w} - \vec{v}^T A \vec{w}\\[NEWLINE][TAB]&= 0.[NEWLINE]\end{align*}">$$\begin{align*}(\lambda_i - \lambda_j) \left( \vec{v} \bullet \vec{w} \right) &= \lambda_i \left( \vec{v} \bullet \vec{w} \right) - \lambda_j \left( \vec{v} \bullet \vec{w} \right)\\[4px]&= \left( \lambda_i \vec{v} \right) \bullet \vec{w} - \vec{v} \bullet \left( \lambda_j \vec{w} \right)\\[4px]&= \left( A \vec{v} \right) \bullet \vec{w} - \vec{v} \bullet \left( A \vec{w} \right)\\[4px]&= \left( A \vec{v} \right)^T \vec{w} - \vec{v}^T \left( A \vec{w} \right)\\[4px]&= \vec{v}^T A^T \vec{w} - \vec{v}^T A \vec{w}\\[4px]&= \vec{v}^T A \vec{w} - \vec{v}^T A \vec{w}\\[4px]&= 0.\end{align*}$$</span></p><p class="body-text">Since <span class="tex-holder inline-math" data-source-tex="\lambda_i \neq \lambda_j">$\lambda_i \neq \lambda_j$,</span> <span class="tex-holder inline-math" data-source-tex="\lambda_i - \lambda_j \neq 0">$\lambda_i - \lambda_j \neq 0$,</span> and so <span class="tex-holder inline-math" data-source-tex="\vec{v} \bullet \vec{w} = 0">$\vec{v} \bullet \vec{w} = 0$.</span></p></div><p class="body-text">Our final lemma is considerably more technical, but it&#x2019;ll let us put all the pieces together.</p><div class="notes-lem notes-environment"><div class="notes-lem-title notes-title">Lemma: symmetric matrices have symmetric restrictions</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> symmetric matrix, let <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> be an eigenvector of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> with eigenvalue <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$,</span> and let <span class="tex-holder inline-math" data-source-tex="X = \operatorname{span}\left\{ \vec{v} \right\}">$X = \operatorname{span}\left\{ \vec{v} \right\}$</span> Then if <span class="tex-holder inline-math" data-source-tex="\vec{w} \in X^\perp">$\vec{w} \in X^\perp$,</span> <span class="tex-holder inline-math" data-source-tex="A\vec{w} \in X^\perp">$A\vec{w} \in X^\perp$</span> too, and if <span class="tex-holder inline-math" data-source-tex="A|_{X^\perp} : X^\perp \to X^\perp">$A|_{X^\perp} : X^\perp \to X^\perp$</span> is the linear map given by restricting <span class="tex-holder inline-math" data-source-tex="A">$A$</span> to <span class="tex-holder inline-math" data-source-tex="X^\perp">$X^\perp$,</span> then the matrix for <span class="tex-holder inline-math" data-source-tex="A|_{X^\perp}">$A|_{X^\perp}$</span> is symmetric.</p></div><div class="notes-pf notes-environment"><div class="notes-pf-title notes-title">Proof</div><p class="body-text">There are a ton of symbols here, but the moral of this lemma is that <span class="tex-holder inline-math" data-source-tex="A">$A$</span> can be split cleanly into its action on two subspaces: <span class="tex-holder inline-math" data-source-tex="X = \operatorname{span}\left\{ \vec{v} \right\}">$X = \operatorname{span}\left\{ \vec{v} \right\}$</span> and <span class="tex-holder inline-math" data-source-tex="X^\perp">$X^\perp$.</span> We already know that <span class="tex-holder inline-math" data-source-tex="A : X \to X">$A : X \to X$,</span> since <span class="tex-holder inline-math" data-source-tex="A \vec{v} = \lambda \vec{v} \in X">$A \vec{v} = \lambda \vec{v} \in X$,</span> and now we&#x2019;ll also show that <span class="tex-holder inline-math" data-source-tex="A : X^\perp \to X^\perp">$A : X^\perp \to X^\perp$</span> as the lemma states.</p><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="\vec{w} \in X^\perp">$\vec{w} \in X^\perp$.</span> Then <span class="tex-holder inline-math" data-source-tex="\vec{w} \bullet \vec{v} = 0">$\vec{w} \bullet \vec{v} = 0$,</span> and so</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left( A\vec{w} \right) \bullet \vec{v} &= \left( A\vec{w} \right)^T \bullet \vec{v}\\[NEWLINE][TAB]&= \vec{w}^T A^T \vec{v}\\[NEWLINE][TAB]&= \vec{w}^T A \vec{v}\\[NEWLINE][TAB]&= \vec{w}^T \lambda \vec{v}\\[NEWLINE][TAB]&= \lambda \left( \vec{w} \bullet \vec{v} \right)\\[NEWLINE][TAB]&= 0,[NEWLINE]\end{align*}">$$\begin{align*}\left( A\vec{w} \right) \bullet \vec{v} &= \left( A\vec{w} \right)^T \bullet \vec{v}\\[4px]&= \vec{w}^T A^T \vec{v}\\[4px]&= \vec{w}^T A \vec{v}\\[4px]&= \vec{w}^T \lambda \vec{v}\\[4px]&= \lambda \left( \vec{w} \bullet \vec{v} \right)\\[4px]&= 0,\end{align*}$$</span></p><p class="body-text">meaning <span class="tex-holder inline-math" data-source-tex="A\vec{w} \in X^\perp">$A\vec{w} \in X^\perp$.</span></p><p class="body-text">Now let <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{w_1}, ..., \vec{w_{n - 1}} \right\}">$\left\{ \vec{w_1}, ..., \vec{w_{n - 1}} \right\}$</span> be a basis for <span class="tex-holder inline-math" data-source-tex="X^\perp">$X^\perp$.</span> What we&#x2019;ve shown is that the matrix of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> with respect to the basis <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{v}, \vec{w_1}, ..., \vec{w_{n - 1}} \right\}">$\left\{ \vec{v}, \vec{w_1}, ..., \vec{w_{n - 1}} \right\}$</span> for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$</span> is</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{c|ccc} \lambda & 0& \cdots& 0 \\ \hline 0 & && \\ \vdots & & C& \\ 0 & && \end{array}\right][NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{c|ccc} \lambda & 0& \cdots& 0 \\ \hline 0 & && \\ \vdots & & C& \\ 0 & && \end{array}\right]\end{align*}$$</span></p><p class="body-text">for some matrix <span class="tex-holder inline-math" data-source-tex="C">$C$</span> &mdash; the second statement of this lemma is that <span class="tex-holder inline-math" data-source-tex="C">$C$</span> is also symmetric. This amounts to showing that changing basis preserves symmetry: if</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]B = \left[\begin{array}{cccc} \mid& \mid& & \mid \\ \vec{v}& \vec{w_1}& \cdots& \vec{w_{n - 1}} \\ \mid& \mid& & \mid \end{array}\right],[NEWLINE]\end{align*}">$$\begin{align*}B = \left[\begin{array}{cccc} \mid& \mid& & \mid \\ \vec{v}& \vec{w_1}& \cdots& \vec{w_{n - 1}} \\ \mid& \mid& & \mid \end{array}\right],\end{align*}$$</span></p><p class="body-text">then</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]B^{-1}AB = \left[\begin{array}{c|ccc} \lambda & 0& \cdots& 0 \\ \hline 0 & && \\ \vdots & & C& \\ 0 & && \end{array}\right],[NEWLINE]\end{align*}">$$\begin{align*}B^{-1}AB = \left[\begin{array}{c|ccc} \lambda & 0& \cdots& 0 \\ \hline 0 & && \\ \vdots & & C& \\ 0 & && \end{array}\right],\end{align*}$$</span></p></div><div class="notes-pf notes-environment"><div class="notes-pf-title notes-title">Proof of the Real Spectral Theorem</div><p class="body-text">To begin, let&#x2019;s show that if <span class="tex-holder inline-math" data-source-tex="A">$A$</span> has an orthonormal basis <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{v_1}, ..., \vec{v_n} \right\}">$\left\{ \vec{v_1}, ..., \vec{v_n} \right\}$</span> of eigenvectors, then it must be symmetric. This is just a direct computation: we know <span class="tex-holder inline-math" data-source-tex="A = BDB^{-1}">$A = BDB^{-1}$,</span> but <span class="tex-holder inline-math" data-source-tex="B">$B$</span> must be a unitary matrix, and so <span class="tex-holder inline-math" data-source-tex="B^{-1} = B^T">$B^{-1} = B^T$.</span> Then</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A^T &= \left( BDB^T \right)^T\\[NEWLINE][TAB]&= \left( B^T \right)^T D^T B^T\\[NEWLINE][TAB]&= BDB^T\\[NEWLINE][TAB]&= A,[NEWLINE]\end{align*}">$$\begin{align*}A^T &= \left( BDB^T \right)^T\\[4px]&= \left( B^T \right)^T D^T B^T\\[4px]&= BDB^T\\[4px]&= A,\end{align*}$$</span></p><p class="body-text">so <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is symmetric.</p><p class="body-text">The other direction is a little more complicated. We&#x2019;ll start by showing that if <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is a symmetric matrix with real entries, then all of its eigenvalues must be real. </p><p class="body-text">To finish the proof, we now need to show that <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is diagonalizable and that all its eigenvectors are orthonormal. That sounds like quite a lot, but all it means for <span class="tex-holder inline-math" data-source-tex="A">$A$</span> to be diagonalizable is that it has <span class="tex-holder inline-math" data-source-tex="n">$n$</span> linearly independent eigenvectors, and since orthogonality implies linear independence, all we need to show is that there are <span class="tex-holder inline-math" data-source-tex="n">$n$</span> orthogonal eigenvectors &mdash; we can also freely rescale them, so orthonormality will follow for free.</p><p class="body-text">First of all, we can always arrange for eigenvectors corresponding to the same eigenvalue to be orthogonal: if <span class="tex-holder inline-math" data-source-tex="E_i">$E_i$</span> is the eigenspace corresponding to <span class="tex-holder inline-math" data-source-tex="\lambda_i">$\lambda_i$</span> (i.e. the subspace consisting of all eigenvectors with eigenvalue <span class="tex-holder inline-math" data-source-tex="\lambda_i">$\lambda_i$),</span> then we can just run the Gram-Schmidt process on <span class="tex-holder inline-math" data-source-tex="E_i">$E_i$</span> to produce an orthogonal basis. That approach doesn&#x2019;t work for the entire space, though, since the linear combinations of eigenvectors that the Gram-Schmidt process produces are only still eigenvectors when they share all the same eigenvalue.</p><p class="body-text">We&#x2019;re almost done! We&#x2019;ve shown that all the eigenvalues and eigenvectors are real, and that the eigenvectors are also orthogonal. The only thing left to show is that there are enough eigenvectors &mdash; that the algebraic multiplicity of every eigenvalue is the same as the geometric multiplicity.</p></div><div class="text-buttons nav-buttons"><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div></section></main>