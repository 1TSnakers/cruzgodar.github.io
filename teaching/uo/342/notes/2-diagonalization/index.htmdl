### nav-buttons

Suppose we have an $n \times n$ matrix $A$ whose eigenvectors $\{\vec{v_1}, ..., \vec{v_n}\}$ form a basis and have corresponding eigenvalues $\lambda_1, ..., \lambda_n$. As we've seen in the last section, it's most convenient to deal with $A$ when we're using this basis. If $B$ is the matrix with columns $\vec{v_i}$, then $B^{-1}AB$ acts (from right to left) by converting a vector in the basis $\{\vec{v_1}, ..., \vec{v_n}\}$ to the standard basis, applies $A$ on it, and then converts the output back to $\{\vec{v_1}, ..., \vec{v_n}\}$. Specifically,

$$
	B^{-1}AB\vec{e_i} &= BA\vec{v_i}

	&= B^{-1}\lambda_i \vec{v_i}

	&= \lambda_i\vec{e_i},
$$

so the total matrix is

$$
	B^{-1}AB = [[ \lambda_1, 0, \cdots, 0 ; 0, \lambda_2, \cdots, 0 ; \vdots, \vdots, \ddots, \vdots ; 0, 0, \cdots, \lambda_n ]].
$$

That matrix is what's called **diagonal**: only its diagonal entries are nonzero. Calling it $D$, we've expressed $A$ as $A = BDB^{-1}$. This puts the ideas of the previous section into a more symbolic form: the reason why this makes $A^{100}$ easier to compute is that

$$
	A^{100} &= \left( BDB^{-1} \right)^{100}

	&= BDB^{-1} BDB^{-1} \cdots BDB^{-1}

	&= BD^{100}B^{-1},
$$

and since $D$ is diagonal, $D^{100}$ is just $D$ with each entry raised to the 100th power. The process of expressing $A$ in the form $BDB^{-1}$ is called **diagonalizing** $A$, and it's possible exactly when $B$ is an invertible matrix --- that is, when the eigenvectors of $A$ are linearly independent. We know that's the case when the eigenvalues are all distinct, but when that's not the case --- when some eigenvalues are repeated --- we'll need to do some more work.



## Repeated Eigenvalues and Eigenspaces

Having $n$ distinct eigenvalues might be a sufficient condition for a matrix to be diagonalizable, but we know it can't be necessary: the $2 \times 2$ identity matrix has the single eigenvalue of $1$ repeated twice, and it's definitely diagonalizable since it's already diagonal. On the other hand, there are matrices where the diagonalization process breaks down because of repeated eigenvalues. Let's take a look at one as a case study to see where things go wrong.

Let $A$ be the matrix

$$
	A = [[ -1, -3, 3 ; -3, -2, 4 ; -3, -4, 6 ]].
$$

Find the eigenvectors and eigenvalues of $A$.

We'll begin as usual by find the roots of the characteristic polynomial $\chi_A(\lambda)$. We have

$$
	\chi_A(\lambda) &= \det (A - \lambda I)

	&= (-1 - \lambda)((-2 - \lambda)(6 - \lambda) + 16) + 3(-3(6 - \lambda) + 12) + 3(12 + 3(-2 - \lambda))
	
	&= -\lambda^3 + 3\lambda^2 - 4.
$$

Cubics are hard to factor, and unless there's an easy root of $\lambda = 0$, we may as well just ask a computer. This one factors as

$$
	\chi_A(\lambda) &= -(\lambda + 1)(\lambda - 2)^2,
$$

so either $\lambda = -1$ or $\lambda = 2$. For the first possibility, we have

$$
	[[ 0, -3, 3 | 0 ; -3, -1, 4 | 0 ; -3, -4, 7 | 0 ]] & 

	[[ 0, -3, 3 | 0 ; -3, -1, 4 | 0 ; 0, -3, 3 | 0 ]] & \quad \vec{r_3} \me \vec{r_2}

	[[ 0, -3, 3 | 0 ; -3, -1, 4 | 0 ; 0, 0, 0 | 0 ]] & \quad \vec{r_3} \me \vec{r_1}

	[[ 0, 1, -1 | 0 ; -3, -1, 4 | 0 ; 0, 0, 0 | 0 ]] & \quad \vec{r_1} \te -\frac{1}{3}

	[[ 0, 1, -1 | 0 ; -3, 0, 3 | 0 ; 0, 0, 0 | 0 ]] & \quad \vec{r_2} \pe \vec{r_1}

	[[ 0, 1, -1 | 0 ; 1, 0, -1 | 0 ; 0, 0, 0 | 0 ]] & \quad \vec{r_2} \te -\frac{1}{3}
$$

and so a solution is

$$
	\vec{v_1} = [[ 1 ; 1 ; 1 ]].
$$

Let's take a look at the other eigenvalue:

$$
	[[ -3, -3, 3 | 0 ; -3, -4, 4 | 0 ; -3, -4, 4 | 0 ]] & 

	[[ -3, -3, 3 | 0 ; -3, -4, 4 | 0 ; 0, 0, 0 | 0 ]] & \quad \vec{r_3} \me \vec{r_2}

	[[ -3, -3, 3 | 0 ; 0, -1, 1 | 0 ; 0, 0, 0 | 0 ]] & \quad \vec{r_2} \me \vec{r_1}

	[[ 1, 1, -1 | 0 ; 0, 1, -1 | 0 ; 0, 0, 0 | 0 ]] & \quad :: \vec{r_1} \te -\frac{1}{3} ; \vec{r_2} \te -1 ::

	[[ 1, 0, 0 | 0 ; 0, 1, -1 | 0 ; 0, 0, 0 | 0 ]] & \quad \vec{r_1} \me \vec{r_2}
$$

Our solution here is

$$
	\vec{v_2} = [[ 0 ; 1 ; 1 ]].
$$

There are only two eigenvectors for $A$, and so $A$ isn't diagonalizable. So what went wrong? The repeated eigenvalue of $\lambda = 2$ feels like it might be the culprit: since it appeared as a root of the characteristic polynomial with multiplicity 2, we'd expect there to be two linearly independent eigenvectors corresponding to it. Let's dig into that a little more and see if we can find exactly where our expectation deviates from reality.

Suppose $\vec{v_1}, ..., \vec{v_k}$ are all the eigenvectors of an $n \times n$ matrix $A$ with the eigenvalue $\lambda_1$. Then we can form a basis for $#R#^n$ by extending $\{ \vec{v_1}, ..., \vec{v_k} \}$ to

$$
	\{ \vec{v_1}, ..., \vec{v_k}, \vec{v_{k+1}}, ..., \vec{v_n} \}.
$$

If $B$ is the matrix with these $\vec{v_i}$ as columns (i.e. the change-of-basis matrix from this basis to the standard one), then

$$
	A = B [[ \lambda_1 I | C_1 ; \hline 0, C_2 ]] B^{-1}.
$$

This is a matrix written in *block* form: in the top left is a $k \times k$ block with $\lambda_1$ down the diagonal, in the bottom right is an $(n - k) \times (n - k)$ block called $C_2$ with arbitrary entries, and similarly for the other two blocks. What's important is that the bottom-left block is all zeros, since $A\vec{v_i} = \lambda_1\vec{v_i}$ for all $i$ with $1 \leq i \leq k$. If we take the characteristic polynomial of both sides, we find

$$
	\chi_A(\lambda) &= \det \left( B [[ \lambda_1 I | C_1 ; \hline 0, C_2 ]] B^{-1} - \lambda I \right)

	&= \det \left( B [[ \lambda_1 I | C_1 ; \hline 0, C_2 ]] B^{-1} - \lambda (BIB^{-1}) \right)

	&= \det \left( B \left( [[ \lambda_1 I | C_1 ; \hline 0, C_2 ]] - \lambda I \right) B^{-1} \right)

	&= \det B \det \left( [[ \lambda_1 I | C_1 ; \hline 0, C_2 ]] - \lambda I \right) \det B^{-1}

	&= \det \left( [[ \lambda_1 I | C_1 ; \hline 0, C_2 ]] - \lambda I \right).

	&= (\lambda_1 - \lambda)^k \chi_{C_2}(\lambda).
$$

So, what has all this work told us? If a matrix has at least $k$ linearly independent eigenvectors with eigenvalue $\lambda_1$, then $\lambda_1$ appears as a root of the characteristic polynomial $\chi_A(\lambda)$ with multiplicity *at least* $k$, but possibly more. Let's give these two multiplicities names to properly distinguish them.

### def "algebraic and geometric multiplicity"

	Let $A$ be an $n \times n$ matrix. The **algebraic multiplicity** of an eigenvalue $\lambda_i$ of $A$ is the power of the factor $(\lambda_i - \lambda)$ in $\chi_A(\lambda)$. The **eigenspace** $E_i$ corresponding to $\lambda_i$ is the subspace of $#R#^n$ of eigenvectors $\vec{v}$ with eigenvalue $\lambda_i$, and the **geometric multiplicity** of $\lambda_i$ is $\dim E_i$.

	The sum of the algebraic multiplicities of the eigenvalues of $A$ is always equal to $n$ (since $\deg \chi_A(\lambda) = n$), but the sum of the geometric multiplicities is equal to $n$ only if $A$ is diagonalizable (and vice versa). Moreover, the geometric multiplicity of a $\lambda_i$ is at least 1 and no more than its algebraic multiplicity.

###

### exc "eigenspaces"

	Let $A$ be an $n \times n$ matrix and let $\lambda_1$ be an eigenvalue of $A$. Show that the eigenspace $E_1$ is actually a subspace of $#R#^n$.

###

### ex "algebraic and geometric multiplicity"
	
	Consider the matrices

	$$
		A_1 = [[ 3, 0, 0 ; 0, 5, 0 ; 0, 0, 5 ]], \quad A_2 = [[ 3, 0, 0 ; 0, 5, 1 ; 0, 0, 5 ]].
	$$

	Both have characteristic polynomials of $(3 - \lambda)(5 - \lambda)^2$, so their eigenvalues are $\lambda_1 = 3$ and $\lambda_2 = 5$, with algebraic multiplicities 1 and 2, respectively. The previous definition tells us that the geometric multiplicity of $\lambda_1$ should be 1 for both $A_1$ and $A_2$, but that the geometric multiplicity of $\lambda_2$ will either be 1 or 2. Solving for the eigenvectors of $A_1$, we have

	$$
		[[ A_1 - 3I | \vec{0} ]] &= [[ 0, 0, 0 | 0 ; 0, 2, 0 | 0 ; 0, 0, 2 | 0 ]]

		[[ A_1 - 5I | \vec{0} ]] &= [[ -2, 0, 0 | 0 ; 0, 0, 0 | 0 ; 0, 0, 0 | 0 ]],
	$$

	so the eigenspaces $E_1$ and $E_2$ are

	$$
		E_1 &= \span\left\{ [[ 1 ; 0 ; 0 ]] \right\}

		E_2 &= \span\left\{ [[ 0 ; 1 ; 0 ]], [[ 0 ; 0 ; 1 ]] \right\},
	$$

	meaning the geometric multiplicities of both $\lambda_1$ and $\lambda_2$ are equal to their algebraic multiplicities. That means $A_1$ is diagonalizable, which is pretty unsurprising since it's already diagonal. In contrast, $A_2$ fails to be diagonalizable:

	$$
		[[ A_2 - 3I | \vec{0} ]] &= [[ 0, 0, 0 | 0 ; 0, 2, 1 | 0 ; 0, 0, 2 | 0 ]]

		[[ A_2 - 5I | \vec{0} ]] &= [[ -2, 0, 0 | 0 ; 0, 0, 1 | 0 ; 0, 0, 0 | 0 ]],
	$$

	so the eigenspaces are

	$$
		E_1 &= \span\left\{ [[ 1 ; 0 ; 0 ]] \right\}

		E_2 &= \span\left\{ [[ 0 ; 1 ; 0 ]] \right\},
	$$

	so both eigenvalues have geometric multiplicities of 1. Later in the course, we'll develop a sense in $A_2$ is "almost" diagonal, or at least the closest we can get, but for now, it's a black-and-white distinction.

###



### nav-buttons