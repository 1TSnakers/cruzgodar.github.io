<header><div id="logo"><a href="/home/" tabindex="-1"><img src="/graphics/general-icons/logo.png" alt="Logo" tabindex="1"></img></a></div><div style="height: 20px"></div><h1 class="heading-text">Section 2: Diagonalization</h1></header><main><section><div class="text-buttons nav-buttons"><div class="focus-on-child tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div><p class="body-text">Suppose we have an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix <span class="tex-holder inline-math" data-source-tex="A">$A$</span> whose eigenvectors <span class="tex-holder inline-math" data-source-tex="\{\vec{v_1}, ..., \vec{v_n}\}">$\{\vec{v_1}, ..., \vec{v_n}\}$</span> form a basis and have corresponding eigenvalues <span class="tex-holder inline-math" data-source-tex="\lambda_1, ..., \lambda_n">$\lambda_1, ..., \lambda_n$</span>. As we&#x2019;ve seen in the last section, it&#x2019;s most convenient to deal with <span class="tex-holder inline-math" data-source-tex="A">$A$</span> when we&#x2019;re using this basis. If <span class="tex-holder inline-math" data-source-tex="B">$B$</span> is the matrix with columns <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$</span>, then <span class="tex-holder inline-math" data-source-tex="B^{-1}AB">$B^{-1}AB$</span> acts (from right to left) by converting a vector in the basis <span class="tex-holder inline-math" data-source-tex="\{\vec{v_1}, ..., \vec{v_n}\}">$\{\vec{v_1}, ..., \vec{v_n}\}$</span> to the standard basis, applies <span class="tex-holder inline-math" data-source-tex="A">$A$</span> on it, and then converts the output back to <span class="tex-holder inline-math" data-source-tex="\{\vec{v_1}, ..., \vec{v_n}\}">$\{\vec{v_1}, ..., \vec{v_n}\}$</span>. Specifically,</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]B^{-1}AB\vec{e_i} &= BA\vec{v_i}\\[NEWLINE][TAB]&= B^{-1}\lambda_i \vec{v_i}\\[NEWLINE][TAB]&= \lambda_i\vec{e_i},[NEWLINE]\end{align*}">$$\begin{align*}B^{-1}AB\vec{e_i} &= BA\vec{v_i}\\[4px]&= B^{-1}\lambda_i \vec{v_i}\\[4px]&= \lambda_i\vec{e_i},\end{align*}$$</span></p><p class="body-text">so the total matrix is</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]B^{-1}AB = \left[\begin{array}{cccc} \lambda_1& 0& \cdots& 0 \\ 0& \lambda_2& \cdots& 0 \\ \vdots& \vdots& \ddots& \vdots \\ 0& 0& \cdots& \lambda_n \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}B^{-1}AB = \left[\begin{array}{cccc} \lambda_1& 0& \cdots& 0 \\ 0& \lambda_2& \cdots& 0 \\ \vdots& \vdots& \ddots& \vdots \\ 0& 0& \cdots& \lambda_n \end{array}\right].\end{align*}$$</span></p><p class="body-text">That matrix is what&#x2019;s called <strong>diagonal</strong>: only its diagonal entries are nonzero. Calling it <span class="tex-holder inline-math" data-source-tex="D">$D$</span>, we&#x2019;ve expressed <span class="tex-holder inline-math" data-source-tex="A">$A$</span> as <span class="tex-holder inline-math" data-source-tex="A = BDB^{-1}">$A = BDB^{-1}$</span>. This puts the ideas of the previous section into a more symbolic form: the reason why this makes <span class="tex-holder inline-math" data-source-tex="A^{100}">$A^{100}$</span> easier to compute is that</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A^{100} &= \left( BDB^{-1} \right)^{100}\\[NEWLINE][TAB]&= BDB^{-1} BDB^{-1} \cdots BDB^{-1}\\[NEWLINE][TAB]&= BD^{100}B^{-1},[NEWLINE]\end{align*}">$$\begin{align*}A^{100} &= \left( BDB^{-1} \right)^{100}\\[4px]&= BDB^{-1} BDB^{-1} \cdots BDB^{-1}\\[4px]&= BD^{100}B^{-1},\end{align*}$$</span></p><p class="body-text">and since <span class="tex-holder inline-math" data-source-tex="D">$D$</span> is diagonal, <span class="tex-holder inline-math" data-source-tex="D^{100}">$D^{100}$</span> is just <span class="tex-holder inline-math" data-source-tex="D">$D$</span> with each entry raised to the 100th power. The process of expressing <span class="tex-holder inline-math" data-source-tex="A">$A$</span> in the form <span class="tex-holder inline-math" data-source-tex="BDB^{-1}">$BDB^{-1}$</span> is called <strong>diagonalizing</strong> <span class="tex-holder inline-math" data-source-tex="A">$A$</span>, and it&#x2019;s possible exactly when <span class="tex-holder inline-math" data-source-tex="B">$B$</span> is an invertible matrix &mdash; that is, when the eigenvectors of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> are linearly independent. We know that&#x2019;s the case when the eigenvalues are all distinct, but when that&#x2019;s not the case &mdash; when some eigenvalues are repeated &mdash; we&#x2019;ll need to do some more work.</p></section><h2 class="section-text"> Repeated Eigenvalues and Eigenspaces</h2><section><p class="body-text">Having <span class="tex-holder inline-math" data-source-tex="n">$n$</span> distinct eigenvalues might be a sufficient condition for a matrix to be diagonalizable, but we know it can&#x2019;t be necessary: the <span class="tex-holder inline-math" data-source-tex="2 \times 2">$2 \times 2$</span> identity matrix has the single eigenvalue of <span class="tex-holder inline-math" data-source-tex="1">$1$</span> repeated twice, and it&#x2019;s definitely diagonalizable since it&#x2019;s already diagonal. On the other hand, there are matrices where the diagonalization process breaks down because of repeated eigenvalues. Let&#x2019;s take a look at one as a case study to see where things go wrong.</p><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be the matrix</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A = \left[\begin{array}{ccc} -1& -3& 3 \\ -3& -2& 4 \\ -3& -4& 6 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}A = \left[\begin{array}{ccc} -1& -3& 3 \\ -3& -2& 4 \\ -3& -4& 6 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Find the eigenvectors and eigenvalues of <span class="tex-holder inline-math" data-source-tex="A">$A$</span>.</p><p class="body-text">We&#x2019;ll begin as usual by find the roots of the characteristic polynomial <span class="tex-holder inline-math" data-source-tex="\chi_A(\lambda)">$\chi_A(\lambda)$</span>. We have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\chi_A(\lambda) &= \det (A - \lambda I)\\[NEWLINE][TAB]&= (-1 - \lambda)((-2 - \lambda)(6 - \lambda) + 16) + 3(-3(6 - \lambda) + 12) + 3(12 + 3(-2 - \lambda))\\[NEWLINE][TAB]&= -\lambda^3 + 3\lambda^2 - 4.[NEWLINE]\end{align*}">$$\begin{align*}\chi_A(\lambda) &= \det (A - \lambda I)\\[4px]&= (-1 - \lambda)((-2 - \lambda)(6 - \lambda) + 16) + 3(-3(6 - \lambda) + 12) + 3(12 + 3(-2 - \lambda))\\[4px]&= -\lambda^3 + 3\lambda^2 - 4.\end{align*}$$</span></p><p class="body-text">Cubics are hard to factor, and unless there&#x2019;s an easy root of <span class="tex-holder inline-math" data-source-tex="\lambda = 0">$\lambda = 0$</span>, we may as well just ask a computer. This one factors as</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\chi_A(\lambda) &= -(\lambda + 1)(\lambda - 2)^2,[NEWLINE]\end{align*}">$$\begin{align*}\chi_A(\lambda) &= -(\lambda + 1)(\lambda - 2)^2,\end{align*}$$</span></p><p class="body-text">so either <span class="tex-holder inline-math" data-source-tex="\lambda = -1">$\lambda = -1$</span> or <span class="tex-holder inline-math" data-source-tex="\lambda = 2">$\lambda = 2$</span>. For the first possibility, we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{ccc|c} 0& -3& 3 & 0 \\ -3& -1& 4 & 0 \\ -3& -4& 7 & 0 \end{array}\right] & \\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 0& -3& 3 & 0 \\ -3& -1& 4 & 0 \\ 0& -3& 3 & 0 \end{array}\right] & \quad \vec{r_3} \ -\!\!= \vec{r_2}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 0& -3& 3 & 0 \\ -3& -1& 4 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_3} \ -\!\!= \vec{r_1}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 0& 1& -1 & 0 \\ -3& -1& 4 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_1} \ \times\!\!= -\frac{1}{3}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 0& 1& -1 & 0 \\ -3& 0& 3 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ +\!\!= \vec{r_1}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 0& 1& -1 & 0 \\ 1& 0& -1 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ \times\!\!= -\frac{1}{3}[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{ccc|c} 0& -3& 3 & 0 \\ -3& -1& 4 & 0 \\ -3& -4& 7 & 0 \end{array}\right] & \\[4px]\left[\begin{array}{ccc|c} 0& -3& 3 & 0 \\ -3& -1& 4 & 0 \\ 0& -3& 3 & 0 \end{array}\right] & \quad \vec{r_3} \ -\!\!= \vec{r_2}\\[4px]\left[\begin{array}{ccc|c} 0& -3& 3 & 0 \\ -3& -1& 4 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_3} \ -\!\!= \vec{r_1}\\[4px]\left[\begin{array}{ccc|c} 0& 1& -1 & 0 \\ -3& -1& 4 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_1} \ \times\!\!= -\frac{1}{3}\\[4px]\left[\begin{array}{ccc|c} 0& 1& -1 & 0 \\ -3& 0& 3 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ +\!\!= \vec{r_1}\\[4px]\left[\begin{array}{ccc|c} 0& 1& -1 & 0 \\ 1& 0& -1 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ \times\!\!= -\frac{1}{3}\end{align*}$$</span></p><p class="body-text">and so a solution is</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v_1} = \left[\begin{array}{c} 1 \\ 1 \\ 1 \end{array}\right].[NEWLINE]$$">$$\begin{align*}\vec{v_1} = \left[\begin{array}{c} 1 \\ 1 \\ 1 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Let&#x2019;s take a look at the other eigenvalue:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{ccc|c} -3& -3& 3 & 0 \\ -3& -4& 4 & 0 \\ -3& -4& 4 & 0 \end{array}\right] & \\[NEWLINE][TAB]\left[\begin{array}{ccc|c} -3& -3& 3 & 0 \\ -3& -4& 4 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_3} \ -\!\!= \vec{r_2}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} -3& -3& 3 & 0 \\ 0& -1& 1 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ -\!\!= \vec{r_1}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 1& 1& -1 & 0 \\ 0& 1& -1 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \begin{array}{l} \vec{r_1} \ \times\!\!= -\frac{1}{3} \\ \vec{r_2} \ \times\!\!= -1 \end{array}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 1& 0& 0 & 0 \\ 0& 1& -1 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_1} \ -\!\!= \vec{r_2}[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{ccc|c} -3& -3& 3 & 0 \\ -3& -4& 4 & 0 \\ -3& -4& 4 & 0 \end{array}\right] & \\[4px]\left[\begin{array}{ccc|c} -3& -3& 3 & 0 \\ -3& -4& 4 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_3} \ -\!\!= \vec{r_2}\\[4px]\left[\begin{array}{ccc|c} -3& -3& 3 & 0 \\ 0& -1& 1 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ -\!\!= \vec{r_1}\\[4px]\left[\begin{array}{ccc|c} 1& 1& -1 & 0 \\ 0& 1& -1 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \begin{array}{l} \vec{r_1} \ \times\!\!= -\frac{1}{3} \\ \vec{r_2} \ \times\!\!= -1 \end{array}\\[4px]\left[\begin{array}{ccc|c} 1& 0& 0 & 0 \\ 0& 1& -1 & 0 \\ 0& 0& 0 & 0 \end{array}\right] & \quad \vec{r_1} \ -\!\!= \vec{r_2}\end{align*}$$</span></p><p class="body-text">Our solution here is</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v_2} = \left[\begin{array}{c} 0 \\ 1 \\ 1 \end{array}\right].[NEWLINE]$$">$$\begin{align*}\vec{v_2} = \left[\begin{array}{c} 0 \\ 1 \\ 1 \end{array}\right].\end{align*}$$</span></p><p class="body-text">There are only two eigenvectors for <span class="tex-holder inline-math" data-source-tex="A">$A$</span>, and so <span class="tex-holder inline-math" data-source-tex="A">$A$</span> isn&#x2019;t diagonalizable. So what went wrong? The repeated eigenvalue of <span class="tex-holder inline-math" data-source-tex="\lambda = 2">$\lambda = 2$</span> feels like it might be the culprit: since it appeared as a root of the characteristic polynomial with multiplicity 2, we&#x2019;d expect there to be two linearly independent eigenvectors corresponding to it. Let&#x2019;s dig into that a little more and see if we can find exactly where our expectation deviates from reality.</p><p class="body-text">Suppose <span class="tex-holder inline-math" data-source-tex="\vec{v_1}, ..., \vec{v_k}">$\vec{v_1}, ..., \vec{v_k}$</span> are all the eigenvectors of an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix <span class="tex-holder inline-math" data-source-tex="A">$A$</span> with the eigenvalue <span class="tex-holder inline-math" data-source-tex="\lambda_1">$\lambda_1$</span>. Then we can form a basis for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$</span> by extending <span class="tex-holder inline-math" data-source-tex="\{ \vec{v_1}, ..., \vec{v_k} \}">$\{ \vec{v_1}, ..., \vec{v_k} \}$</span> to</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\{ \vec{v_1}, ..., \vec{v_k}, \vec{v_{k+1}}, ..., \vec{v_n} \}.[NEWLINE]$$">$$\begin{align*}\{ \vec{v_1}, ..., \vec{v_k}, \vec{v_{k+1}}, ..., \vec{v_n} \}.\end{align*}$$</span></p><p class="body-text">If <span class="tex-holder inline-math" data-source-tex="B">$B$</span> is the matrix with these <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$</span> as columns (i.e. the change-of-basis matrix from this basis to the standard one), then</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A = B \left[\begin{array}{c|c} \lambda_1 I & C_1 \\ \hline 0& C_2 \end{array}\right] B^{-1}.[NEWLINE]\end{align*}">$$\begin{align*}A = B \left[\begin{array}{c|c} \lambda_1 I & C_1 \\ \hline 0& C_2 \end{array}\right] B^{-1}.\end{align*}$$</span></p><p class="body-text">This is a matrix written in <em>block</em> form: in the top left is a <span class="tex-holder inline-math" data-source-tex="k \times k">$k \times k$</span> block with <span class="tex-holder inline-math" data-source-tex="\lambda_1">$\lambda_1$</span> down the diagonal, in the bottom right is an <span class="tex-holder inline-math" data-source-tex="(n - k) \times (n - k)">$(n - k) \times (n - k)$</span> block called <span class="tex-holder inline-math" data-source-tex="C_2">$C_2$</span> with arbitrary entries, and similarly for the other two blocks. What&#x2019;s important is that the bottom-left block is all zeros, since <span class="tex-holder inline-math" data-source-tex="A\vec{v_i} = \lambda_1\vec{v_i}">$A\vec{v_i} = \lambda_1\vec{v_i}$</span> for all <span class="tex-holder inline-math" data-source-tex="i">$i$</span> with <span class="tex-holder inline-math" data-source-tex="1 \leq i \leq k">$1 \leq i \leq k$</span>. If we take the characteristic polynomial of both sides, we find</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\chi_A(\lambda) &= \det \left( B \left[\begin{array}{c|c} \lambda_1 I & C_1 \\ \hline 0& C_2 \end{array}\right] B^{-1} - \lambda I \right)\\[NEWLINE][TAB]&= \det \left( B \left[\begin{array}{c|c} \lambda_1 I & C_1 \\ \hline 0& C_2 \end{array}\right] B^{-1} - \lambda (BIB^{-1}) \right)\\[NEWLINE][TAB]&= \det \left( B \left( \left[\begin{array}{c|c} \lambda_1 I & C_1 \\ \hline 0& C_2 \end{array}\right] - \lambda I \right) B^{-1} \right)\\[NEWLINE][TAB]&= \det B \det \left( \left[\begin{array}{c|c} \lambda_1 I & C_1 \\ \hline 0& C_2 \end{array}\right] - \lambda I \right) \det B^{-1}\\[NEWLINE][TAB]&= \det \left( \left[\begin{array}{c|c} \lambda_1 I & C_1 \\ \hline 0& C_2 \end{array}\right] - \lambda I \right).\\[NEWLINE][TAB]&= (\lambda_1 - \lambda)^k \chi_{C_2}(\lambda).[NEWLINE]\end{align*}">$$\begin{align*}\chi_A(\lambda) &= \det \left( B \left[\begin{array}{c|c} \lambda_1 I & C_1 \\ \hline 0& C_2 \end{array}\right] B^{-1} - \lambda I \right)\\[4px]&= \det \left( B \left[\begin{array}{c|c} \lambda_1 I & C_1 \\ \hline 0& C_2 \end{array}\right] B^{-1} - \lambda (BIB^{-1}) \right)\\[4px]&= \det \left( B \left( \left[\begin{array}{c|c} \lambda_1 I & C_1 \\ \hline 0& C_2 \end{array}\right] - \lambda I \right) B^{-1} \right)\\[4px]&= \det B \det \left( \left[\begin{array}{c|c} \lambda_1 I & C_1 \\ \hline 0& C_2 \end{array}\right] - \lambda I \right) \det B^{-1}\\[4px]&= \det \left( \left[\begin{array}{c|c} \lambda_1 I & C_1 \\ \hline 0& C_2 \end{array}\right] - \lambda I \right).\\[4px]&= (\lambda_1 - \lambda)^k \chi_{C_2}(\lambda).\end{align*}$$</span></p><p class="body-text">So, what has all this work told us? If a matrix has at least <span class="tex-holder inline-math" data-source-tex="k">$k$</span> linearly independent eigenvectors with eigenvalue <span class="tex-holder inline-math" data-source-tex="\lambda_1">$\lambda_1$</span>, then <span class="tex-holder inline-math" data-source-tex="\lambda_1">$\lambda_1$</span> appears as a root of the characteristic polynomial <span class="tex-holder inline-math" data-source-tex="\chi_A(\lambda)">$\chi_A(\lambda)$</span> with multiplicity <em>at least</em> <span class="tex-holder inline-math" data-source-tex="k">$k$</span>, but possibly more. Let&#x2019;s give these two multiplicities names to properly distinguish them.</p><div class="notes-def notes-environment"><p class="body-text"</p><span class="notes-def-title">Definition: algebraic and geometric multiplicity</span></p><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix. The <strong>algebraic multiplicity</strong> of an eigenvalue <span class="tex-holder inline-math" data-source-tex="\lambda_i">$\lambda_i$</span> of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is the power of the factor <span class="tex-holder inline-math" data-source-tex="(\lambda_i - \lambda)">$(\lambda_i - \lambda)$</span> in <span class="tex-holder inline-math" data-source-tex="\chi_A(\lambda)">$\chi_A(\lambda)$</span>. The <strong>eigenspace</strong> <span class="tex-holder inline-math" data-source-tex="E_i">$E_i$</span> corresponding to <span class="tex-holder inline-math" data-source-tex="\lambda_i">$\lambda_i$</span> is the subspace of <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$</span> of eigenvectors <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> with eigenvalue <span class="tex-holder inline-math" data-source-tex="\lambda_i">$\lambda_i$</span>, and the <strong>geometric multiplicity</strong> of <span class="tex-holder inline-math" data-source-tex="\lambda_i">$\lambda_i$</span> is <span class="tex-holder inline-math" data-source-tex="\dim E_i">$\dim E_i$</span>.</p><p class="body-text">The sum of the algebraic multiplicities of the eigenvalues of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is always equal to <span class="tex-holder inline-math" data-source-tex="n">$n$</span> (since <span class="tex-holder inline-math" data-source-tex="\deg \chi_A(\lambda) = n">$\deg \chi_A(\lambda) = n$</span>), but the sum of the geometric multiplicities is equal to <span class="tex-holder inline-math" data-source-tex="n">$n$</span> only if <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is diagonalizable (and vice versa). Moreover, the geometric multiplicity of a <span class="tex-holder inline-math" data-source-tex="\lambda_i">$\lambda_i$</span> is at least 1 and no more than its algebraic multiplicity.</p></div><div class="notes-exc notes-environment"><p class="body-text"</p><span class="notes-exc-title">Exercise: eigenspaces</span></p><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix and let <span class="tex-holder inline-math" data-source-tex="\lambda_1">$\lambda_1$</span> be an eigenvalue of <span class="tex-holder inline-math" data-source-tex="A">$A$</span>. Show that the eigenspace <span class="tex-holder inline-math" data-source-tex="E_1">$E_1$</span> is actually a subspace of <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$</span>.</p></div><div class="notes-ex notes-environment"><p class="body-text"</p><span class="notes-ex-title">Example: algebraic and geometric multiplicity</span></p><p class="body-text">Consider the matrices</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A_1 = \left[\begin{array}{ccc} 3& 0& 0 \\ 0& 5& 0 \\ 0& 0& 5 \end{array}\right], \quad A_2 = \left[\begin{array}{ccc} 3& 0& 0 \\ 0& 5& 1 \\ 0& 0& 5 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}A_1 = \left[\begin{array}{ccc} 3& 0& 0 \\ 0& 5& 0 \\ 0& 0& 5 \end{array}\right], \quad A_2 = \left[\begin{array}{ccc} 3& 0& 0 \\ 0& 5& 1 \\ 0& 0& 5 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Both have characteristic polynomials of <span class="tex-holder inline-math" data-source-tex="(3 - \lambda)(5 - \lambda)^2">$(3 - \lambda)(5 - \lambda)^2$</span>, so their eigenvalues are <span class="tex-holder inline-math" data-source-tex="\lambda_1 = 3">$\lambda_1 = 3$</span> and <span class="tex-holder inline-math" data-source-tex="\lambda_2 = 5">$\lambda_2 = 5$</span>, with algebraic multiplicities 1 and 2, respectively. The previous definition tells us that the geometric multiplicity of <span class="tex-holder inline-math" data-source-tex="\lambda_1">$\lambda_1$</span> should be 1 for both <span class="tex-holder inline-math" data-source-tex="A_1">$A_1$</span> and <span class="tex-holder inline-math" data-source-tex="A_2">$A_2$</span>, but that the geometric multiplicity of <span class="tex-holder inline-math" data-source-tex="\lambda_2">$\lambda_2$</span> will either be 1 or 2. Solving for the eigenvectors of <span class="tex-holder inline-math" data-source-tex="A_1">$A_1$</span>, we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{c|c} A_1 - 3I & \vec{0} \end{array}\right] &= \left[\begin{array}{ccc|c} 0& 0& 0 & 0 \\ 0& 2& 0 & 0 \\ 0& 0& 2 & 0 \end{array}\right]\\[NEWLINE][TAB]\left[\begin{array}{c|c} A_1 - 5I & \vec{0} \end{array}\right] &= \left[\begin{array}{ccc|c} -2& 0& 0 & 0 \\ 0& 0& 0 & 0 \\ 0& 0& 0 & 0 \end{array}\right],[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{c|c} A_1 - 3I & \vec{0} \end{array}\right] &= \left[\begin{array}{ccc|c} 0& 0& 0 & 0 \\ 0& 2& 0 & 0 \\ 0& 0& 2 & 0 \end{array}\right]\\[4px]\left[\begin{array}{c|c} A_1 - 5I & \vec{0} \end{array}\right] &= \left[\begin{array}{ccc|c} -2& 0& 0 & 0 \\ 0& 0& 0 & 0 \\ 0& 0& 0 & 0 \end{array}\right],\end{align*}$$</span></p><p class="body-text">so the eigenspaces <span class="tex-holder inline-math" data-source-tex="E_1">$E_1$</span> and <span class="tex-holder inline-math" data-source-tex="E_2">$E_2$</span> are</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]E_1 &= \operatorname{span}\left\{ \left[\begin{array}{c} 1 \\ 0 \\ 0 \end{array}\right] \right\}\\[NEWLINE][TAB]E_2 &= \operatorname{span}\left\{ \left[\begin{array}{c} 0 \\ 1 \\ 0 \end{array}\right], \left[\begin{array}{c} 0 \\ 0 \\ 1 \end{array}\right] \right\},[NEWLINE]\end{align*}">$$\begin{align*}E_1 &= \operatorname{span}\left\{ \left[\begin{array}{c} 1 \\ 0 \\ 0 \end{array}\right] \right\}\\[4px]E_2 &= \operatorname{span}\left\{ \left[\begin{array}{c} 0 \\ 1 \\ 0 \end{array}\right], \left[\begin{array}{c} 0 \\ 0 \\ 1 \end{array}\right] \right\},\end{align*}$$</span></p><p class="body-text">meaning the geometric multiplicities of both <span class="tex-holder inline-math" data-source-tex="\lambda_1">$\lambda_1$</span> and <span class="tex-holder inline-math" data-source-tex="\lambda_2">$\lambda_2$</span> are equal to their algebraic multiplicities. That means <span class="tex-holder inline-math" data-source-tex="A_1">$A_1$</span> is diagonalizable, which is pretty unsurprising since it&#x2019;s already diagonal. In contrast, <span class="tex-holder inline-math" data-source-tex="A_2">$A_2$</span> fails to be diagonalizable:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{c|c} A_2 - 3I & \vec{0} \end{array}\right] &= \left[\begin{array}{ccc|c} 0& 0& 0 & 0 \\ 0& 2& 1 & 0 \\ 0& 0& 2 & 0 \end{array}\right]\\[NEWLINE][TAB]\left[\begin{array}{c|c} A_2 - 5I & \vec{0} \end{array}\right] &= \left[\begin{array}{ccc|c} -2& 0& 0 & 0 \\ 0& 0& 1 & 0 \\ 0& 0& 0 & 0 \end{array}\right],[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{c|c} A_2 - 3I & \vec{0} \end{array}\right] &= \left[\begin{array}{ccc|c} 0& 0& 0 & 0 \\ 0& 2& 1 & 0 \\ 0& 0& 2 & 0 \end{array}\right]\\[4px]\left[\begin{array}{c|c} A_2 - 5I & \vec{0} \end{array}\right] &= \left[\begin{array}{ccc|c} -2& 0& 0 & 0 \\ 0& 0& 1 & 0 \\ 0& 0& 0 & 0 \end{array}\right],\end{align*}$$</span></p><p class="body-text">so the eigenspaces are</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]E_1 &= \operatorname{span}\left\{ \left[\begin{array}{c} 1 \\ 0 \\ 0 \end{array}\right] \right\}\\[NEWLINE][TAB]E_2 &= \operatorname{span}\left\{ \left[\begin{array}{c} 0 \\ 1 \\ 0 \end{array}\right] \right\},[NEWLINE]\end{align*}">$$\begin{align*}E_1 &= \operatorname{span}\left\{ \left[\begin{array}{c} 1 \\ 0 \\ 0 \end{array}\right] \right\}\\[4px]E_2 &= \operatorname{span}\left\{ \left[\begin{array}{c} 0 \\ 1 \\ 0 \end{array}\right] \right\},\end{align*}$$</span></p><p class="body-text">so both eigenvalues have geometric multiplicities of 1. Later in the course, we&#x2019;ll develop a sense in <span class="tex-holder inline-math" data-source-tex="A_2">$A_2$</span> is &#x201C;almost&#x201D; diagonal, or at least the closest we can get, but for now, it&#x2019;s a black-and-white distinction.</p></div><div class="text-buttons nav-buttons"><div class="focus-on-child tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div></section></main>