### nav-buttons

The matrix equation $A\vec{x} = \vec{b}$ isn't always solvable, but when there's no solution, we can still use the least-squares methods of section 5 to find a best approximation. In this section, we'll take a similar approach to diagonalization: if a square matrix $A$ isn't diagonalizable, it's because some eigenvalue $\lambda$ has algebraic multiplicity of $k > 1$, but fewer than $k$ linearly independent eigenvectors with eigenvalue $\lambda$. For example,

$$
	A = [[ -3, 9 ; -4, 9 ]]
$$

has characteristic polynomial $(\lambda - 3)^2$, so the only eigenvalue is $\lambda = 3$. Solving for the eigenvectors, we only have one:

$$
	\vec{v_1} = [[ 3 ; 2 ]],
$$

meaning $A$ is nondiagonalizable. There may not be a basis of eigenvectors, but let's see if we can still find some basis that's helpful. We'd definitely like to use the vector $\vec{v_1}$ as one of the two vectors in this prospective basis, and since there's no linearly independent vector $\vec{v}$ with $(A - 3I)\vec{v} = \vec{0}$, the next-best thing to hope for is that there's one so that $(A - 3I)\vec{v} = \vec{v_1}$. Checking that, we have

$$
	[[ -6, 9 | 3 ; -4, 6 | 2 ]] &

	[[ 2, -3 | -1 ; 2, -3 | -1 ]] & \quad :: \vec{r_1} \te -\frac{1}{3} ; \vec{r_2} \te -\frac{1}{2} ::

	[[ 2, 3 | -1 ; 0, 0 | 0 ]] & \quad \vec{r_2} \me 2\vec{r_1}
$$

In total, we have a solution of

$$
	[[ -1 - 3t ; 2t ]],
$$

so with $t = 0$,

$$
	\vec{v_2} = [[ -1 ; 0 ]].
$$

With the basis $\left\{ \vec{v_1}, \vec{v_2} \right\}$, the matrix representation of $A$ isn't so bad! Specifically, $A = BDB^{-1}$, where

$$
	B = [[ 3, -1 ; 2, 0 ]] \qquad D = [[ 3, 1 ; 0, 3 ]].
$$

Since $A$ isn't diagonalizable, something like this is as close as we're going to get. In fact, this is remarkably close to a diagonalization, with the sole exception of the $1$ off the diagonal of $D$ that means the eigenspace corresponding to $\lambda = 3$ has dimension $1$ instead of $2$. With the goal of bringing this almost-diagonalization to any square matrix, let's back up and state a few definitions that formalize what we've seen here.

### def "generalized eigenvector"

	Let $A$ be an $n \times n$ matrix and $\lambda$ an eigenvalue of $A$. A **generalized eigenvector** of $A$ with eigenvalue $\lambda$ is a vector $\vec{v}$ so that $(A - \lambda I)^k\vec{v} = \vec{0}$ for some $k \geq 1$. The **rank** of a generalized eigenvector is the smallest $k$ for which this holds.

###

Let's connect this back to what we've seen. Rank 1 generalized eigenvectors are just regular eigenvectors, and for rank 2 ones, we have $(A - \lambda I)^k\vec{v} = \vec{0}$ for $k = 2$ but not $k = 1$ --- or in other words,

$$
	(A - \lambda I)\vec{v} \in \ker (A - \lambda I)
$$

but $(A - \lambda I)\vec{v} \neq \vec{0}$, which is exactly what it means for $(A - \lambda I)\vec{v}$ to be an eigenvector of $A$ with eigenvalue $\lambda$. Similarly, the rank 3 generalized eigenvectors are those which are taken to rank 2 ones when plugged into $A - \lambda I$.

### thm "f"

### nav-buttons