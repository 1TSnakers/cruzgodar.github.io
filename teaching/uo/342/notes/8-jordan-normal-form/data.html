<header><div id="logo"><a href="/home/" tabindex="-1"><img src="/graphics/general-icons/logo.png" alt="Logo" tabindex="1"></img></a></div><div style="height: 20px"></div><h1 class="heading-text">Section 8: Jordan Normal Form</h1></header><main><section><div class="text-buttons nav-buttons"><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div><p class="body-text">The matrix equation <span class="tex-holder inline-math" data-source-tex="A\vec{x} = \vec{b}">$A\vec{x} = \vec{b}$</span> isn&#x2019;t always solvable, but when there&#x2019;s no solution, we can still use the least-squares methods of section 5 to find a best approximation. In this section, we&#x2019;ll take a similar approach to diagonalization: if a square matrix <span class="tex-holder inline-math" data-source-tex="A">$A$</span> isn&#x2019;t diagonalizable, it&#x2019;s because some eigenvalue <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$</span> has algebraic multiplicity of <span class="tex-holder inline-math" data-source-tex="k > 1">$k > 1$,</span> but fewer than <span class="tex-holder inline-math" data-source-tex="k">$k$</span> linearly independent eigenvectors with eigenvalue <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$.</span> For example,</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A = \left[\begin{array}{cc} -3& 9 \\ -4& 9 \end{array}\right][NEWLINE]\end{align*}">$$\begin{align*}A = \left[\begin{array}{cc} -3& 9 \\ -4& 9 \end{array}\right]\end{align*}$$</span></p><p class="body-text">has characteristic polynomial <span class="tex-holder inline-math" data-source-tex="(\lambda - 3)^2">$(\lambda - 3)^2$,</span> so the only eigenvalue is <span class="tex-holder inline-math" data-source-tex="\lambda = 3">$\lambda = 3$.</span> Solving for the eigenvectors, we only have one:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v_1} = \left[\begin{array}{c} 3 \\ 2 \end{array}\right],[NEWLINE]$$">$$\begin{align*}\vec{v_1} = \left[\begin{array}{c} 3 \\ 2 \end{array}\right],\end{align*}$$</span></p><p class="body-text">meaning <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is nondiagonalizable. There may not be a basis of eigenvectors, but let&#x2019;s see if we can still find some basis that&#x2019;s helpful. We&#x2019;d definitely like to use the vector <span class="tex-holder inline-math" data-source-tex="\vec{v_1}">$\vec{v_1}$</span> as one of the two vectors in this prospective basis, and since there&#x2019;s no linearly independent vector <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> with <span class="tex-holder inline-math" data-source-tex="(A - 3I)\vec{v} = \vec{0}">$(A - 3I)\vec{v} = \vec{0}$,</span> the next-best thing to hope for is that there&#x2019;s one so that <span class="tex-holder inline-math" data-source-tex="(A - 3I)\vec{v} = \vec{v_1}">$(A - 3I)\vec{v} = \vec{v_1}$.</span> Checking that, we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{cc|c} -6& 9 & 3 \\ -4& 6 & 2 \end{array}\right] &\\[NEWLINE][TAB]\left[\begin{array}{cc|c} 2& -3 & -1 \\ 2& -3 & -1 \end{array}\right] & \quad \begin{array}{l} \vec{r_1} \ \times\!\!= -\frac{1}{3} \\ \vec{r_2} \ \times\!\!= -\frac{1}{2} \end{array}\\[NEWLINE][TAB]\left[\begin{array}{cc|c} 2& 3 & -1 \\ 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ -\!\!= 2\vec{r_1}[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{cc|c} -6& 9 & 3 \\ -4& 6 & 2 \end{array}\right] &\\[4px]\left[\begin{array}{cc|c} 2& -3 & -1 \\ 2& -3 & -1 \end{array}\right] & \quad \begin{array}{l} \vec{r_1} \ \times\!\!= -\frac{1}{3} \\ \vec{r_2} \ \times\!\!= -\frac{1}{2} \end{array}\\[4px]\left[\begin{array}{cc|c} 2& 3 & -1 \\ 0& 0 & 0 \end{array}\right] & \quad \vec{r_2} \ -\!\!= 2\vec{r_1}\end{align*}$$</span></p><p class="body-text">In total, we have a solution of</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\left[\begin{array}{c} -1 - 3t \\ 2t \end{array}\right],[NEWLINE]$$">$$\begin{align*}\left[\begin{array}{c} -1 - 3t \\ 2t \end{array}\right],\end{align*}$$</span></p><p class="body-text">so with <span class="tex-holder inline-math" data-source-tex="t = 0">$t = 0$,</span></p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v_2} = \left[\begin{array}{c} -1 \\ 0 \end{array}\right].[NEWLINE]$$">$$\begin{align*}\vec{v_2} = \left[\begin{array}{c} -1 \\ 0 \end{array}\right].\end{align*}$$</span></p><p class="body-text">With the basis <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{v_1}, \vec{v_2} \right\}">$\left\{ \vec{v_1}, \vec{v_2} \right\}$,</span> the matrix representation of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> isn&#x2019;t so bad! Specifically, <span class="tex-holder inline-math" data-source-tex="A = BDB^{-1}">$A = BDB^{-1}$,</span> where</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]B = \left[\begin{array}{cc} 3& -1 \\ 2& 0 \end{array}\right] \qquad D = \left[\begin{array}{cc} 3& 1 \\ 0& 3 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}B = \left[\begin{array}{cc} 3& -1 \\ 2& 0 \end{array}\right] \qquad D = \left[\begin{array}{cc} 3& 1 \\ 0& 3 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Since <span class="tex-holder inline-math" data-source-tex="A">$A$</span> isn&#x2019;t diagonalizable, something like this is as close as we&#x2019;re going to get. In fact, this is remarkably close to a diagonalization, with the sole exception of the <span class="tex-holder inline-math" data-source-tex="1">$1$</span> off the diagonal of <span class="tex-holder inline-math" data-source-tex="D">$D$</span> that means the eigenspace corresponding to <span class="tex-holder inline-math" data-source-tex="\lambda = 3">$\lambda = 3$</span> has dimension <span class="tex-holder inline-math" data-source-tex="1">$1$</span> instead of <span class="tex-holder inline-math" data-source-tex="2">$2$.</span> With the goal of bringing this almost-diagonalization to any square matrix, let&#x2019;s back up and state a few definitions that formalize what we&#x2019;ve seen here.</p><div class="notes-def notes-environment"><div class="notes-def-title notes-title">Definition: generalized</div><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix and <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$</span> an eigenvalue of <span class="tex-holder inline-math" data-source-tex="A">$A$.</span> A <strong>generalized eigenvector</strong> of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> with eigenvalue <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$</span> is a vector <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> so that <span class="tex-holder inline-math" data-source-tex="(A - \lambda I)^k\vec{v} = \vec{0}">$(A - \lambda I)^k\vec{v} = \vec{0}$</span> for some <span class="tex-holder inline-math" data-source-tex="k \geq 1">$k \geq 1$.</span> The <strong>rank</strong> of a generalized eigenvector is the smallest <span class="tex-holder inline-math" data-source-tex="k">$k$</span> for which this holds.</p></div><p class="body-text">Let&#x2019;s connect this back to what we&#x2019;ve seen. Rank 1 generalized eigenvectors are just regular eigenvectors, and for rank 2 ones, we have <span class="tex-holder inline-math" data-source-tex="(A - \lambda I)^k\vec{v} = \vec{0}">$(A - \lambda I)^k\vec{v} = \vec{0}$</span> for <span class="tex-holder inline-math" data-source-tex="k = 2">$k = 2$</span> but not <span class="tex-holder inline-math" data-source-tex="k = 1">$k = 1$</span> &mdash; or in other words,</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB](A - \lambda I)\vec{v} \in \ker (A - \lambda I)[NEWLINE]$$">$$\begin{align*}(A - \lambda I)\vec{v} \in \ker (A - \lambda I)\end{align*}$$</span></p><p class="body-text">but <span class="tex-holder inline-math" data-source-tex="(A - \lambda I)\vec{v} \neq \vec{0}">$(A - \lambda I)\vec{v} \neq \vec{0}$,</span> which is exactly what it means for <span class="tex-holder inline-math" data-source-tex="(A - \lambda I)\vec{v}">$(A - \lambda I)\vec{v}$</span> to be an eigenvector of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> with eigenvalue <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$.</span> Similarly, the rank 3 generalized eigenvectors are those which are taken to rank 2 ones when plugged into <span class="tex-holder inline-math" data-source-tex="A - \lambda I">$A - \lambda I$.</span></p><div class="text-buttons nav-buttons"><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div></section></main>