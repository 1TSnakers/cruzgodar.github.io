<header><div id="logo"><a href="/home/" tabindex="-1"><img src="/graphics/general-icons/logo.png" alt="Logo" tabindex="1"></img></a></div><div style="height: 20px"></div><h1 class="heading-text">Section 5: Least-Squares Approximations</h1></header><main><section><div class="text-buttons nav-buttons"><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div><p class="body-text">After the behemoth that was the last section, let&#x2019;s take the opportunity to apply linear algebra to some more concrete problems. When we solve a matrix equation <span class="tex-holder inline-math" data-source-tex="A\vec{x} = \vec{b}">$A\vec{x} = \vec{b}$</span> currently, there are three different possible outcomes: either there&#x2019;s exactly one solution for <span class="tex-holder inline-math" data-source-tex="\vec{x}">$\vec{x}$,</span> there&#x2019;s infinitely many (i.e. the solution contains one or more free parameters), or there are no solutions. Up to this point, we haven&#x2019;t been able to handle that last case beyond saying that we&#x2019;re unable to solve the system, but now that we have the language of the previous section, we&#x2019;re much better prepared to discuss it in more depth. In particular, what&#x2019;s the closest we can get? For any value of <span class="tex-holder inline-math" data-source-tex="\vec{x}">$\vec{x}$,</span> <span class="tex-holder inline-math" data-source-tex="A\vec{x}">$A\vec{x}$</span> is a linear combination of the columns of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> with weights given by the entries of <span class="tex-holder inline-math" data-source-tex="\vec{x}">$\vec{x}$,</span> so <span class="tex-holder inline-math" data-source-tex="A\vec{x} = \vec{b}">$A\vec{x} = \vec{b}$</span> is solvable exactly when <span class="tex-holder inline-math" data-source-tex="\vec{b} \in \operatorname{image} A">$\vec{b} \in \operatorname{image} A$,</span> which is just the span of the columns of <span class="tex-holder inline-math" data-source-tex="A">$A$.</span> If that&#x2019;s not the case, then the closest vector that <em>is</em> in the image is given by a projection. Specifically, the closest output is <span class="tex-holder inline-math" data-source-tex="\vec{b}'">$\vec{b}'$,</span> where</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{b}' = \operatorname{proj}_{\operatorname{image} A} \vec{b}.[NEWLINE]$$">$$\begin{align*}\vec{b}' = \operatorname{proj}_{\operatorname{image} A} \vec{b}.\end{align*}$$</span></p><p class="body-text">So how can we solve <span class="tex-holder inline-math" data-source-tex="A\vec{x}' = \vec{b}'">$A\vec{x}' = \vec{b}'$</span> for a solution <span class="tex-holder inline-math" data-source-tex="\vec{x}'">$\vec{x}'$?</span> We can&#x2019;t use the convenient projection formula from the last section without an orthonormal basis for the image of <span class="tex-holder inline-math" data-source-tex="A">$A$,</span> but we do know that the vectors <span class="tex-holder inline-math" data-source-tex="\vec{b}'">$\vec{b}'$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{b} - \vec{b}'">$\vec{b} - \vec{b}'$</span> are orthogonal because the latter necessarily lives in the orthogonal complement of <span class="tex-holder inline-math" data-source-tex="\operatorname{image} A">$\operatorname{image} A$.</span> That means that the dot product of <span class="tex-holder inline-math" data-source-tex="\vec{b} - \vec{b}'">$\vec{b} - \vec{b}'$</span> with any column of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is zero, and so</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\left( \vec{b} - \vec{b}' \right)^T A = \vec{0}^T,[NEWLINE]$$">$$\begin{align*}\left( \vec{b} - \vec{b}' \right)^T A = \vec{0}^T,\end{align*}$$</span></p><p class="body-text">since every component of that vector is one of those dot products. That&#x2019;s a very awkward way to write the product, though, and we&#x2019;re better off transposing both sides, remembering that transposing a matrix product reverses the order of the factors:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A^T \left( \vec{b} - \vec{b}' \right) &= \vec{0}\\[NEWLINE][TAB]A^T \vec{b}' &= A^T \vec{b}\\[NEWLINE][TAB]A^T A \vec{x}' &= A^T \vec{b}.[NEWLINE]\end{align*}">$$\begin{align*}A^T \left( \vec{b} - \vec{b}' \right) &= \vec{0}\\[4px]A^T \vec{b}' &= A^T \vec{b}\\[4px]A^T A \vec{x}' &= A^T \vec{b}.\end{align*}$$</span></p><p class="body-text">And now this is a standard matrix equation we can solve for <span class="tex-holder inline-math" data-source-tex="\vec{x}'">$\vec{x}'$!</span> The moral here is that if <span class="tex-holder inline-math" data-source-tex="A\vec{x} = \vec{b}">$A\vec{x} = \vec{b}$</span> is unsolvable, multiplying both sides by <span class="tex-holder inline-math" data-source-tex="A^T">$A^T$</span> produces a solvable equation whose solution is the best approximation to <span class="tex-holder inline-math" data-source-tex="\vec{b}">$\vec{b}$.</span> Since &#x201C;best&#x201D; means minimizing <span class="tex-holder inline-math" data-source-tex="\left| \left| \vec{b}' - \vec{b} \right| \right|">$\left| \left| \vec{b}' - \vec{b} \right| \right|$,</span> or equivalently minimizing</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\left| \left| \vec{b}' - \vec{b} \right| \right|^2 = \left( \vec{b}' - \vec{b} \right)_1^2 + \cdots + \left( \vec{b}' - \vec{b} \right)_n^2,[NEWLINE]$$">$$\begin{align*}\left| \left| \vec{b}' - \vec{b} \right| \right|^2 = \left( \vec{b}' - \vec{b} \right)_1^2 + \cdots + \left( \vec{b}' - \vec{b} \right)_n^2,\end{align*}$$</span></p><p class="body-text">these approximation problems are called <strong>least-squares problems</strong>.</p><div class="text-buttons nav-buttons"><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div></section></main>