
### nav-buttons

Welcome to Math 342! You're viewing the interactive lecture notes --- reading these is required for the class, since we'll have a short reading quiz on Canvas due before most lectures. To get started, let's make sure your browser handles equations and graphs correctly. You should see an equation on its own line below and a graph below that.

$$
	\dim V = \dim \ker T + \dim \image T
$$

### desmos test-graph

If anything doesn't appear correctly, update your browser and then reach out if that doesn't work.

With linear algebra I behind us, we can take a high level view and summarize the whole of the course. Linear algebra is the study of **vector spaces** and maps (i.e. functions) between them, called **linear transformations**. The definition of a vector space is slightly technical, but let's repeat it here for completeness.

### def vector space

	A **vector space** is a set $V$, a method $+$ that can add any two elements $v, w \in V$, and a method $\cdot$ that can multiply any vector $v \in V$ by any real number $c$, such that

	1. $V$ is **closed** under addition and scalar multiplication: $v + w \in V$ for any $v, w \in V$, and $cv \in V$ for any $v \in V$ and $c \in #R#$.

	2. Addition is associative and commutative: $(u + v) + w = u + (v + w)$ and $v + w = w + v$ for any $u, v, w \in #R#$.

	3. Multiplication is associative and distributive: $(cd)v = c(dv)$, $(c + d)v = cv + dv$, and $c(v + w) = cv + cw$.

	4. There is a **zero vector** $0_V$ so that $v + 0_V = v$ for all $v \in V$.

	5. Given any $v \in V$, there is a vector $-v$ with $v + (-v) = 0_V$.

	6. For any $v \in V$, $1v = v$.

###

Intuitively, a vector space is a set of elements that can be added together and scaled by real numbers without leaving the set, and the addition and multiplication has most of the properties we'd expect. In practice, we usually only need to show that a set $X$ is a **subspace** of a larger set $V$ that we already know is a vector space. That only requires showing that $X$ is closed under addition and scalar multiplication and contains the zero vector --- all of the other properties are inherited from $V$ since the addition and multiplication are the same.

### ex a subspace
	
	Let $V$ be the vector space $C^0(#R#)$ consisting of continuous functions $f: #R# \to #R#$ and let $X$ be the set of functions $f$ such that $f(1) = 0$. Show that $X$ is a subspace of $V$.
	
	To show closure under addition and multiplication at once, we can show that $cv + w \in X$ for any $v, w \in X$ and $c \in #R#$. If $f, g \in X$ are functions and $c \in #R#$, then
	
	$$
		(cf + g)(1) &= cf(1) + g(1)
		
		&= c(0) + 0
		
		&= 0.
	$$
	
	That verifies that $X$ is closed under addition, and since the zero vector in $C^0(#R#)$ is the zero function $z : #R# \to #R#$ defined by $z(x) = 0$, $z \in X$ since $z(1) = 0$. Therefore, $X$ is a subspace.
	
###

It's worth going back over the notation for some of the most common vector spaces, which usually contain literal column vectors, matrices, or functions.

> . $#R#$: the vector space of all real numbers.

> . $#R#^n$: the vector space of length-$n$ column vectors.

> . $#R#^#R#$: the vector space of all functions $f : #R# \to #R#$.

> . $C^0(#R#)$: the vector space of continuous functions $f : #R# \to #R#$.

> . $#R#[x]$: the vector space of polynomials in $x$ with coefficients in $#R#$.

> . $M_{m \times n}(#R#)$: the vector space of $m \times n$ matrices with entries in $#R#$.

> . $\mathcal{L}(V, W)$: the vector space of linear transformations $T : V \to W$.

The last entry in the list deserves some explanation. Given two vector spaces $V$ and $W$, a **linear transformation** $T : V \to W$ is a function that preserves the structure of the vector spaces --- specifically, $T(v + w) = T(v) + T(w)$ and $T(cv) = cT(v)$ for any $v, w \in V$ and $c \in #R#$. When $V = #R#^n$ and $W = #R#^m$, we can represent any linear transformation $T : V \to W$ by an $m \times n$ **matrix** $A$. We find the $j$th column by evaluating $T(e_j)$, where $e_j \in #R#^n$ is the column vector consisting of all zeros except a $1$ in position $j$. Then $T(v) = Av$ for any vector $v \in #R#^n$.

### exc the matrix of a linear transformation
	
	Let $T : #R#^3 \to #R#^2$ be defined by
	
	$$
		T\left( [[ x ; y ; z ]] \right) = [[ 2x + y - z ; x + z ]].
	$$
	
	Find the matrix for $T$ and use it to evaluate $T\left( [[ 1 ; 2 ; 3 ]] \right)$.
	
###

The ability to break a column vector in $#R#^n$ into a linear combination of the various $e_i$ is what allows us to write matrices so cleanly and understand linear transformations so well, and it's something we'd have great use for in more abstract vector spaces as well. To that end, we have **bases**, which are collections of linearly independent and spanning vectors in a vector space. If $V$ is the vector space of polynomials with degree at most $3$, then a basis for $V$ is the collection $\{1, x, x^2, x^3\}$ --- they're all linearly independent, since the only linear combination $c_0 + c_1x + c_2x^2 + c_3x^3 = 0$ is $c_0 = c_1 = c_2 = c_3 = 0$, and they span $V$, since any polynomial in $V$ is a linear combination of them by definition. While bases for a vector space aren't unique, their length is, and the **dimension** of a vector space $V$, written $\dim V$, is the number of elements in a basis for $V$. For example, $\dim #R#^n = n$, $\dim M_{m \times n} = mn$, and $\dim \mathcal{L}(V, W) = (\dim V)(\dim W)$ for finite-dimensional spaces $V$ and $W$.

With a basis $\mathcal{B} = \{v_1, ..., v_n\}$ for a vector space $V$, we can uniquely represent any vector $v \in V$ as a linear combination of the elements of $\mathcal{B}$ --- this is called **expressing** $v$ in $\mathcal{B}$. To concisely store the expression of $v$ in $\mathcal{B}$, we can write $[v]_\mathcal{B}$, the **coordinate vector** of $v$ in $\mathcal{B}$, by taking the unique expression

$$
	v = c_1v_1 + \cdots + c_nv_n
$$

and writing

$$
	[v]_\mathcal{B} = [[ c_1 ; \vdots ; c_n ]].
$$

Many vector spaces have what is commonly referred to as a **standard basis**, which is a basis makes vectors particularly easy to express, and is usually the default choice for a basis. The standard basis for $#R#^n$ is the set $\{e_1, ..., e_n\}$, where $e_i$ is the vector of all zeros and a 1 in position $i$, as before. The standard basis for the vector space of polynomials of degree at most $n$ is $\{1, x, x^2, ..., x^n\}$, and the standard basis for $M_{m \times n}(#R#)$ is the collection of $mn$ matrices that are all zeros except for a single 1, similar to the standard basis for $#R#^n$. The standard basis for $\mathcal{L}(#R#^n, #R#^m)$ is very similar, and it just inherits directly from the standard basis for $M_{m \times n}(#R#)$ by interpreting the matrices as linear transformations.

The process of changing the basis of a coordinate vector --- effectively converting an expresion in one basis to an expression in another --- is itself a linear transformation, and so it has a matrix representation. Given a basis $\mathcal{B} = \{v_1, ..., v_n\}$ for a vector space $V$, we can form the **change-of-basis matrix** $B$ that converts from $\mathcal{B}$ to $\mathcal{E}$, the standard basis, by finding $[v_i]_\mathcal{E}$ for each $v_i$ and placing them as columns in a matrix. While $B$ isn't particularly helpful --- we typically don't need the machinery of a matrix to help us express a vector in the standard basis --- the inverse matrix $B^{-1}$ is quite useful. It satisfies $B^{-1}[v]_\mathcal{E} = [v]_\mathcal{B}$ for any vector $v \in V$, and so we can freely convert from the standard basis to $\mathcal{B}$. 

Coordinate vectors even let us get back to matrices for linear transformations. Suppose $T : V \to W$ for finite-dimensional vector spaces $V$ and $W$ and we choose bases $\mathcal{B} = \{v_1, ..., v_n\}$ for $V$ and $\mathcal{C} = \{w_1, ..., w_m\}$ for $W$. Then the matrix $A$ for $T$ with respect to those two bases is formed by evaluating $T(v_j)$ for every $v_j \in \mathcal{B}$ and placing $[T(v_j)]_\mathcal{C}$ in the $j$th column, and it satisfies $[T(v)]_\mathcal{C} = A[v]_\mathcal{B}$ for any vector $v \in V$. In other words, $T$ acts like a matrix / linear transformation from $#R#^n$ to $#R#^m$ as long as we represent both the input and output as column vectors.

In practice, it's often a little cumbersome to find a matrix with respect to a different basis, particularly when the basis for the codomain is complicated enough that expressing vectors in it is difficult without constructing a change-of-basis matrix. Instead, we can do the same work in pieces: if $T : V \to W$ is a linear transformation and $\mathcal{B}$ and $\mathcal{C}$ are bases for $V$ and $W$ and we'd like to find the matrix for $T$ with respect to $V$ and $W$, we can first find the matrix $A$ for $T$ with respect to the *standard* bases for $V$ and $W$, a task which is usually much simpler. Then we can find the change-of-basis matrices $B$ and $C$ that convert from $\mathcal{B}$ and $\mathcal{C}$ to the respective standard bases for $V$ and $W$, and then simply take the product $C^{-1}AB$. The final effect is that if we take a vector $v \in V$ and express it in the basis $\mathcal{B}$ to produce a coordinate vector $[v]_\mathcal{B}$, then $B[v]_\mathcal{B} = [v]_\mathcal{E}$, the expression in the standard basis for $V$. Next, $A[v]_\mathcal{E} = [T(v)]_\mathcal{E}$, the coordinate vector of $[T(v)]$ in the standard basis for $W$, and finally $C^{-1}[T(v)]_\mathcal{E} = [T(v)]_\mathcal{C}$. In the particular case when $V = W$, this operation of changing basis by matrix multiplication simplifies to $B^{-1}AB$, and we call this **conjugating** $A$ by $B$.

### ex the matrix for a linar transformation
	
	Let $V = M_{2 \times 2}(#R#)$ and $W = \span\{1, x\}$, with bases
	
	$$
		\mathcal{B} = \left\{ [[ 1, 0 ; 0, 1 ]], [[ 1, 0 ; 0, -1 ]], [[ 0, 1 ; 1, 0 ]], [[ 0, 1 ; -1, 0 ]] \right\}
	$$
	
	and $\mathcal{C} = \{ 1 + x, 2 + 3x \}$. With $T : V \to W$ defined by
	
	$$
		T\left( [[ a, b ; c, d ]] \right) = a + b + c + dx,
	$$
	
	find the matrix for $T$ with respect to these two bases.
	
	The matrix $A$ for $T$ with respect to the standard basis is given by evaluating $T$ on each of the vectors in the standard basis for $V = M_{2 \times 2}(#R#)$ and writing them as coordinate vectors in the standard basis for $W$ (i.e. $\{1, x\}$. We have
	
	$$
		T\left( [[ 1, 0 ; 0, 0 ]] \right) = 1,
	$$
	
	which has a coordinate vector of $[[ 1 ; 0 ]]$ in the standard basis for $W$, so that becomes the first column in the matrix $A$. Continuing with the rest, we have
	
	$$
		A = [[ 1, 1, 1, 0 ; 0, 0, 0, 1 ]].
	$$
	
	To find the change-of-basis matrix $B$ from $\mathcal{B}$ to $\mathcal{E}$, we write each of the vectors in $\mathcal{B}$ as a coordinate vector in $\mathcal{E}$ and place them as columns, resulting in
	
	$$
		B = [[ 1, 1, 0, 0 ; 0, 0, 1, 1 ; 0, 0, 1, -1 ; 1, -1, 0, 0 ]].
	$$
	
	Similarly,
	
	$$
		C = [[ 1, 2 ; 1, 3 ]].
	$$
	
	However, we want $C^{-1}$. To find that, we augment $C$ with the $2 \times 2$ identity matrix and row reduce:
	
	$$
		[[ 1, 2 | 1, 0 ; 1, 3 | 0, 1 ]]
		
		[[ 1, 2 | 1, 0 ; 0, 1 | -1, 1 ]] & \qquad r_2 \me r_1
		
		[[ 1, 0 | 3, -2 ; 0, 1 | -1, 1 ]] & \qquad r_1 \me 2r_2
	$$
	
	Our inverse matrix $C^{-1}$ is therefore $[[ 3, -2 ; -1, 1 ]]$, and so our total matrix for $T$ is
	
	$$
		C^{-1}AB &= [[ 3, -2 ; -1, 1 ]][[ 1, 1, 1, 0 ; 0, 0, 0, 1 ]][[ 1, 1, 0, 0 ; 0, 0, 1, 1 ; 0, 0, 1, -1 ; 1, -1, 0, 0 ]]
		
		&= [[ 3, -2 ; -1, 1 ]][[ 1, 1, 2, 0 ; 1, -1, 0, 0 ]]
		
		&= [[ 1, 5, 6, 0 ; 0, -2, -2, 0 ]].
	$$
	
	For example, this tells us that
	
	$$
		T\left([[ 1, 0 ; 0, -1 ]]\right) &= 5(1 + x) -2(2 + 3x)
		
		&= 1 - x,
	$$
	
	as expected.
	
###

One final but crucial result from linear algebra I is the **fundamental theorem of linear algebra**, which we'll see earn its name throughout this course. To begin with, we need two brief definitions for a linear transformation. The **kernel** of a linear transformation $T : V \to W$, written $\ker T$, is the space of vectors $v \in V$ for which $T(v) = 0$, and the **image** of $T$, written $\image T$, is the space of vectors $w \in W$ with $w = T(v)$ for some $v \in V$ (i.e. the space of outputs of $T$ that actually occur). With these definitions in place, we can state the fundamental theorem.

### thm The Fundamental Theorem of Linear Algebra
	
	Let $V$ and $W$ be vector spaces, where $V$ is finite-dimensional, and let $T : V \to W$ be a linear transformation. Then
	
	$$
		\dim V = \dim \ker T + \dim \image T.
	$$
	
	Specifically, if $\{v_1, ..., v_k\}$ is a basis for $\ker T$ and we extend it to a basis
	
	$$
		\{v_1, ..., v_k, v_{k + 1}, ..., v_n\}
	$$
	
	for $V$, then $\{T(v_{k + 1}), ..., T(v_n)\}$ is a basis for $\image T$.
	
###

### exc kernel and image
	
	With the transformation $T$ from the previous example, find bases for $\ker T$ and $\image T$.
	
###

### nav-buttons