### nav-buttons

So far in linear algebra, we've focused on vectors as fairly abstract concepts. Vectors can look like lists of $n$ numbers in $#R#^n$, polynomials in $#R#[x]$, or even matrices or linear transformations. But largely speaking, we've refrained from talking about the *geometry* of these spaces. Vectors in $#R#^2$, typically represented as arrows in the plane, have notions of length and direction in addition to the component representation we're more familiar with, and there's often a lot to be gained from thinking about vectors in this way. There's also the **dot product**, which takes two vectors and produces a number that measures their lengths and the angle between them, and in $#R#^3$ specifically, there's also the **cross product**, which takes two vectors and produces a third vector that's orthogonal (i.e. perpendicular) to them.

So what can we expect to generalize to arbitrary vector spaces? Probably not the cross product, since it's not even defined in $#R#^n$ unless $n = 3$ (there are technically generalizations to higher dimensions, but they're still specific to $#R#^n$ and won't generalize past that). Also, the notion of a vector having a single direction in $#R#^2$ breaks down immediately in $#R#^3$, where we need to know two angles and a length (i.e. spherical coordinates) to pin down a vector. In an $n$-dimensional space, there are $n$ degrees of freedom, so we'd need $n - 1$ angles to keep track of a vector if we know its length, and that's probably not a realistic goal. On the other hand, the angle *between* two vectors works just fine in every $#R#^n$, as does the dot product. In fact, everything that feels like it should generalize --- the angle between vectors, orthogonality, and magnitude --- can be defined in terms of the dot product. We'll see how it works in $#R#^n$ in this section, apply it to some common applications in the next, and then we'll bring its ideas to more general vector spaces in the section after that.

### def "the dot product"

	Let $\vec{v}, \vec{w} \in #R#^n$. The **dot product** of $\vec{v}$ and $\vec{w}$, written $\vec{v} \bullet \vec{w}$, is defined by

	$$
		[[ v_1 ; v_2 ; \vdots ; v_n ]] \bullet [[ w_1 ; w_2 ; \vdots ; w_n ]] = v_1 w_1 + v_2 w_2 + \cdots + v_n w_n.
	$$

###

It's not particularly clear why this is something we'd care about initially, especially when $\vec{v} \bullet \vec{w}$ isn't even a vector in $#R#^n$. To start, let's write down a few fundamental properties of the dot product, and then we'll look at what it can be used for.

### prop "properties of the dot product"

	1. The dot product is **symmetric**: $\vec{v} \bullet \vec{w} = \vec{w} \bullet \vec{v}$ for all $\vec{v}, \vec{w} \in #R#^n$.

	2. The dot product is **bilinear**:
	
	$$
		\left( \vec{u} + \vec{v} \right) \bullet \vec{w} = \vec{u} \bullet \vec{w} + \vec{v} \bullet \vec{w}
	$$

	and

	$$
		\left( c\vec{v} \right) \bullet \vec{w} = c \left( \vec{v} \bullet \vec{w} \right)
	$$

	for all $\vec{u}, \vec{v}, \vec{w} \in #R#^n$ and $c \in #R#$.

	3. The dot product is **positive definite**: $\vec{v} \bullet \vec{v} \geq 0$ for all $\vec{v} \in #R#^n$, and the only $\vec{v}$ for which $\vec{v} \bullet \vec{v} = 0$ is $\vec{v} = 0$.

###

The last property lets us define a notion of length in terms of only the dot product, finally connecting us to some of the geometry that we haven't been dealing with directly.

### def "magnitude"

	Let $\vec{v} \in #R#^n$. The **magnitude** of $\vec{v}$, also called the **length** or **norm**, is $||\vec{v}|| = \sqrt{\vec{v} \bullet \vec{v}}$.

###

This is exactly the notion of length we're used to: the vector $\vec{v} = [[ 2 ; -3 ]] \in #R#^2$ has magnitude

$$
	||\vec{v}|| &= \sqrt{\vec{v} \bullet \vec{v}}

	&= \sqrt{(2)(2) + (-3)(-3)}

	&= \sqrt{13}.
$$

By dividing a nonzero vector by its magnitude, we produce a parallel vector with magnitude $1$. This is called **normalizing**, and any vector with magnitude $1$ is called a **unit vector**. For example, the vector $\vec{v}$ from before normalizes to the unit vector

$$
	\frac{\vec{v}}{||\vec{v}||} = [[ \frac{2}{\sqrt{13}} ; -\frac{3}{\sqrt{13}} ]].
$$

The magnitude of a vector is implicitly its distance from the zero vector, and that notion generalizes to the distance *between* two vectors as well.

### def "distance between vectors"

	Let $\vec{v}, \vec{w} \in #R#^n$. The **distance** between $\vec{v}$ and $\vec{w}$ is $||\vec{v} - \vec{w}||$.

###

Again, this is exactly the usual distance between points in $#R#^n$.

### desmos vector-subtraction

Here, the purple vector is $\vec{v} = [[ 3 ; 2 ]]$, the blue one is $\vec{w} = [[ -2 ; 1 ]]$, and the distance between them is the magnitude of the red vector, which is

$$
	\vec{v} - \vec{w} = [[ 3 - (-2) ; 2 - 1 ]] = [[ 5 ; 1 ]].
$$

Let's use these vectors to demonstrate one more property of the dot product: its ability to measure the angle between vectors. We'll need the rarely-used **law of cosines**, which states that for any triangle with sides of length $a$, $b$, and $c$, and angle $\theta$ opposite $c$,

$$
	c^2 = a^2 + b^2 - 2ab\cos(\theta).
$$

If $\theta$ is the angle between $\vec{v}$ and $\vec{w}$, then we can see from the previous graph and the law of cosines that

$$
	||\vec{v} - \vec{w}||^2 &= ||\vec{v}||^2 + ||\vec{w}||^2 - 2||\vec{v}|| \cdot ||\vec{w}|| \cos(\theta)

	\left( \vec{v} - \vec{w} \right) \bullet \left( \vec{v} - \vec{w} \right) &= \vec{v} \bullet \vec{v} + \vec{w} \bullet \vec{w} - 2||\vec{v}|| \cdot ||\vec{w}|| \cos(\theta)

	\vec{v} \bullet \vec{v} - 2\left( \vec{v} \bullet \vec{w} \right) + \vec{w} \bullet \vec{w} &= \vec{v} \bullet \vec{v} + \vec{w} \bullet \vec{w} - 2||\vec{v}|| \cdot ||\vec{w}|| \cos(\theta)

	\vec{v} \bullet \vec{w} &= ||\vec{v}|| \cdot ||\vec{w}|| \cos(\theta)

	\theta &= \arccos \left( \frac{\vec{v} \bullet \vec{w}}{||\vec{v}|| \cdot ||\vec{w}||} \right).
$$

We generally need to be careful when applying inverse trig functions, but everything's fine here since $\arccos$ has a range of $[0, \pi]$, which is exactly the interval of possible angles between vectors.

### exc "distance and angle between vectors"

	Let $\vec{v}$ and $\vec{w}$ be the vectors

	$$
		\vec{v} = [[ 1 ; 2 ; 4 ]], \quad \vec{w} = [[ 0 ; -3 ; 3 ]].
	$$

	Find the distance between $\vec{v}$ and $\vec{w}$ and the angle between them.

###

This definition of angle lets us talk about perpendicular vectors. If the angle between $\vec{v}$ and $\vec{w}$ is exactly $90^\circ$, then

$$
	\vec{v} \bullet \vec{w} = ||\vec{v}|| \cdot ||\vec{w}|| \cos(90^\circ) = 0.
$$

Since we're defining everything in terms of the dot product, let's do that here too.

### def orthogonality

	Two vectors $\vec{v}, \vec{w} \in #R#^n$ are **orthogonal** if $\vec{v} \bullet \vec{w} = 0$.

###

We can immediately start getting applications of orthogonality: notably, the Pythagorean theorem can be stated in the language of linear algebra.

### thm "The Pythagorean Theorem"

	Two vectors $\vec{v}, \vec{w} \in #R#^n$ are **orthogonal** if and only if $||\vec{v}||^2 + ||\vec{w}||^2 = ||\vec{v} + \vec{w}||^2$.

###

### exc "the Pythagorean theorem"

	Show that this theorem holds. Start by assuming $\vec{v}$ and $\vec{w}$ are orthogonal and show $||\vec{v}||^2 + ||\vec{w}||^2 = ||\vec{v} + \vec{w}||^2$. Then assume that $||\vec{v}||^2 + ||\vec{w}||^2 = ||\vec{v} + \vec{w}||^2$ and show that $\vec{v}$ and $\vec{w}$ must be orthogonal.

###

Orthogonality will be surprisingly critical to most of the rest of the class. We'll want to talk often about all the vectors orthogonal to a given one, and so we'll give that object a name.

### def "orthogonal complement"

	Let $X$ be a subspace of $#R#^n$. The **orthogonal complement** to $X$ is the subspace $X^\perp$ of $#R#^n$ consisting of the vectors $\vec{v}$ that are orthogonal to every vector in $X$.

###

### ex "orthogonal complement"

	Let $X = \span\left\{ [[ 1 ; 1 ; 0 ]], [[ 0 ; -1 ; 2 ]] \right\}$. Find a basis for $X^\perp$.

	For a vector $\vec{v} = [[ v_1 ; v_2 ; v_3 ]]$ to be in the orthogonal complement, we need

	$$
		[[ v_1 ; v_2 ; v_3 ]] \bullet [[ 1 ; 1 ; 0 ]] &= 0

		[[ v_1 ; v_2 ; v_3 ]] \bullet [[ 0 ; -1 ; 2 ]] &= 0,
	$$

	so

	$$
		v_1 + v_2 &= 0

		-v_2 + 2v_3 &= 0.
	$$

	And that's a matrix equation once again! Row reducing, we have

	$$
		[[ 1, 1, 0 | 0 ; 0, -1, 2 | 0 ]] & 

		[[ 1, 0, 2 | 0 ; 0, -1, 2 | 0 ]] & \quad \vec{r_1} \pe \vec{r_2}

		[[ 1, 0, 2 | 0 ; 0, 1, -2 | 0 ]] & \quad \vec{r_2} \te -1

		\vec{v} = [[ -2t ; 2t ; t ]] & .
	$$

	In total, the orthogonal complement is $X^\perp = \span\left\{ [[ -2 ; 2 ; 1 ]] \right\}$.

###

### exc "orthogonal complement"

	Let $X$ be a subspace of $#R#^n$. Show that $X^\perp$ is in fact a subspace.

###



## Orthonormal Bases

When we're asked to come up with a basis for $#R#^n$, there are two generally good choices we've seen at this point: if we have some diagonalizable $n \times n$ matrix $A$ that's relevant to the situation, we might want to choose a basis of eigenvectors of $A$, and otherwise, the standard basis $\left\{ \vec{e_1}, ..., \vec{e_n} \right\}$ is a reliable choice. Let's dig into that a little more and understand *why* it's so reliable. The standard basis has many useful properties, but let's focus on the most relevant:

### def "orthonormal"

	A collection of vectors $\left\{ \vec{v_1}, ..., \vec{v_n} \right\}$ is **orthogonal** if $\vec{v_i} \bullet \vec{v_j} = 0$ whenever $i \neq j$, and it is **orthonormal** if additionally, $||\vec{v_i}|| = 1$ for every $\vec{v_i}$.

###

When a basis $\left\{ \vec{v_1}, ..., \vec{v_n} \right\}$ is **orthonormal**, finding the expression of a vector in terms of the basis is much, *much* easier than it otherwise would be. Normally, that process requires inverting a change-of-basis matrix, but now we can sidestep that entirely. Let $\vec{v} \in #R#^n$ be a vector with the expression

$$
	\vec{v} = c_1 \vec{v_1} + \cdots + c_n \vec{v_n}.
$$

Now take the dot product of both sides with $\vec{v_i}$:

$$
	\vec{v} \bullet \vec{v_i} &= \left( c_1 \vec{v_1} + \cdots + c_n \vec{v_n} \right) \bullet \vec{v_i}

	&= c_1 \vec{v_1} \bullet \vec{v_i} + \cdots + c_n \vec{v_n} \bullet \vec{v_i}

	&= c_i.
$$

In other words,

### prop "expression in an orthonormal basis"

	Let $\left\{ \vec{v_1}, ..., \vec{v_n} \right\}$ be an orthonormal basis for $#R#^n$. Then for any $\vec{v} \in #R#^n$,

	$$
		\vec{v} = \left( \vec{v} \bullet \vec{v_1} \right) \vec{v_1} + \cdots + \left( \vec{v} \bullet \vec{v_n} \right) \vec{v_n}.
	$$

###

Another point of convenience is that any collection of nonzero orthonormal vectors --- or even just orthogonal ones --- is necessarily linearly independent, so a collection of $n$ of them in $#R#^n$ must be a basis.

### prop "orthogonal vectors are linearly independent"

	If $\vec{v_1}, ..., \vec{v_k} \in #R#^n$ are orthogonal and nonzero, then they are linearly independent.

###

### pf

	Suppose $c_1\vec{v_1} + \cdots + c_k\vec{v_k} = \vec{0}$. Dotting both sides by $\vec{v_i}$ results in

	$$
		c_i \left( \vec{v_i} \bullet \vec{v_i} \right) \vec{v_i} = 0,
	$$

	and since $\vec{v_i} \neq 0$, $\vec{v_i} \bullet \vec{v_i} \neq 0$, meaning $c_i = 0$. In total, every $c_i = 0$, and so the $\vec{v_i}$ are linearly independent.

###

### exc "expression in an orthonormal basis"

	Given the vectors

	$$
		\vec{v_1} = [[ 1 ; -2 ; 1 ]], \quad \vec{v_2} = [[ -1 ; -1 ; -1 ]], \quad \vec{v_3} = [[ 3 ; 0 ; -3 ]].
	$$

	1. Show that $\left\{ \vec{v_1}, \vec{v_2}, \vec{v_3} \right\}$ is an orthogonal set, and therefore a basis for $#R#^3$.

	2. Find an orthonormal basis $\left\{ \vec{w_1}, \vec{w_2}, \vec{w_3} \right\}$, where $\vec{w_i}$ is parallel to $\vec{v_i}$.

	3. Express the vector

	$$
		\vec{v} = [[ 4 ; 3 ; -1 ]]
	$$

	in the basis $\left\{ \vec{w_1}, \vec{w_2}, \vec{w_3} \right\}$.

###

Just like linear independence is closely related to invertibility of matrices --- in that $n$ vectors in $#R#^n$ are linearly independent if and only if the matrix with them as columns is invertible --- orthonormality is closely related to another property a matrix can have.

### def "unitary matrix"

	An $m \times n$ matrix is **unitary** if its columns are orthonormal.

###

Unitary matrices don't have to be square, but any square unitary matrix has to be invertible, since the columns have to be linearly independent. Even non-square unitary matrices have a sort of pseudoinverse in their transpose: if $A$ is unitary, then $A^T A = I$, since every entry in a matrix product is really just a dot product in disguise, and the rows of $A^T$ are the columns of $A$ by definition. Putting these two facts together, a square unitary matrix is always invertible, and $A^{-1} = A^T$.

We'll have more to say about unitary matrices later on, but for now, let's see a few nice properties they have.

### prop "properties of unitary matrices"

 Let $A$ be a unitary $m \times n$ matrix. Then $A$ preserves distances and angles: $\left| \left| A\vec{v} \right| \right| = \left| \left| \vec{v} \right| \right|$ and $\left( A\vec{v} \right) \bullet \left( A\vec{w} \right) = \vec{v} \bullet \vec{w}$ for all $\vec{v}, \vec{w} \in #R#^n$.

###

### pf

	We'll show that $A$ preserves distances. Let

	$$
		\vec{v} = [[ c_1 ; \vdots ; c_n ]],
	$$

	and denote the columns of $A$ by $\vec{v_1}, ..., \vec{v_n}$. Then

	$$
		\left| \left| A\vec{v} \right| \right| &= \left| \left|  \right| \right|
	$$

###



## Projections



### nav-buttons