<header><div id="logo"><a href="/home/" tabindex="-1"><img src="/graphics/general-icons/logo.png" alt="Logo" tabindex="1"></img></a></div><div style="height: 20px"></div><h1 class="heading-text">Section 4: The Dot Product</h1></header><main><section><div class="text-buttons nav-buttons"><div class="focus-on-child tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div><p class="body-text">So far in linear algebra, we&#x2019;ve focused on vectors as fairly abstract concepts. Vectors can look like lists of <span class="tex-holder inline-math" data-source-tex="n">$n$</span> numbers in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$,</span> polynomials in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}[x]">$\mathbb{R}[x]$,</span> or even matrices or linear transformations. But largely speaking, we&#x2019;ve refrained from talking about the <em>geometry</em> of these spaces. Vectors in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^2">$\mathbb{R}^2$,</span> typically represented as arrows in the plane, have notions of length and direction in addition to the component representation we&#x2019;re more familiar with, and there&#x2019;s often a lot to be gained from thinking about vectors in this way. There&#x2019;s also the <strong>dot product</strong>, which takes two vectors and produces a number that measures their lengths and the angle between them, and in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^3">$\mathbb{R}^3$</span> specifically, there&#x2019;s also the <strong>cross product</strong>, which takes two vectors and produces a third vector that&#x2019;s orthogonal (i.e. perpendicular) to them.</p><p class="body-text">So what can we expect to generalize to arbitrary vector spaces? Probably not the cross product, since it&#x2019;s not even defined in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$</span> unless <span class="tex-holder inline-math" data-source-tex="n = 3">$n = 3$</span> (there are technically generalizations to higher dimensions, but they&#x2019;re still specific to <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$</span> and won&#x2019;t generalize past that). Also, the notion of a vector having a single direction in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^2">$\mathbb{R}^2$</span> breaks down immediately in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^3">$\mathbb{R}^3$,</span> where we need to know two angles and a length (i.e. spherical coordinates) to pin down a vector. In an <span class="tex-holder inline-math" data-source-tex="n">$n$-dimensional</span> space, there are <span class="tex-holder inline-math" data-source-tex="n">$n$</span> degrees of freedom, so we&#x2019;d need <span class="tex-holder inline-math" data-source-tex="n - 1">$n - 1$</span> angles to keep track of a vector if we know its length, and that&#x2019;s probably not a realistic goal. On the other hand, the angle <em>between</em> two vectors works just fine in every <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$,</span> as does the dot product. In fact, everything that feels like it should generalize &mdash; the angle between vectors, orthogonality, and magnitude &mdash; can be defined in terms of the dot product. We&#x2019;ll see how it works in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$</span> in this section, apply it to some common applications in the next, and then we&#x2019;ll bring its ideas to more general vector spaces in the section after that.</p><div class="notes-def notes-environment"><p class="body-text"</p><span class="notes-def-title">Definition: the dot product</span></p><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="\vec{v}, \vec{w} \in \mathbb{R}^n">$\vec{v}, \vec{w} \in \mathbb{R}^n$.</span> The <strong>dot product</strong> of <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{w}">$\vec{w}$,</span> written <span class="tex-holder inline-math" data-source-tex="\vec{v} \bullet \vec{w}">$\vec{v} \bullet \vec{w}$,</span> is defined by</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\left[\begin{array}{c} v_1 \\ v_2 \\ \vdots \\ v_n \end{array}\right] \bullet \left[\begin{array}{c} w_1 \\ w_2 \\ \vdots \\ w_n \end{array}\right] = v_1 w_1 + v_2 w_2 + \cdots + v_n w_n.[NEWLINE]$$">$$\begin{align*}\left[\begin{array}{c} v_1 \\ v_2 \\ \vdots \\ v_n \end{array}\right] \bullet \left[\begin{array}{c} w_1 \\ w_2 \\ \vdots \\ w_n \end{array}\right] = v_1 w_1 + v_2 w_2 + \cdots + v_n w_n.\end{align*}$$</span></p></div><p class="body-text">It&#x2019;s not particularly clear why this is something we&#x2019;d care about initially, especially when <span class="tex-holder inline-math" data-source-tex="\vec{v} \bullet \vec{w}">$\vec{v} \bullet \vec{w}$</span> isn&#x2019;t even a vector in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$.</span> To start, let&#x2019;s write down a few fundamental properties of the dot product, and then we&#x2019;ll look at what it can be used for.</p><div class="notes-prop notes-environment"><p class="body-text"</p><span class="notes-prop-title">Proposition: properties of the dot product</span></p><p class="body-text numbered-list-item">1. The dot product is <strong>symmetric</strong>: <span class="tex-holder inline-math" data-source-tex="\vec{v} \bullet \vec{w} = \vec{w} \bullet \vec{v}">$\vec{v} \bullet \vec{w} = \vec{w} \bullet \vec{v}$</span> for all <span class="tex-holder inline-math" data-source-tex="\vec{v}, \vec{w} \in \mathbb{R}^n">$\vec{v}, \vec{w} \in \mathbb{R}^n$.</span></p><p class="body-text numbered-list-item">2. The dot product is <strong>bilinear</strong>:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\left( \vec{u} + \vec{v} \right) \bullet \vec{w} = \vec{u} \bullet \vec{w} + \vec{v} \bullet \vec{w}[NEWLINE]$$">$$\begin{align*}\left( \vec{u} + \vec{v} \right) \bullet \vec{w} = \vec{u} \bullet \vec{w} + \vec{v} \bullet \vec{w}\end{align*}$$</span></p><p class="body-text">and</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\left( c\vec{v} \right) \bullet \vec{w} = c \left( \vec{v} \bullet \vec{w} \right)[NEWLINE]$$">$$\begin{align*}\left( c\vec{v} \right) \bullet \vec{w} = c \left( \vec{v} \bullet \vec{w} \right)\end{align*}$$</span></p><p class="body-text">for all <span class="tex-holder inline-math" data-source-tex="\vec{u}, \vec{v}, \vec{w} \in \mathbb{R}^n">$\vec{u}, \vec{v}, \vec{w} \in \mathbb{R}^n$</span> and <span class="tex-holder inline-math" data-source-tex="c \in \mathbb{R}">$c \in \mathbb{R}$.</span></p><p class="body-text numbered-list-item">3. The dot product is <strong>positive definite</strong>: <span class="tex-holder inline-math" data-source-tex="\vec{v} \bullet \vec{v} \geq 0">$\vec{v} \bullet \vec{v} \geq 0$</span> for all <span class="tex-holder inline-math" data-source-tex="\vec{v} \in \mathbb{R}^n">$\vec{v} \in \mathbb{R}^n$,</span> and the only <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> for which <span class="tex-holder inline-math" data-source-tex="\vec{v} \bullet \vec{v} = 0">$\vec{v} \bullet \vec{v} = 0$</span> is <span class="tex-holder inline-math" data-source-tex="\vec{v} = 0">$\vec{v} = 0$.</span></p></div><p class="body-text">The last property lets us define a notion of length in terms of only the dot product, finally connecting us to some of the geometry that we haven&#x2019;t been dealing with directly.</p><div class="notes-def notes-environment"><p class="body-text"</p><span class="notes-def-title">Definition: magnitude</span></p><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="\vec{v} \in \mathbb{R}^n">$\vec{v} \in \mathbb{R}^n$.</span> The <strong>magnitude</strong> of <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$,</span> also called the <strong>length</strong> or <strong>norm</strong>, is <span class="tex-holder inline-math" data-source-tex="||\vec{v}|| = \sqrt{\vec{v} \bullet \vec{v}}">$||\vec{v}|| = \sqrt{\vec{v} \bullet \vec{v}}$.</span></p></div><p class="body-text">This is exactly the notion of length we&#x2019;re used to: the vector <span class="tex-holder inline-math" data-source-tex="\vec{v} = \left[\begin{array}{c} 2 \\ -3 \end{array}\right] \in \mathbb{R}^2">$\vec{v} = \left[\begin{array}{c} 2 \\ -3 \end{array}\right] \in \mathbb{R}^2$</span> has magnitude</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]||\vec{v}|| &= \sqrt{\vec{v} \bullet \vec{v}}\\[NEWLINE][TAB]&= \sqrt{(2)(2) + (-3)(-3)}\\[NEWLINE][TAB]&= \sqrt{13}.[NEWLINE]\end{align*}">$$\begin{align*}||\vec{v}|| &= \sqrt{\vec{v} \bullet \vec{v}}\\[4px]&= \sqrt{(2)(2) + (-3)(-3)}\\[4px]&= \sqrt{13}.\end{align*}$$</span></p><p class="body-text">By dividing a nonzero vector by its magnitude, we produce a parallel vector with magnitude <span class="tex-holder inline-math" data-source-tex="1">$1$.</span> This is called <strong>normalizing</strong>, and any vector with magnitude <span class="tex-holder inline-math" data-source-tex="1">$1$</span> is called a <strong>unit vector</strong>. For example, the vector <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> from before normalizes to the unit vector</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\frac{\vec{v}}{||\vec{v}||} = \left[\begin{array}{c} \frac{2}{\sqrt{13}} \\ -\frac{3}{\sqrt{13}} \end{array}\right].[NEWLINE]$$">$$\begin{align*}\frac{\vec{v}}{||\vec{v}||} = \left[\begin{array}{c} \frac{2}{\sqrt{13}} \\ -\frac{3}{\sqrt{13}} \end{array}\right].\end{align*}$$</span></p><p class="body-text">The magnitude of a vector is implicitly its distance from the zero vector, and that notion generalizes to the distance <em>between</em> two vectors as well.</p><div class="notes-def notes-environment"><p class="body-text"</p><span class="notes-def-title">Definition: distance between vectors</span></p><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="\vec{v}, \vec{w} \in \mathbb{R}^n">$\vec{v}, \vec{w} \in \mathbb{R}^n$.</span> The <strong>distance</strong> between <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{w}">$\vec{w}$</span> is <span class="tex-holder inline-math" data-source-tex="||\vec{v} - \vec{w}||">$||\vec{v} - \vec{w}||$.</span></p></div><p class="body-text">Again, this is exactly the usual distance between points in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$.</span></p><div class="desmos-border"><div id="vector-subtraction" class="desmos-container"></div></div><p class="body-text">Here, the purple vector is <span class="tex-holder inline-math" data-source-tex="\vec{v} = \left[\begin{array}{c} 3 \\ 2 \end{array}\right]">$\vec{v} = \left[\begin{array}{c} 3 \\ 2 \end{array}\right]$,</span> the blue one is <span class="tex-holder inline-math" data-source-tex="\vec{w} = \left[\begin{array}{c} -2 \\ 1 \end{array}\right]">$\vec{w} = \left[\begin{array}{c} -2 \\ 1 \end{array}\right]$,</span> and the distance between them is the magnitude of the red vector, which is</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v} - \vec{w} = \left[\begin{array}{c} 3 - (-2) \\ 2 - 1 \end{array}\right] = \left[\begin{array}{c} 5 \\ 1 \end{array}\right].[NEWLINE]$$">$$\begin{align*}\vec{v} - \vec{w} = \left[\begin{array}{c} 3 - (-2) \\ 2 - 1 \end{array}\right] = \left[\begin{array}{c} 5 \\ 1 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Let&#x2019;s use these vectors to demonstrate one more property of the dot product: its ability to measure the angle between vectors. We&#x2019;ll need the rarely-used <strong>law of cosines</strong>, which states that for any triangle with sides of length <span class="tex-holder inline-math" data-source-tex="a">$a$,</span> <span class="tex-holder inline-math" data-source-tex="b">$b$,</span> and <span class="tex-holder inline-math" data-source-tex="c">$c$,</span> and angle <span class="tex-holder inline-math" data-source-tex="\theta">$\theta$</span> opposite <span class="tex-holder inline-math" data-source-tex="c">$c$,</span></p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]c^2 = a^2 + b^2 - 2ab\cos(\theta).[NEWLINE]$$">$$\begin{align*}c^2 = a^2 + b^2 - 2ab\cos(\theta).\end{align*}$$</span></p><p class="body-text">If <span class="tex-holder inline-math" data-source-tex="\theta">$\theta$</span> is the angle between <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{w}">$\vec{w}$,</span> then we can see from the previous graph and the law of cosines that</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]||\vec{v} - \vec{w}||^2 &= ||\vec{v}||^2 + ||\vec{w}||^2 - 2||\vec{v}|| \cdot ||\vec{w}|| \cos(\theta)\\[NEWLINE][TAB]\left( \vec{v} - \vec{w} \right) \bullet \left( \vec{v} - \vec{w} \right) &= \vec{v} \bullet \vec{v} + \vec{w} \bullet \vec{w} - 2||\vec{v}|| \cdot ||\vec{w}|| \cos(\theta)\\[NEWLINE][TAB]\vec{v} \bullet \vec{v} - 2\left( \vec{v} \bullet \vec{w} \right) + \vec{w} \bullet \vec{w} &= \vec{v} \bullet \vec{v} + \vec{w} \bullet \vec{w} - 2||\vec{v}|| \cdot ||\vec{w}|| \cos(\theta)\\[NEWLINE][TAB]\vec{v} \bullet \vec{w} &= ||\vec{v}|| \cdot ||\vec{w}|| \cos(\theta)\\[NEWLINE][TAB]\theta &= \arccos \left( \frac{\vec{v} \bullet \vec{w}}{||\vec{v}|| \cdot ||\vec{w}||} \right).[NEWLINE]\end{align*}">$$\begin{align*}||\vec{v} - \vec{w}||^2 &= ||\vec{v}||^2 + ||\vec{w}||^2 - 2||\vec{v}|| \cdot ||\vec{w}|| \cos(\theta)\\[4px]\left( \vec{v} - \vec{w} \right) \bullet \left( \vec{v} - \vec{w} \right) &= \vec{v} \bullet \vec{v} + \vec{w} \bullet \vec{w} - 2||\vec{v}|| \cdot ||\vec{w}|| \cos(\theta)\\[4px]\vec{v} \bullet \vec{v} - 2\left( \vec{v} \bullet \vec{w} \right) + \vec{w} \bullet \vec{w} &= \vec{v} \bullet \vec{v} + \vec{w} \bullet \vec{w} - 2||\vec{v}|| \cdot ||\vec{w}|| \cos(\theta)\\[4px]\vec{v} \bullet \vec{w} &= ||\vec{v}|| \cdot ||\vec{w}|| \cos(\theta)\\[4px]\theta &= \arccos \left( \frac{\vec{v} \bullet \vec{w}}{||\vec{v}|| \cdot ||\vec{w}||} \right).\end{align*}$$</span></p><p class="body-text">We generally need to be careful when applying inverse trig functions, but everything&#x2019;s fine here since <span class="tex-holder inline-math" data-source-tex="\arccos">$\arccos$</span> has a range of <span class="tex-holder inline-math" data-source-tex="[0, \pi]">$[0, \pi]$,</span> which is exactly the interval of possible angles between vectors.</p><div class="notes-exc notes-environment"><p class="body-text"</p><span class="notes-exc-title">Exercise: distance and angle between vectors</span></p><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{w}">$\vec{w}$</span> be the vectors</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v} = \left[\begin{array}{c} 1 \\ 2 \\ 4 \end{array}\right], \quad \vec{w} = \left[\begin{array}{c} 0 \\ -3 \\ 3 \end{array}\right].[NEWLINE]$$">$$\begin{align*}\vec{v} = \left[\begin{array}{c} 1 \\ 2 \\ 4 \end{array}\right], \quad \vec{w} = \left[\begin{array}{c} 0 \\ -3 \\ 3 \end{array}\right].\end{align*}$$</span></p><p class="body-text">Find the distance between <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{w}">$\vec{w}$</span> and the angle between them.</p></div><p class="body-text">This definition of angle lets us talk about perpendicular vectors. If the angle between <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{w}">$\vec{w}$</span> is exactly <span class="tex-holder inline-math" data-source-tex="90^\circ">$90^\circ$,</span> then</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v} \bullet \vec{w} = ||\vec{v}|| \cdot ||\vec{w}|| \cos(90^\circ) = 0.[NEWLINE]$$">$$\begin{align*}\vec{v} \bullet \vec{w} = ||\vec{v}|| \cdot ||\vec{w}|| \cos(90^\circ) = 0.\end{align*}$$</span></p><p class="body-text">Since we&#x2019;re defining everything in terms of the dot product, let&#x2019;s do that here too.</p><div class="notes-def notes-environment"><p class="body-text"</p><span class="notes-def-title">Definition: orthogonality</span></p><p class="body-text">Two vectors <span class="tex-holder inline-math" data-source-tex="\vec{v}, \vec{w} \in \mathbb{R}^n">$\vec{v}, \vec{w} \in \mathbb{R}^n$</span> are <strong>orthogonal</strong> if <span class="tex-holder inline-math" data-source-tex="\vec{v} \bullet \vec{w} = 0">$\vec{v} \bullet \vec{w} = 0$.</span></p></div><p class="body-text">We can immediately start getting applications of orthogonality: notably, the Pythagorean theorem can be stated in the language of linear algebra.</p><div class="notes-thm notes-environment"><p class="body-text"</p><span class="notes-thm-title">The Pythagorean Theorem</span></p><p class="body-text">Two vectors <span class="tex-holder inline-math" data-source-tex="\vec{v}, \vec{w} \in \mathbb{R}^n">$\vec{v}, \vec{w} \in \mathbb{R}^n$</span> are <strong>orthogonal</strong> if and only if <span class="tex-holder inline-math" data-source-tex="||\vec{v}||^2 + ||\vec{w}||^2 = ||\vec{v} + \vec{w}||^2">$||\vec{v}||^2 + ||\vec{w}||^2 = ||\vec{v} + \vec{w}||^2$.</span></p></div><div class="notes-exc notes-environment"><p class="body-text"</p><span class="notes-exc-title">Exercise</span></p><p class="body-text">Show that this theorem holds. Start by assuming <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{w}">$\vec{w}$</span> are orthogonal and show <span class="tex-holder inline-math" data-source-tex="||\vec{v}||^2 + ||\vec{w}||^2 = ||\vec{v} + \vec{w}||^2">$||\vec{v}||^2 + ||\vec{w}||^2 = ||\vec{v} + \vec{w}||^2$.</span> Then assume that <span class="tex-holder inline-math" data-source-tex="||\vec{v}||^2 + ||\vec{w}||^2 = ||\vec{v} + \vec{w}||^2">$||\vec{v}||^2 + ||\vec{w}||^2 = ||\vec{v} + \vec{w}||^2$</span> and show that <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> and <span class="tex-holder inline-math" data-source-tex="\vec{w}">$\vec{w}$</span> must be orthogonal.</p></div><p class="body-text">Orthogonality will be surprisingly critical to most of the rest of the class. We&#x2019;ll want to talk often about all the vectors orthogonal to a given one, and so we&#x2019;ll give that object a name.</p><div class="notes-def notes-environment"><p class="body-text"</p><span class="notes-def-title">Definition: orthogonal complement</span></p><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="X">$X$</span> be a subspace of <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$.</span> The <strong>orthogonal complement</strong> to <span class="tex-holder inline-math" data-source-tex="X">$X$</span> is the subspace <span class="tex-holder inline-math" data-source-tex="X^\perp">$X^\perp$</span> of <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$</span> consisting of the vectors <span class="tex-holder inline-math" data-source-tex="\vec{v}">$\vec{v}$</span> that are orthogonal to every vector in <span class="tex-holder inline-math" data-source-tex="X">$X$.</span></p></div><div class="notes-ex notes-environment"><p class="body-text"</p><span class="notes-ex-title">Example: orthogonal complement</span></p><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="X = \operatorname{span}\left\{ \left[\begin{array}{c} 1 \\ 1 \\ 0 \end{array}\right], \left[\begin{array}{c} 0 \\ -1 \\ 2 \end{array}\right] \right\}">$X = \operatorname{span}\left\{ \left[\begin{array}{c} 1 \\ 1 \\ 0 \end{array}\right], \left[\begin{array}{c} 0 \\ -1 \\ 2 \end{array}\right] \right\}$.</span> Find a basis for <span class="tex-holder inline-math" data-source-tex="X^\perp">$X^\perp$.</span></p><p class="body-text">For a vector <span class="tex-holder inline-math" data-source-tex="\vec{v} = \left[\begin{array}{c} v_1 \\ v_2 \\ v_3 \end{array}\right]">$\vec{v} = \left[\begin{array}{c} v_1 \\ v_2 \\ v_3 \end{array}\right]$</span> to be in the orthogonal complement, we need</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{c} v_1 \\ v_2 \\ v_3 \end{array}\right] \bullet \left[\begin{array}{c} 1 \\ 1 \\ 0 \end{array}\right] &= 0\\[NEWLINE][TAB]\left[\begin{array}{c} v_1 \\ v_2 \\ v_3 \end{array}\right] \bullet \left[\begin{array}{c} 0 \\ -1 \\ 2 \end{array}\right] &= 0,[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{c} v_1 \\ v_2 \\ v_3 \end{array}\right] \bullet \left[\begin{array}{c} 1 \\ 1 \\ 0 \end{array}\right] &= 0\\[4px]\left[\begin{array}{c} v_1 \\ v_2 \\ v_3 \end{array}\right] \bullet \left[\begin{array}{c} 0 \\ -1 \\ 2 \end{array}\right] &= 0,\end{align*}$$</span></p><p class="body-text">so</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]v_1 + v_2 &= 0\\[NEWLINE][TAB]-v_2 + 2v_3 &= 0.[NEWLINE]\end{align*}">$$\begin{align*}v_1 + v_2 &= 0\\[4px]-v_2 + 2v_3 &= 0.\end{align*}$$</span></p><p class="body-text">And that&#x2019;s a matrix equation once again! Row reducing, we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{ccc|c} 1& 1& 0 & 0 \\ 0& -1& 2 & 0 \end{array}\right] & \\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 1& 0& 2 & 0 \\ 0& -1& 2 & 0 \end{array}\right] & \quad \vec{r_1} \ +\!\!= \vec{r_2}\\[NEWLINE][TAB]\left[\begin{array}{ccc|c} 1& 0& 2 & 0 \\ 0& 1& -2 & 0 \end{array}\right] & \quad \vec{r_2} \ \times\!\!= -1\\[NEWLINE][TAB]\vec{v} = \left[\begin{array}{c} -2t \\ 2t \\ t \end{array}\right] & .[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{ccc|c} 1& 1& 0 & 0 \\ 0& -1& 2 & 0 \end{array}\right] & \\[4px]\left[\begin{array}{ccc|c} 1& 0& 2 & 0 \\ 0& -1& 2 & 0 \end{array}\right] & \quad \vec{r_1} \ +\!\!= \vec{r_2}\\[4px]\left[\begin{array}{ccc|c} 1& 0& 2 & 0 \\ 0& 1& -2 & 0 \end{array}\right] & \quad \vec{r_2} \ \times\!\!= -1\\[4px]\vec{v} = \left[\begin{array}{c} -2t \\ 2t \\ t \end{array}\right] & .\end{align*}$$</span></p><p class="body-text">In total, the orthogonal complement is <span class="tex-holder inline-math" data-source-tex="X^\perp = \operatorname{span}\left\{ \left[\begin{array}{c} -2 \\ 2 \\ 1 \end{array}\right] \right\}">$X^\perp = \operatorname{span}\left\{ \left[\begin{array}{c} -2 \\ 2 \\ 1 \end{array}\right] \right\}$.</span></p></div><div class="notes-exc notes-environment"><p class="body-text"</p><span class="notes-exc-title">Exercise</span></p><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="X">$X$</span> be a subspace of <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$.</span> Show that <span class="tex-holder inline-math" data-source-tex="X^\perp">$X^\perp$</span> is in fact a subspace.</p></div></section><h2 class="section-text"> Orthonormal Bases</h2><section><p class="body-text">When we&#x2019;re asked to come up with a basis for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$,</span> there are two generally good choices we&#x2019;ve seen at this point: if we have some diagonalizable <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix <span class="tex-holder inline-math" data-source-tex="A">$A$</span> that&#x2019;s relevant to the situation, we might want to choose a basis of eigenvectors of <span class="tex-holder inline-math" data-source-tex="A">$A$,</span> and otherwise, the standard basis <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{e_1}, ..., \vec{e_n} \right\}">$\left\{ \vec{e_1}, ..., \vec{e_n} \right\}$</span> is a reliable choice. Let&#x2019;s dig into that a little more and understand <em>why</em> it&#x2019;s so reliable. The standard basis has many useful properties, but let&#x2019;s focus on the most relevant:</p><div class="notes-def notes-environment"><p class="body-text"</p><span class="notes-def-title">Definition: orthonormal</span></p><p class="body-text">A collection of vectors <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{v_1}, ..., \vec{v_n} \right\}">$\left\{ \vec{v_1}, ..., \vec{v_n} \right\}$</span> is <strong>orthogonal</strong> if <span class="tex-holder inline-math" data-source-tex="\vec{v_i} \bullet \vec{v_j} = 0">$\vec{v_i} \bullet \vec{v_j} = 0$</span> whenever <span class="tex-holder inline-math" data-source-tex="i \neq j">$i \neq j$,</span> and it is <strong>orthonormal</strong> if additionally, <span class="tex-holder inline-math" data-source-tex="||\vec{v_i}|| = 1">$||\vec{v_i}|| = 1$</span> for every <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$.</span></p></div><p class="body-text">When a basis <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{v_1}, ..., \vec{v_n} \right\}">$\left\{ \vec{v_1}, ..., \vec{v_n} \right\}$</span> is <strong>orthonormal</strong>, finding the expression of a vector in terms of the basis is much, <em>much</em> easier than it otherwise would be. Normally, that process requires inverting a change-of-basis matrix, but now we can sidestep that entirely. Let <span class="tex-holder inline-math" data-source-tex="\vec{v} \in \mathbb{R}^n">$\vec{v} \in \mathbb{R}^n$</span> be a vector with the expression</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v} = c_1 \vec{v_1} + \cdots + c_n \vec{v_n}.[NEWLINE]$$">$$\begin{align*}\vec{v} = c_1 \vec{v_1} + \cdots + c_n \vec{v_n}.\end{align*}$$</span></p><p class="body-text">Now take the dot product of both sides with <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$:</span></p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\vec{v} \bullet \vec{v_i} &= \left( c_1 \vec{v_1} + \cdots + c_n \vec{v_n} \right) \bullet \vec{v_i}\\[NEWLINE][TAB]&= c_1 \vec{v_1} \bullet \vec{v_i} + \cdots + c_n \vec{v_n} \bullet \vec{v_i}\\[NEWLINE][TAB]&= c_i.[NEWLINE]\end{align*}">$$\begin{align*}\vec{v} \bullet \vec{v_i} &= \left( c_1 \vec{v_1} + \cdots + c_n \vec{v_n} \right) \bullet \vec{v_i}\\[4px]&= c_1 \vec{v_1} \bullet \vec{v_i} + \cdots + c_n \vec{v_n} \bullet \vec{v_i}\\[4px]&= c_i.\end{align*}$$</span></p><p class="body-text">In other words,</p><div class="notes-prop notes-environment"><p class="body-text"</p><span class="notes-prop-title">Proposition: expression in an orthonormal basis</span></p><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{v_1}, ..., \vec{v_n} \right\}">$\left\{ \vec{v_1}, ..., \vec{v_n} \right\}$</span> be an orthonormal basis for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$.</span> Then for any <span class="tex-holder inline-math" data-source-tex="\vec{v} \in \mathbb{R}^n">$\vec{v} \in \mathbb{R}^n$,</span></p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v} = \left( \vec{v} \bullet \vec{v_1} \right) \vec{v_1} + \cdots + \left( \vec{v} \bullet \vec{v_n} \right) \vec{v_n}.[NEWLINE]$$">$$\begin{align*}\vec{v} = \left( \vec{v} \bullet \vec{v_1} \right) \vec{v_1} + \cdots + \left( \vec{v} \bullet \vec{v_n} \right) \vec{v_n}.\end{align*}$$</span></p></div><p class="body-text">Another point of convenience is that any collection of nonzero orthonormal vectors &mdash; or even just orthogonal ones &mdash; is necessarily linearly independent, so a collection of <span class="tex-holder inline-math" data-source-tex="n">$n$</span> of them in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$</span> must be a basis.</p><div class="notes-prop notes-environment"><p class="body-text"</p><span class="notes-prop-title">Proposition: orthogonal vectors are linearly independent</span></p><p class="body-text">If <span class="tex-holder inline-math" data-source-tex="\vec{v_1}, ..., \vec{v_k} \in \mathbb{R}^n">$\vec{v_1}, ..., \vec{v_k} \in \mathbb{R}^n$</span> are orthogonal and nonzero, then they are linearly independent.</p></div><div class="notes-pf notes-environment"><p class="body-text"</p><span class="notes-pf-title">Proof</span></p><p class="body-text">Suppose <span class="tex-holder inline-math" data-source-tex="c_1\vec{v_1} + \cdots + c_k\vec{v_k} = \vec{0}">$c_1\vec{v_1} + \cdots + c_k\vec{v_k} = \vec{0}$.</span> Dotting both sides by <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$</span> results in</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]c_i \left( \vec{v_i} \bullet \vec{v_i} \right) \vec{v_i} = 0,[NEWLINE]$$">$$\begin{align*}c_i \left( \vec{v_i} \bullet \vec{v_i} \right) \vec{v_i} = 0,\end{align*}$$</span></p><p class="body-text">and since <span class="tex-holder inline-math" data-source-tex="\vec{v_i} \neq 0">$\vec{v_i} \neq 0$,</span> <span class="tex-holder inline-math" data-source-tex="\vec{v_i} \bullet \vec{v_i} \neq 0">$\vec{v_i} \bullet \vec{v_i} \neq 0$,</span> meaning <span class="tex-holder inline-math" data-source-tex="c_i = 0">$c_i = 0$.</span> In total, every <span class="tex-holder inline-math" data-source-tex="c_i = 0">$c_i = 0$,</span> and so the <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$</span> are linearly independent.</p></div><div class="notes-exc notes-environment"><p class="body-text"</p><span class="notes-exc-title">Exercise: expression in an orthonormal basis</span></p><p class="body-text">Given the vectors</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v_1} = \left[\begin{array}{c} 1 \\ -2 \\ 1 \end{array}\right], \quad \vec{v_2} = \left[\begin{array}{c} -1 \\ -1 \\ -1 \end{array}\right], \quad \vec{v_3} = \left[\begin{array}{c} 3 \\ 0 \\ -3 \end{array}\right].[NEWLINE]$$">$$\begin{align*}\vec{v_1} = \left[\begin{array}{c} 1 \\ -2 \\ 1 \end{array}\right], \quad \vec{v_2} = \left[\begin{array}{c} -1 \\ -1 \\ -1 \end{array}\right], \quad \vec{v_3} = \left[\begin{array}{c} 3 \\ 0 \\ -3 \end{array}\right].\end{align*}$$</span></p><p class="body-text numbered-list-item">1. Show that <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{v_1}, \vec{v_2}, \vec{v_3} \right\}">$\left\{ \vec{v_1}, \vec{v_2}, \vec{v_3} \right\}$</span> is an orthogonal set, and therefore a basis for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^3">$\mathbb{R}^3$.</span></p><p class="body-text numbered-list-item">2. Find an orthonormal basis <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{w_1}, \vec{w_2}, \vec{w_3} \right\}">$\left\{ \vec{w_1}, \vec{w_2}, \vec{w_3} \right\}$,</span> where <span class="tex-holder inline-math" data-source-tex="\vec{w_i}">$\vec{w_i}$</span> is parallel to <span class="tex-holder inline-math" data-source-tex="\vec{v_i}">$\vec{v_i}$.</span></p><p class="body-text numbered-list-item">3. Express the vector</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\vec{v} = \left[\begin{array}{c} 4 \\ 3 \\ -1 \end{array}\right][NEWLINE]$$">$$\begin{align*}\vec{v} = \left[\begin{array}{c} 4 \\ 3 \\ -1 \end{array}\right]\end{align*}$$</span></p><p class="body-text">in the basis <span class="tex-holder inline-math" data-source-tex="\left\{ \vec{w_1}, \vec{w_2}, \vec{w_3} \right\}">$\left\{ \vec{w_1}, \vec{w_2}, \vec{w_3} \right\}$.</span></p></div><p class="body-text">Just like linear independence is closely related to invertibility of matrices &mdash; in that <span class="tex-holder inline-math" data-source-tex="n">$n$</span> vectors in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$</span> are linearly independent if and only if the matrix with them as columns is invertible &mdash; orthonormality is closely related to another property a matrix can have.</p><div class="notes-def notes-environment"><p class="body-text"</p><span class="notes-def-title">Definition: unitary matrix</span></p><p class="body-text">An <span class="tex-holder inline-math" data-source-tex="m \times n">$m \times n$</span> matrix is <strong>unitary</strong> if its columns are orthonormal.</p></div><p class="body-text">Unitary matrices don&#x2019;t have to be square, but any square unitary matrix has to be invertible, since the columns have to be linearly independent. Even non-square unitary matrices have a sort of pseudoinverse in their transpose: if <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is unitary, then <span class="tex-holder inline-math" data-source-tex="A^T A = I">$A^T A = I$,</span> since every entry in a matrix product is really just a dot product in disguise, and the rows of <span class="tex-holder inline-math" data-source-tex="A^T">$A^T$</span> are the columns of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> by definition.</p><p class="body-text">We&#x2019;ll have more to say about unitary matrices later on, but for now, let&#x2019;s see a few nice properties they have.</p><div class="notes-prop notes-environment"><p class="body-text"</p><span class="notes-prop-title">Proposition: properties of unitary matrices</span></p></div></section><h2 class="section-text"> Projections</h2><section><div class="text-buttons nav-buttons"><div class="focus-on-child tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div></section></main>