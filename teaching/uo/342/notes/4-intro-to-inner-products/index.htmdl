### nav-buttons

So far in linear algebra, we've focused on vectors as fairly abstract concepts. Vectors can look like lists of $n$ numbers in $#R#^n$, polynomials in $#R#[x]$, or even matrices or linear transformations. But largely speaking, we've refrained from talking about the *geometry* of these spaces. Vectors in $#R#^2$, typically represented as arrows in the plane, have notions of length and direction in addition to the component representation we're more familiar with, and there's often a lot to be gained from thinking about vectors in this way. There's also the **dot product**, which takes two vectors and produces a number that measures their lengths and the angle between them, and in $#R#^3$ specifically, there's also the **cross product**, which takes two vectors and produces a third vector that's orthogonal (i.e. perpendicular) to them.

So what can we expect to generalize to arbitrary vector spaces? Probably not the cross product, since it's not even defined in $#R#^n$ unless $n = 3$ (there are technically generalizations to higher dimensions, but they're still specific to $#R#^n$ and won't generalize past that). Also, the notion of a vector having a single direction in $#R#^2$ breaks down immediately in $#R#^3$, where we need to know two angles and a length (i.e. spherical coordinates) to pin down a vector. In an $n$-dimensional space, there are $n$ degrees of freedom, so we'd need $n - 1$ angles to keep track of a vector if we know its length, and that's probably not a realistic goal. On the other hand, the angle *between* two vectors works just fine in every $#R#^n$, as does the dot product. In fact, everything that feels like it should generalize --- the angle between vectors, orthogonality, and magnitude --- can be defined in terms of the dot product. Let's review how it works in $#R#^n$, and then we'll begin to bring its ideas to more general vector spaces.

### def "the dot product"

	Let $\vec{v}, \vec{w} \in #R#^n$. The **dot product** of $\vec{v}$ and $\vec{w}$, written $\vec{v} \bullet \vec{w}$, is defined by

	$$
		[[ v_1 ; v_2 ; \vdots ; v_n ]] \bullet [[ w_1 ; w_2 ; \vdots ; w_n ]] = v_1 w_1 + v_2 w_2 + \cdots + v_n w_n.
	$$

###

It's not particularly clear why this is something we'd care about initially, especially when $\vec{v} \bullet \vec{w}$ isn't even a vector in $#R#^n$. To start, let's write down a few fundamental properties of the dot product, and then we'll look at what it can be used for.

### prop "properties of the dot product"

	1. The dot product is **symmetric**: $\vec{v} \bullet \vec{w} = \vec{w} \bullet \vec{v}$ for all $\vec{v}, \vec{w} \in #R#^n$.

	2. The dot product is **bilinear**:
	
	$$
		\left( \vec{u} + \vec{v} \right) \bullet \vec{w} = \vec{u} \bullet \vec{w} + \vec{v} \bullet \vec{w}
	$$

	and

	$$
		\left( c\vec{v} \right) \bullet \vec{w} = c \left( \vec{v} \bullet \vec{w} \right)
	$$

	for all $\vec{u}, \vec{v}, \vec{w} \in #R#^n$ and $c \in #R#$.

	3. The dot product is **positive definite**: $\vec{v} \bullet \vec{v} \geq 0$ for all $\vec{v} \in #R#^n$, and the only $\vec{v}$ for which $\vec{v} \bullet \vec{v} = 0$ is $\vec{v} = 0$.

###

The last property lets us define a notion of length in terms of only the dot product, finally connecting us to some of the geometry that we haven't been dealing with directly.

### def "magnitude"

	Let $\vec{v} \in #R#^n$. The **magnitude** of $\vec{v}$, also called the **length** or **norm**, is $||\vec{v}|| = \sqrt{\vec{v} \bullet \vec{v}}$.

###

This is exactly the notion of length we're used to: the vector $\vec{v} = [[ 2 ; -3 ]] \in #R#^2$ has magnitude

$$
	||\vec{v}|| &= \sqrt{\vec{v} \bullet \vec{v}}

	&= \sqrt{(2)(2) + (-3)(-3)}

	&= \sqrt{13}.
$$

By dividing a nonzero vector by its magnitude, we produce a parallel vector with magnitude $1$. This is called **normalizing**, and any vector with magnitude $1$ is called a **unit vector**. For example, the vector $\vec{v}$ from before normalizes to the unit vector

$$
	\frac{\vec{v}}{||\vec{v}||} = [[ \frac{2}{\sqrt{13}} ; -\frac{3}{\sqrt{13}} ]].
$$

The magnitude of a vector is implicitly its distance from the zero vector, and that notion generalizes to the distance *between* two vectors as well.

### def "distance between vectors"

	Let $\vec{v}, \vec{w} \in #R#^n$. The **distance** between $\vec{v}$ and $\vec{w}$ is $||\vec{v} - \vec{w}||$.

###

Again, this is exactly the usual distance between points in $#R#^n$.

### desmos vector-subtraction

Here, the purple vector is $\vec{v} = [[ 3 ; 2 ]]$, the blue one is $\vec{w} = [[ -2 ; 1 ]]$, and the distance between them is the magnitude of the red vector, which is

$$
	\vec{v} - \vec{w} = [[ 3 - (-2) ; 2 - 1 ]] = [[ 5 ; 1 ]].
$$

Let's use these vectors to demonstrate one more property of the dot product: its ability to measure the angle between vectors. We'll need the rarely-used **law of cosines**, which states that for any triangle with sides of length $a$, $b$, and $c$, and angle $\theta$ opposite $c$,

$$
	c^2 = a^2 + b^2 - 2ab\cos(\theta).
$$

If $\theta$ is the angle between $\vec{v}$ and $\vec{w}$, then we can see from the previous graph and the law of cosines that

$$
	||\vec{v} - \vec{w}||^2 &= ||\vec{v}||^2 + ||\vec{w}||^2 - 2||\vec{v}|| \cdot ||\vec{w}|| \cos(\theta)

	\left( \vec{v} - \vec{w} \right) \bullet \left( \vec{v} - \vec{w} \right) &= \vec{v} \bullet \vec{v} + \vec{w} \bullet \vec{w} - 2||\vec{v}|| \cdot ||\vec{w}|| \cos(\theta)

	\vec{v} \bullet \vec{v} - 2\left( \vec{v} \bullet \vec{w} \right) + \vec{w} \bullet \vec{w} &= \vec{v} \bullet \vec{v} + \vec{w} \bullet \vec{w} - 2||\vec{v}|| \cdot ||\vec{w}|| \cos(\theta)

	\vec{v} \bullet \vec{w} &= ||\vec{v}|| \cdot ||\vec{w}|| \cos(\theta)

	\theta &= \arccos \left( \frac{\vec{v} \bullet \vec{w}}{||\vec{v}|| \cdot ||\vec{w}||} \right).
$$

We generally need to be careful when applying inverse trig functions, but everything's fine here since $\arccos$ has a range of $[0, \pi]$, which is exactly the interval of possible angles between vectors.

### exc "distance and angle between vectors"

	Let $\vec{v}$ and $\vec{w}$ be the vectors

	$$
		\vec{v} = [[ 1 ; 2 ; 4 ]], \quad \vec{w} = [[ 0 ; -3 ; 3 ]].
	$$

	Find the distance between $\vec{v}$ and $\vec{w}$ and the angle between them.

###

This definition of angle lets us talk about perpendicular vectors. If the angle between $\vec{v}$ and $\vec{w}$ is exactly $90^\circ$, then

$$
	\vec{v} \bullet \vec{w} = ||\vec{v}|| \cdot ||\vec{w}|| \cos(90^\circ) = 0.
$$

Since we're defining everything in terms of the dot product, let's do that here too.

### def orthogonality

	Two vectors $\vec{v}, \vec{w} \in #R#^n$ are **orthogonal** if $\vec{v} \bullet \vec{w} = 0$.

###

We can immediately start getting applications of orthogonality: notably, the Pythagorean theorem can be stated in the language of linear algebra.

### thm "The Pythagorean Theorem"

	Two vectors $\vec{v}, \vec{w} \in #R#^n$ are **orthogonal** if and only if $||\vec{v}||^2 + ||\vec{w}||^2 = ||\vec{v} + \vec{w}||^2$.

###

### exc

	Show that this theorem holds. Start by assuming $\vec{v}$ and $\vec{w}$ are orthogonal and show $||\vec{v}||^2 + ||\vec{w}||^2 = ||\vec{v} + \vec{w}||^2$. Then assume that $||\vec{v}||^2 + ||\vec{w}||^2 = ||\vec{v} + \vec{w}||^2$ and show that $\vec{v}$ and $\vec{w}$ must be orthogonal.

###

Orthogonality will be surprisingly critical to most of the rest of the class. We'll want to talk often about all the vectors orthogonal to a given one, and so we'll give that object a name.

### def "orthogonal complement"

	Let $X$ be a subspace of $#R#^n$. The **orthogonal complement** to $X$ is the subspace $X^\perp$ of $#R#^n$ consisting of the vectors $\vec{v}$ that are orthogonal to every vector in $X$.

###

### ex "orthogonal complement"

	Let $X = \span\left\{ [[ 1 ; 1 ; 0 ]], [[ 0 ; -1 ; 2 ]] \right\}$. Find a basis for $X^\perp$.

	For a vector $\vec{v} = [[ v_1 ; v_2 ; v_3 ]]$ to be in the orthogonal complement, we need

	$$
		[[ v_1 ; v_2 ; v_3 ]] \bullet [[ 1 ; 1 ; 0 ]] &= 0

		[[ v_1 ; v_2 ; v_3 ]] \bullet [[ 0 ; -1 ; 2 ]] &= 0,
	$$

	so

	$$
		v_1 + v_2 &= 0

		-v_2 + 2v_3 &= 0.
	$$

	And that's a matrix equation once again! Row reducing, we have

	$$
		[[ 1, 1, 0 | 0 ; 0, -1, 2 | 0 ]] & 

		[[ 1, 0, 2 | 0 ; 0, -1, 2 | 0 ]] & \quad \vec{r_1} \pe \vec{r_2}

		[[ 1, 0, 2 | 0 ; 0, 1, -2 | 0 ]] & \quad \vec{r_2} \te -1

		\vec{v} = [[ -2t ; 2t ; t ]] & .
	$$

	In total, the orthogonal complement is $X^\perp = \span\left\{ [[ -2 ; 2 ; 1 ]] \right\}$.

###

### exc

	Let $\vec{v} \in #R#^n$. Show that $\vec{v}^\perp$ is a subspace.

###



## Orthonormal Bases

In general

### nav-buttons