### nav-buttons

Before we dive into more linear algebra, I want to take a step back and look at some of its many applications that motivate devoting two terms to its study. Now that we have the terminology of bases, the definition of a linear map $T : V \to W$ couldn't be much simpler: it's a function that splits across addition and scalar multiplication (i.e. $T(cv_1 + v_2) = cT(v_1) + T(v_2)$) that's completely determined by its action on a basis.

With $V = #R#^3$ and $W = #R#^2$, there's the classic example of 3D graphics. *Raster* graphics, the technology underlying most 3D video games, works by modeling 3D shapes as a massive collection of triangles, then projecting each onto the 2D screen by a linear map determined by the position and rotation of the camera. The projection of a 3D triangle is a 2D triangle (this takes a little thought!), and so the result is a collection of triangles in the image plane. We can then test every pixel to see if it's in the interior of a triangle, and color it accordingly if so. At the time of writing, there's no Desmos 3D API I can use to embed a graph directly in the notes, but here's <a href="https://www.desmos.com/3d/947d6c19bf">a small demonstration I put together</a>. The gray lines connecting the vertices of the red triangle in the world to its blue rendered image are projections from the ambient space onto the camera's image plane.

### exc "3D rendering"

	In the previous example, what would be a good choice of bases for the domain and codomain of the projection map to make its matrix representation as simple as possible? Explain your answer --- it doesn't need to be in symbols.

###

On the subject of 3D rendering, I completed a project to render the Thurston geometries not too long ago. These are the eight different geometries possible in curved three-dimensional space; as a two-dimensional analogue, the geometry of the sphere is different than that of the plane. Here's a rendering of three-dimensional *hyperbolic* space --- drag on the scene to look around and use WASD or hold down with two fingers on a touchscreen to move.

### canvas h3-geometry

Linear algebra is all over this project. All of the geometries are represented as curved spaces in 4-dimensional space, similar to how a sphere is curved in 3-dimensional space, and so the facing of the camera is stored as four vectors in $#R#^4$ that are all perpendicular to one another. Although the scene itself looks like an infinite collection of rooms, there's actually only a single one. The camera casts a rays out to render each pixel, and whenever it passes through one of the room's windows, a linear map (i.e. a $4 \times 4$ matrix) gets applied to its position and rotation to teleport it to the opposite window. By doing the same thing to the camera itself when it passes through windows and keeping careful track of the color changes, we get a perfect illusion of an infinite series of rooms with the rendering cost of just one.

When the domain and codomain are the same, linear maps can be *iterated*: applied over and over to an input. If we take four specific maps from $#R#^2$ to itself (let's call them $A$, $B$, $C$, and $D$), we can repeatedly apply them to a starting point and see where it goes. At each step, we take a random map out of the four and apply it to a starting point of $(0, 0)$, then take a random map and apply it to that point, and so on. We'll plot the points it visits --- the brighter the point, the more frequently it's been there. 

### canvas barnsley-fern

The result for these four maps in particular is called the *Barnsley fern*, named after its creator --- amazingly, with enough iterations (10 million in this example), the picture converges to the same thing, even with the random choices of maps. Strictly speaking, these are *affine* linear maps, which means they multiply by a matrix and then add a vector, but it's a good demonstration nevertheless.

As one final application, we can use linear maps to create systems of linear *differential* equations, not just algebraic ones. For example, the system

$$
	x' &= x - y
	y' &= x + y
$$

is really a matrix equation in disguise:

$$
	[[ x' ; y' ]] = [[ 1, -1 ; 1, 1 ]] [[ x ; y ]].
$$

A great way to visualize these is by plotting a field of moving particles, where the velocity of one at $(x, y)$ is given by this formula for $(x', y')$. Coloring the points by their velocity and direction, we get a striking picture.

### canvas vector-field

We'll continue to touch on topics from all of these examples throughout the term. If you'd like to see more from any of them, they come from interactive applets I've written --- have a look!

### image-links
	/applets/thurston-geometries/
	/applets/barnsley-fern/
	/applets/vector-fields/
###



## Eigenvectors

Let's start with a topic from the second example: iterated linear maps. If we're trying to evaluate $A^{100}v$ for an $n \times n$ matrix $A$ and a vector $v \in #R#^n$, then we'd desperately like a trick to avoid doing $100$ matrix-vector products. The best possible scenario would be if we had a basis $\{ v_1, ..., v_n \}$ of fixed points for $A$: vectors so that $Av_i = v_i$. Then for any vector $v = c_1 v_1 + \cdots + c_n v_n$, we'd have

$$
	Av &= A(c_1 v_1 + \cdots + c_n v_n)

	&= c_1 A v_1 + \cdots + c_n A v_n

	&= c_1 v_1 + \cdots + c_n v_n

	&= v,
$$

so $A^{100}v = v$, and we've sidestepped the problem entirely. Unfortunately, this calculation shows that having a basis of fixed points implies that *every* vector is a fixed point of $A$, meaning $A$ has to be the identity matrix $I$. The reality of what we can expect will turn out to not be that far off, though, as surprising as that may be. Let's take a look at an example to get our bearings. With $A$, $v_1$, and $v_2$ defined as

$$
	A &= [[ 5, 6 ; -3, -4 ]]
	
	v_1 &= [[ -2 ; 1 ]]

	v_2 &= [[ 1 ; -1 ]],
$$

we have

$$
	Av_1 &= [[ -4 ; 2 ]] = 2v_1

	Av_2 &= [[ -1 ; 1 ]] = -v_2.
$$

Neither of these is a fixed point, but they're the next best thing. If we want to evaluate $A^{100}v_1$, it's just $2^{100}v_1$, since every application of $A$ just multiplies by 2. Similarly, $A^{100}v_2 = (-1)^{100}v_2 = v_2$. And if we want to evaluate $A^{100}v$ for any other vector $v$, all we have to do is express $v = c_1v_1 + c_2v_2$ (since $v_1$ and $v_2$ are linearly independent and therefore form a basis for $#R#^2$), and then we can apply powers of $A$ with no trouble at all. These objects are important enough that we'll want to give them a name:

### def "eigenvectors and eigenvalues"

	Let $A$ be an $n \times n$ matrix. An **eigenvector** of $A$ is a nonzero vector $v \in #R#^n$ such that $Av = \lambda v$ for some number $\lambda \in #R#$, called the **eigenvalue** corresponding to $v$.

###

*Eigen* is German for *characteristic* (and *strange*), and refers to the way these objects are intrinsic to the matrix they come from. Eigenvectors and eigenvalues are defined only for square matrices, since if $A$ isn't square, its outputs have different lengths than its inputs, so there's no hope of the two being multiples of one another. We also exclude the zero vector since $A\vec{0} = \vec{0}$

So --- how can we find the eigenvectors and eigenvalues of a matrix from scratch? We can rewrite the equation $Av = \lambda v$ as $Av - \lambda v = \vec{0}$, and to express the left side as a single linear map acting on $v$, we can write it as $(A - \lambda I)v = \vec{0}$. If we knew $\lambda$, we could solve for $v$ by row reduction, we usually won't know the eigenvalues ahead of time. To that end, let's start by finding all the possible values for $\lambda$. If $(A - \lambda I)v = \vec{0}$, then $v \in \ker (A - \lambda I)$, which means $A - \lambda I$ isn't one-to-one. An equivalent condition is that $\det(A - \lambda I) = 0$, and that's an equation we can solve. Since the determinant is just a complicated series of multiplications and additions, the result will be a polynomial in $\lambda$, which we call the **characteristic polynomial** of $A$ and denote $\chi_A(\lambda)$. Once we solve $\chi_A(\lambda) = 0$ for the eigenvalues of $A$, we can then plug each into $(A - \lambda I)v = \vec{0}$ in turn to find the corresponding eigenvectors.

### ex "eigenvalues and eigenvectors"

	Find the eigenvalues and eigenvectors of

	$$
		A = [[ 5, -3 ; 2, -2 ]].
	$$
	
	To get the characteristic polynomial, we subtract $\lambda I$, which just means subtracting $\lambda$ from every entry in the diagonal, and then take the determinant.
	
	$$
		\det (A - \lambda I) &= \det [[ 5 - \lambda, -3 ; 2, -2 - \lambda ]]
		
		&= (5 - \lambda)(-2 - \lambda) - (-3)(2)
		
		&= -10 - 3\lambda + \lambda^2 + 6
		
		&= \lambda^2 - 3\lambda - 4
		
		&= (\lambda - 4)(\lambda + 1).
	$$
	
	Since the characteristic polynomial is supposed to equal zero, the eigenvalues are $4$ and $-1$. We'll handle them one at a time to find the eigenvectors.
	
	First, let's take $\lambda = 4$. That gives us the system $(A - 4 I)v = 0$, which corresponds to the augmented matrix
	
	$$
		[[ 1, -3 | 0 ; 2, -6 | 0 ]] & 
		
		[[ 1, -3 | 0 ; 0, 0 | 0 ]] & \qquad r_2 \pe -2r_1.
	$$
	
	In the form of an equation, $v_1 - 3v_2 = 0$, so $v_1 = 3v_2$. We just need one eigenvector, so let's take $v_2 = 1$ and $v_1 = 3$ to get our first eigenvector of
	
	$$
		v = [[ 3 ; 1 ]]
	$$
	
	with $\lambda_1 = 4$. For the other eigenvalue, we have
	
	$$
		[[ 6, -3 | 0 ; 2, -1 | 0 ]] & 
			
		[[ 6, -3 | 0 ; 0, 0 | 0 ]] & \qquad r_2 \pe -\frac{1}{3}r_1.
	$$
	
	Now $6v_1 - 3v_2 = 0$, so $2v_1 = v_2$. We'll just take $v_1 = 1$ and $v_2 = 2$ to get our second eigenvector of
	
	$$
		v = [[ 1 ; 2 ]].
	$$

###

### exc "eigenvectors and eigenvalues"
	
	Find the eigenvalues and eigenvectors of
	
	$$
		B = [[ 1, 1, 0 ; 0, 0, 0 ; 0, 1, 1 ]].
	$$
	
###

As we mentioned before, we'd ideally like a *basis* of eigenvectors for a matrix. The characteristic polynomial $\chi_A(\lambda)$ of an $n \times n$ matrix is always degree-$n$, since there's exactly one term in the determinant expansion that involves multiplying together every diagonal entry of $A - \lambda I$, each of which is of the form $a_{ii} - \lambda$. That means that we'll at least have $n$ eigenvalues $\lambda$, but that number is counted with multiplicity --- the eigenvalues could be repeated or even complex numbers. We'll dig more into those possibilities in future sections, but for now, we can at least verify that when all the eigenvalues are distinct, we get the basis of eigenvectors we're looking for.

### prop "distinct eigenvalues guarantee a basis of eigenvectors"

	Let $A$ be an $n \times n$ matrix with eigenvalues $\lambda_1, ..., \lambda_n$ so that $\lambda_i \neq \lambda_j$ for $i \neq j$. Then the corresponding eigenvectors $v_1, ..., v_n$ form a basis for $#R#^n$.

###

### pf

	We'll often get the chance to prove our results in this course, and it's worth taking it whenever we can. While this course isn't proof-based, it's good preparation for future ones that are, and there's usually value in seeing *why* results are true regardless.

	Since $v_1, ..., v_n$ is a list of $n$ vectors in $#R#^n$, we only need to check if they're linearly independent or they span --- if one is true, the other must be (this might be a more familiar fact if the $v_i$ are placed as columns in a matrix; then the two conditions are that the corresponding map is one-to-one or onto). Linear independence is usually an easier condition to check, so let's try that.

	We'll show this result one vector at a time, first showing that $v_1$ and $v_2$ are linearly independent. If $c_1v_1 + c_2v_2 = \vec{0}$, then all we can really do is apply $A$ to both sides --- otherwise, the $\lambda_i$ don't appear. The result of doing that is

	$$
		c_1 \lambda_1 v_1 + c_2 \lambda_2 v_2 = \vec{0},
	$$

	which looks very similar to our original equation. Multiplying it by $\lambda_1$ results in

	$$
		c_1 \lambda_1 v_1 + c_2 \lambda_1 v_2 = \vec{0},
	$$

	and subtracting these two equations from one another, we have

	$$
		c_2 (\lambda_2 - \lambda_1) v_2 &= \vec{0}

		c_2 (\lambda_2 - \lambda_1) v_2 &= \vec{0}.
	$$

	Since $v_2$ is an eigenvector, it's not the zero vector, and $\lambda_1 \neq \lambda_2$ by assumption, so $c_2 = 0$. Then $c_1 = 0$ too since $v_1 \neq \vec{0}$, and so $v_1$ and $v_2$ are linearly independent. 

###



### nav-buttons