<header><div id="logo"><a href="/home/" tabindex="-1"><img src="/graphics/general-icons/logo.png" alt="Logo" tabindex="1"></img></a></div><div style="height: 20px"></div><h1 class="heading-text">Section 1: Eigenvectors and Eigenvalues</h1></header><main><section><div class="text-buttons nav-buttons"><div class="focus-on-child tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div><p class="body-text">Before we dive into more linear algebra, I want to take a step back and look at some of its many applications that motivate devoting two terms to its study. Now that we have the terminology of bases, the definition of a linear map <span class="tex-holder inline-math" data-source-tex="T : V \to W">$T : V \to W$</span> couldn&#x2019;t be much simpler: it&#x2019;s a function that splits across addition and scalar multiplication (i.e. <span class="tex-holder inline-math" data-source-tex="T(cv_1 + v_2) = cT(v_1) + T(v_2)">$T(cv_1 + v_2) = cT(v_1) + T(v_2)$</span>) that&#x2019;s completely determined by its action on a basis.</p><p class="body-text">With <span class="tex-holder inline-math" data-source-tex="V = \mathbb{R}^3">$V = \mathbb{R}^3$</span> and <span class="tex-holder inline-math" data-source-tex="W = \mathbb{R}^2">$W = \mathbb{R}^2$</span>, there&#x2019;s the classic example of 3D graphics. <em>Raster</em> graphics, the technology underlying most 3D video games, works by modeling 3D shapes as a massive collection of triangles, then projecting each onto the 2D screen by a linear map determined by the position and rotation of the camera. The projection of a 3D triangle is a 2D triangle (this takes a little thought!), and so the result is a collection of triangles in the image plane. We can then test every pixel to see if it&#x2019;s in the interior of a triangle, and color it accordingly if so. At the time of writing, there&#x2019;s no Desmos 3D API I can use to embed a graph directly in the notes, but here&#x2019;s <a href="https://www.desmos.com/3d/947d6c19bf">a small demonstration I put together</a>. The gray lines connecting the vertices of the red triangle in the world to its blue rendered image are projections from the ambient space onto the camera&#x2019;s image plane.</p><div class="notes-exc notes-environment"><p class="body-text"</p><span class="notes-exc-title">Exercise: 3D rendering</span></p><p class="body-text">In the previous example, what would be a good choice of bases for the domain and codomain of the projection map to make its matrix representation as simple as possible? Explain your answer &mdash; it doesn&#x2019;t need to be in symbols.</p></div><p class="body-text">On the subject of 3D rendering, I completed a project to render the Thurston geometries not too long ago. These are the eight different geometries possible in curved three-dimensional space; as a two-dimensional analogue, the geometry of the sphere is different than that of the plane. Here&#x2019;s a rendering of three-dimensional <em>hyperbolic</em> space &mdash; drag on the scene to look around and use WASD or hold down with two fingers on a touchscreen to move.</p><div class="desmos-border"><canvas id="h3-geometry-canvas" class="output-canvas"></canvas></div><p class="body-text">Linear algebra is all over this project. All of the geometries are represented as curved spaces in 4-dimensional space, similar to how a sphere is curved in 3-dimensional space, and so the facing of the camera is stored as four vectors in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^4">$\mathbb{R}^4$</span> that are all perpendicular to one another. Although the scene itself looks like an infinite collection of rooms, there&#x2019;s actually only a single one. The camera casts a rays out to render each pixel, and whenever it passes through one of the room&#x2019;s windows, a linear map (i.e. a <span class="tex-holder inline-math" data-source-tex="4 \times 4">$4 \times 4$</span> matrix) gets applied to its position and rotation to teleport it to the opposite window. By doing the same thing to the camera itself when it passes through windows and keeping careful track of the color changes, we get a perfect illusion of an infinite series of rooms with the rendering cost of just one.</p><p class="body-text">When the domain and codomain are the same, linear maps can be <em>iterated</em>: applied over and over to an input. If we take four specific maps from <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^2">$\mathbb{R}^2$</span> to itself (let&#x2019;s call them <span class="tex-holder inline-math" data-source-tex="A">$A$</span>, <span class="tex-holder inline-math" data-source-tex="B">$B$</span>, <span class="tex-holder inline-math" data-source-tex="C">$C$</span>, and <span class="tex-holder inline-math" data-source-tex="D">$D$</span>), we can repeatedly apply them to a starting point and see where it goes. At each step, we take a random map out of the four and apply it to a starting point of <span class="tex-holder inline-math" data-source-tex="(0, 0)">$(0, 0)$</span>, then take a random map and apply it to that point, and so on. We&#x2019;ll plot the points it visits &mdash; the brighter the point, the more frequently it&#x2019;s been there. </p><div class="desmos-border"><canvas id="barnsley-fern-canvas" class="output-canvas"></canvas></div><p class="body-text">The result for these four maps in particular is called the <em>Barnsley fern</em>, named after its creator &mdash; amazingly, with enough iterations (10 million in this example), the picture converges to the same thing, even with the random choices of maps. Strictly speaking, these are <em>affine</em> linear maps, which means they multiply by a matrix and then add a vector, but it&#x2019;s a good demonstration nevertheless.</p><p class="body-text">As one final application, we can use linear maps to create systems of linear <em>differential</em> equations, not just algebraic ones. For example, the system</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]x' &= x - y\\[NEWLINE][TAB]y' &= x + y[NEWLINE]\end{align*}">$$\begin{align*}x' &= x - y\\[4px]y' &= x + y\end{align*}$$</span></p><p class="body-text">is really a matrix equation in disguise:</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{c} x' \\ y' \end{array}\right] = \left[\begin{array}{cc} 1& -1 \\ 1& 1 \end{array}\right] \left[\begin{array}{c} x \\ y \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{c} x' \\ y' \end{array}\right] = \left[\begin{array}{cc} 1& -1 \\ 1& 1 \end{array}\right] \left[\begin{array}{c} x \\ y \end{array}\right].\end{align*}$$</span></p><p class="body-text">A great way to visualize these is by plotting a field of moving particles, where the velocity of one at <span class="tex-holder inline-math" data-source-tex="(x, y)">$(x, y)$</span> is given by this formula for <span class="tex-holder inline-math" data-source-tex="(x', y')">$(x', y')$</span>. Coloring the points by their velocity and direction, we get a striking picture.</p><div class="desmos-border"><canvas id="vector-field-canvas" class="output-canvas"></canvas></div><p class="body-text">We&#x2019;ll continue to touch on topics from all of these examples throughout the term. If you&#x2019;d like to see more from any of them, they come from interactive applets I&#x2019;ve written &mdash; have a look!</p><div class="image-links"><div class="image-link"><a href="/applets/thurston-geometries/" tabindex="-1"><img src="/applets/thurston-geometries/cover.webp" alt="Thurston Geometries" tabindex="1"></img></a><p class="image-link-subtext">Thurston Geometries</p></div><div class="image-link"><a href="/applets/barnsley-fern/" tabindex="-1"><img src="/applets/barnsley-fern/cover.webp" alt="The Barnsley Fern" tabindex="1"></img></a><p class="image-link-subtext">The Barnsley Fern</p></div><div class="image-link"><a href="/applets/vector-fields/" tabindex="-1"><img src="/applets/vector-fields/cover.webp" alt="Vector Fields" tabindex="1"></img></a><p class="image-link-subtext">Vector Fields</p></div></div></section><h2 class="section-text"> Eigenvectors</h2><section><p class="body-text">Let&#x2019;s start with a topic from the second example: iterated linear maps. If we&#x2019;re trying to evaluate <span class="tex-holder inline-math" data-source-tex="A^{100}v">$A^{100}v$</span> for an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix <span class="tex-holder inline-math" data-source-tex="A">$A$</span> and a vector <span class="tex-holder inline-math" data-source-tex="v \in \mathbb{R}^n">$v \in \mathbb{R}^n$</span>, then we&#x2019;d desperately like a trick to avoid doing <span class="tex-holder inline-math" data-source-tex="100">$100$</span> matrix-vector products. The best possible scenario would be if we had a basis <span class="tex-holder inline-math" data-source-tex="\{ v_1, ..., v_n \}">$\{ v_1, ..., v_n \}$</span> of fixed points for <span class="tex-holder inline-math" data-source-tex="A">$A$</span>: vectors so that <span class="tex-holder inline-math" data-source-tex="Av_i = v_i">$Av_i = v_i$</span>. Then for any vector <span class="tex-holder inline-math" data-source-tex="v = c_1 v_1 + \cdots + c_n v_n">$v = c_1 v_1 + \cdots + c_n v_n$</span>, we&#x2019;d have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]Av &= A(c_1 v_1 + \cdots + c_n v_n)\\[NEWLINE][TAB]&= c_1 A v_1 + \cdots + c_n A v_n\\[NEWLINE][TAB]&= c_1 v_1 + \cdots + c_n v_n\\[NEWLINE][TAB]&= v,[NEWLINE]\end{align*}">$$\begin{align*}Av &= A(c_1 v_1 + \cdots + c_n v_n)\\[4px]&= c_1 A v_1 + \cdots + c_n A v_n\\[4px]&= c_1 v_1 + \cdots + c_n v_n\\[4px]&= v,\end{align*}$$</span></p><p class="body-text">so <span class="tex-holder inline-math" data-source-tex="A^{100}v = v">$A^{100}v = v$</span>, and we&#x2019;ve sidestepped the problem entirely. Unfortunately, this calculation shows that having a basis of fixed points implies that <em>every</em> vector is a fixed point of <span class="tex-holder inline-math" data-source-tex="A">$A$</span>, meaning <span class="tex-holder inline-math" data-source-tex="A">$A$</span> has to be the identity matrix <span class="tex-holder inline-math" data-source-tex="I">$I$</span>. The reality of what we can expect will turn out to not be that far off, though, as surprising as that may be. Let&#x2019;s take a look at an example to get our bearings. With <span class="tex-holder inline-math" data-source-tex="A">$A$</span>, <span class="tex-holder inline-math" data-source-tex="v_1">$v_1$</span>, and <span class="tex-holder inline-math" data-source-tex="v_2">$v_2$</span> defined as</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A &= \left[\begin{array}{cc} 5& 6 \\ -3& -4 \end{array}\right]\\[NEWLINE][TAB]v_1 &= \left[\begin{array}{c} -2 \\ 1 \end{array}\right]\\[NEWLINE][TAB]v_2 &= \left[\begin{array}{c} 1 \\ -1 \end{array}\right],[NEWLINE]\end{align*}">$$\begin{align*}A &= \left[\begin{array}{cc} 5& 6 \\ -3& -4 \end{array}\right]\\[4px]v_1 &= \left[\begin{array}{c} -2 \\ 1 \end{array}\right]\\[4px]v_2 &= \left[\begin{array}{c} 1 \\ -1 \end{array}\right],\end{align*}$$</span></p><p class="body-text">we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]Av_1 &= \left[\begin{array}{c} -4 \\ 2 \end{array}\right] = 2v_1\\[NEWLINE][TAB]Av_2 &= \left[\begin{array}{c} -1 \\ 1 \end{array}\right] = -v_2.[NEWLINE]\end{align*}">$$\begin{align*}Av_1 &= \left[\begin{array}{c} -4 \\ 2 \end{array}\right] = 2v_1\\[4px]Av_2 &= \left[\begin{array}{c} -1 \\ 1 \end{array}\right] = -v_2.\end{align*}$$</span></p><p class="body-text">Neither of these is a fixed point, but they&#x2019;re the next best thing. If we want to evaluate <span class="tex-holder inline-math" data-source-tex="A^{100}v_1">$A^{100}v_1$</span>, it&#x2019;s just <span class="tex-holder inline-math" data-source-tex="2^{100}v_1">$2^{100}v_1$</span>, since every application of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> just multiplies by 2. Similarly, <span class="tex-holder inline-math" data-source-tex="A^{100}v_2 = (-1)^{100}v_2 = v_2">$A^{100}v_2 = (-1)^{100}v_2 = v_2$</span>. And if we want to evaluate <span class="tex-holder inline-math" data-source-tex="A^{100}v">$A^{100}v$</span> for any other vector <span class="tex-holder inline-math" data-source-tex="v">$v$</span>, all we have to do is express <span class="tex-holder inline-math" data-source-tex="v = c_1v_1 + c_2v_2">$v = c_1v_1 + c_2v_2$</span> (since <span class="tex-holder inline-math" data-source-tex="v_1">$v_1$</span> and <span class="tex-holder inline-math" data-source-tex="v_2">$v_2$</span> are linearly independent and therefore form a basis for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^2">$\mathbb{R}^2$</span>), and then we can apply powers of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> with no trouble at all. These objects are important enough that we&#x2019;ll want to give them a name:</p><div class="notes-def notes-environment"><p class="body-text"</p><span class="notes-def-title">Definition: eigenvectors and eigenvalues</span></p><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix. An <strong>eigenvector</strong> of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> is a nonzero vector <span class="tex-holder inline-math" data-source-tex="v \in \mathbb{R}^n">$v \in \mathbb{R}^n$</span> such that <span class="tex-holder inline-math" data-source-tex="Av = \lambda v">$Av = \lambda v$</span> for some number <span class="tex-holder inline-math" data-source-tex="\lambda \in \mathbb{R}">$\lambda \in \mathbb{R}$</span>, called the <strong>eigenvalue</strong> corresponding to <span class="tex-holder inline-math" data-source-tex="v">$v$</span>.</p></div><p class="body-text"><em>Eigen</em> is German for <em>characteristic</em> (and <em>strange</em>), and refers to the way these objects are intrinsic to the matrix they come from. Eigenvectors and eigenvalues are defined only for square matrices, since if <span class="tex-holder inline-math" data-source-tex="A">$A$</span> isn&#x2019;t square, its outputs have different lengths than its inputs, so there&#x2019;s no hope of the two being multiples of one another. We also exclude the zero vector since <span class="tex-holder inline-math" data-source-tex="A\vec{0} = \vec{0}">$A\vec{0} = \vec{0}$</span></p><p class="body-text">So &mdash; how can we find the eigenvectors and eigenvalues of a matrix from scratch? We can rewrite the equation <span class="tex-holder inline-math" data-source-tex="Av = \lambda v">$Av = \lambda v$</span> as <span class="tex-holder inline-math" data-source-tex="Av - \lambda v = \vec{0}">$Av - \lambda v = \vec{0}$</span>, and to express the left side as a single linear map acting on <span class="tex-holder inline-math" data-source-tex="v">$v$</span>, we can write it as <span class="tex-holder inline-math" data-source-tex="(A - \lambda I)v = \vec{0}">$(A - \lambda I)v = \vec{0}$</span>. If we knew <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$</span>, we could solve for <span class="tex-holder inline-math" data-source-tex="v">$v$</span> by row reduction, we usually won&#x2019;t know the eigenvalues ahead of time. To that end, let&#x2019;s start by finding all the possible values for <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$</span>. If <span class="tex-holder inline-math" data-source-tex="(A - \lambda I)v = \vec{0}">$(A - \lambda I)v = \vec{0}$</span>, then <span class="tex-holder inline-math" data-source-tex="v \in \ker (A - \lambda I)">$v \in \ker (A - \lambda I)$</span>, which means <span class="tex-holder inline-math" data-source-tex="A - \lambda I">$A - \lambda I$</span> isn&#x2019;t one-to-one. An equivalent condition is that <span class="tex-holder inline-math" data-source-tex="\det(A - \lambda I) = 0">$\det(A - \lambda I) = 0$</span>, and that&#x2019;s an equation we can solve. Since the determinant is just a complicated series of multiplications and additions, the result will be a polynomial in <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$</span>, which we call the <strong>characteristic polynomial</strong> of <span class="tex-holder inline-math" data-source-tex="A">$A$</span> and denote <span class="tex-holder inline-math" data-source-tex="\chi_A(\lambda)">$\chi_A(\lambda)$</span>. Once we solve <span class="tex-holder inline-math" data-source-tex="\chi_A(\lambda) = 0">$\chi_A(\lambda) = 0$</span> for the eigenvalues of <span class="tex-holder inline-math" data-source-tex="A">$A$</span>, we can then plug each into <span class="tex-holder inline-math" data-source-tex="(A - \lambda I)v = \vec{0}">$(A - \lambda I)v = \vec{0}$</span> in turn to find the corresponding eigenvectors.</p><div class="notes-ex notes-environment"><p class="body-text"</p><span class="notes-ex-title">Example: eigenvalues and eigenvectors</span></p><p class="body-text">Find the eigenvalues and eigenvectors of</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]A = \left[\begin{array}{cc} 5& -3 \\ 2& -2 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}A = \left[\begin{array}{cc} 5& -3 \\ 2& -2 \end{array}\right].\end{align*}$$</span></p><p class="body-text">To get the characteristic polynomial, we subtract <span class="tex-holder inline-math" data-source-tex="\lambda I">$\lambda I$</span>, which just means subtracting <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$</span> from every entry in the diagonal, and then take the determinant.</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\det (A - \lambda I) &= \det \left[\begin{array}{cc} 5 - \lambda& -3 \\ 2& -2 - \lambda \end{array}\right]\\[NEWLINE][TAB]&= (5 - \lambda)(-2 - \lambda) - (-3)(2)\\[NEWLINE][TAB]&= -10 - 3\lambda + \lambda^2 + 6\\[NEWLINE][TAB]&= \lambda^2 - 3\lambda - 4\\[NEWLINE][TAB]&= (\lambda - 4)(\lambda + 1).[NEWLINE]\end{align*}">$$\begin{align*}\det (A - \lambda I) &= \det \left[\begin{array}{cc} 5 - \lambda& -3 \\ 2& -2 - \lambda \end{array}\right]\\[4px]&= (5 - \lambda)(-2 - \lambda) - (-3)(2)\\[4px]&= -10 - 3\lambda + \lambda^2 + 6\\[4px]&= \lambda^2 - 3\lambda - 4\\[4px]&= (\lambda - 4)(\lambda + 1).\end{align*}$$</span></p><p class="body-text">Since the characteristic polynomial is supposed to equal zero, the eigenvalues are <span class="tex-holder inline-math" data-source-tex="4">$4$</span> and <span class="tex-holder inline-math" data-source-tex="-1">$-1$</span>. We&#x2019;ll handle them one at a time to find the eigenvectors.</p><p class="body-text">First, let&#x2019;s take <span class="tex-holder inline-math" data-source-tex="\lambda = 4">$\lambda = 4$</span>. That gives us the system <span class="tex-holder inline-math" data-source-tex="(A - 4 I)v = 0">$(A - 4 I)v = 0$</span>, which corresponds to the augmented matrix</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{cc|c} 1& -3 & 0 \\ 2& -6 & 0 \end{array}\right] & \\[NEWLINE][TAB]\left[\begin{array}{cc|c} 1& -3 & 0 \\ 0& 0 & 0 \end{array}\right] & \qquad r_2 \ +\!\!= -2r_1.[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{cc|c} 1& -3 & 0 \\ 2& -6 & 0 \end{array}\right] & \\[4px]\left[\begin{array}{cc|c} 1& -3 & 0 \\ 0& 0 & 0 \end{array}\right] & \qquad r_2 \ +\!\!= -2r_1.\end{align*}$$</span></p><p class="body-text">In the form of an equation, <span class="tex-holder inline-math" data-source-tex="v_1 - 3v_2 = 0">$v_1 - 3v_2 = 0$</span>, so <span class="tex-holder inline-math" data-source-tex="v_1 = 3v_2">$v_1 = 3v_2$</span>. We just need one eigenvector, so let&#x2019;s take <span class="tex-holder inline-math" data-source-tex="v_2 = 1">$v_2 = 1$</span> and <span class="tex-holder inline-math" data-source-tex="v_1 = 3">$v_1 = 3$</span> to get our first eigenvector of</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]v = \left[\begin{array}{c} 3 \\ 1 \end{array}\right][NEWLINE]$$">$$\begin{align*}v = \left[\begin{array}{c} 3 \\ 1 \end{array}\right]\end{align*}$$</span></p><p class="body-text">with <span class="tex-holder inline-math" data-source-tex="\lambda_1 = 4">$\lambda_1 = 4$</span>. For the other eigenvalue, we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]\left[\begin{array}{cc|c} 6& -3 & 0 \\ 2& -1 & 0 \end{array}\right] & \\[NEWLINE][TAB]\left[\begin{array}{cc|c} 6& -3 & 0 \\ 0& 0 & 0 \end{array}\right] & \qquad r_2 \ +\!\!= -\frac{1}{3}r_1.[NEWLINE]\end{align*}">$$\begin{align*}\left[\begin{array}{cc|c} 6& -3 & 0 \\ 2& -1 & 0 \end{array}\right] & \\[4px]\left[\begin{array}{cc|c} 6& -3 & 0 \\ 0& 0 & 0 \end{array}\right] & \qquad r_2 \ +\!\!= -\frac{1}{3}r_1.\end{align*}$$</span></p><p class="body-text">Now <span class="tex-holder inline-math" data-source-tex="6v_1 - 3v_2 = 0">$6v_1 - 3v_2 = 0$</span>, so <span class="tex-holder inline-math" data-source-tex="2v_1 = v_2">$2v_1 = v_2$</span>. We&#x2019;ll just take <span class="tex-holder inline-math" data-source-tex="v_1 = 1">$v_1 = 1$</span> and <span class="tex-holder inline-math" data-source-tex="v_2 = 2">$v_2 = 2$</span> to get our second eigenvector of</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]v = \left[\begin{array}{c} 1 \\ 2 \end{array}\right].[NEWLINE]$$">$$\begin{align*}v = \left[\begin{array}{c} 1 \\ 2 \end{array}\right].\end{align*}$$</span></p></div><div class="notes-exc notes-environment"><p class="body-text"</p><span class="notes-exc-title">Exercise: eigenvectors and eigenvalues</span></p><p class="body-text">Find the eigenvalues and eigenvectors of</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]B = \left[\begin{array}{ccc} 1& 1& 0 \\ 0& 0& 0 \\ 0& 1& 1 \end{array}\right].[NEWLINE]\end{align*}">$$\begin{align*}B = \left[\begin{array}{ccc} 1& 1& 0 \\ 0& 0& 0 \\ 0& 1& 1 \end{array}\right].\end{align*}$$</span></p></div><p class="body-text">As we mentioned before, we&#x2019;d ideally like a <em>basis</em> of eigenvectors for a matrix. The characteristic polynomial <span class="tex-holder inline-math" data-source-tex="\chi_A(\lambda)">$\chi_A(\lambda)$</span> of an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix is always degree-<span class="tex-holder inline-math" data-source-tex="n">$n$</span>, since there&#x2019;s exactly one term in the determinant expansion that involves multiplying together every diagonal entry of <span class="tex-holder inline-math" data-source-tex="A - \lambda I">$A - \lambda I$</span>, each of which is of the form <span class="tex-holder inline-math" data-source-tex="a_{ii} - \lambda">$a_{ii} - \lambda$</span>. That means that we&#x2019;ll at least have <span class="tex-holder inline-math" data-source-tex="n">$n$</span> eigenvalues <span class="tex-holder inline-math" data-source-tex="\lambda">$\lambda$</span>, but that number is counted with multiplicity &mdash; the eigenvalues could be repeated or even complex numbers. We&#x2019;ll dig more into those possibilities in future sections, but for now, we can at least verify that when all the eigenvalues are distinct, we get the basis of eigenvectors we&#x2019;re looking for.</p><div class="notes-prop notes-environment"><p class="body-text"</p><span class="notes-prop-title">Proposition: distinct eigenvalues guarantee a basis of eigenvectors</span></p><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="A">$A$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix with eigenvalues <span class="tex-holder inline-math" data-source-tex="\lambda_1, ..., \lambda_n">$\lambda_1, ..., \lambda_n$</span> so that <span class="tex-holder inline-math" data-source-tex="\lambda_i \neq \lambda_j">$\lambda_i \neq \lambda_j$</span> for <span class="tex-holder inline-math" data-source-tex="i \neq j">$i \neq j$</span>. Then the corresponding eigenvectors <span class="tex-holder inline-math" data-source-tex="v_1, ..., v_n">$v_1, ..., v_n$</span> form a basis for <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$</span>.</p></div><div class="notes-pf notes-environment"><p class="body-text"</p><span class="notes-pf-title">Proof</span></p><p class="body-text">We&#x2019;ll often get the chance to prove our results in this course, and it&#x2019;s worth taking it whenever we can. While this course isn&#x2019;t proof-based, it&#x2019;s good preparation for future ones that are, and there&#x2019;s usually value in seeing <em>why</em> results are true regardless.</p><p class="body-text">Since <span class="tex-holder inline-math" data-source-tex="v_1, ..., v_n">$v_1, ..., v_n$</span> is a list of <span class="tex-holder inline-math" data-source-tex="n">$n$</span> vectors in <span class="tex-holder inline-math" data-source-tex="\mathbb{R}^n">$\mathbb{R}^n$</span>, we only need to check if they&#x2019;re linearly independent or they span &mdash; if one is true, the other must be (this might be a more familiar fact if the <span class="tex-holder inline-math" data-source-tex="v_i">$v_i$</span> are placed as columns in a matrix; then the two conditions are that the corresponding map is one-to-one or onto). Linear independence is usually an easier condition to check, so let&#x2019;s try that.</p><p class="body-text">We&#x2019;ll show this result one vector at a time, first showing that <span class="tex-holder inline-math" data-source-tex="v_1">$v_1$</span> and <span class="tex-holder inline-math" data-source-tex="v_2">$v_2$</span> are linearly independent. If <span class="tex-holder inline-math" data-source-tex="c_1v_1 + c_2v_2 = \vec{0}">$c_1v_1 + c_2v_2 = \vec{0}$</span>, then all we can really do is apply <span class="tex-holder inline-math" data-source-tex="A">$A$</span> to both sides &mdash; otherwise, the <span class="tex-holder inline-math" data-source-tex="\lambda_i">$\lambda_i$</span> don&#x2019;t appear. The result of doing that is</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]c_1 \lambda_1 v_1 + c_2 \lambda_2 v_2 = \vec{0},[NEWLINE]$$">$$\begin{align*}c_1 \lambda_1 v_1 + c_2 \lambda_2 v_2 = \vec{0},\end{align*}$$</span></p><p class="body-text">which looks very similar to our original equation. Multiplying it by <span class="tex-holder inline-math" data-source-tex="\lambda_1">$\lambda_1$</span> results in</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]c_1 \lambda_1 v_1 + c_2 \lambda_1 v_2 = \vec{0},[NEWLINE]$$">$$\begin{align*}c_1 \lambda_1 v_1 + c_2 \lambda_1 v_2 = \vec{0},\end{align*}$$</span></p><p class="body-text">and subtracting these two equations from one another, we have</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="\begin{align*}[NEWLINE][TAB]c_2 (\lambda_2 - \lambda_1) v_2 &= \vec{0}\\[NEWLINE][TAB]c_2 (\lambda_2 - \lambda_1) v_2 &= \vec{0}.[NEWLINE]\end{align*}">$$\begin{align*}c_2 (\lambda_2 - \lambda_1) v_2 &= \vec{0}\\[4px]c_2 (\lambda_2 - \lambda_1) v_2 &= \vec{0}.\end{align*}$$</span></p><p class="body-text">Since <span class="tex-holder inline-math" data-source-tex="v_2">$v_2$</span> is an eigenvector, it&#x2019;s not the zero vector, and <span class="tex-holder inline-math" data-source-tex="\lambda_1 \neq \lambda_2">$\lambda_1 \neq \lambda_2$</span> by assumption, so <span class="tex-holder inline-math" data-source-tex="c_2 = 0">$c_2 = 0$</span>. Then <span class="tex-holder inline-math" data-source-tex="c_1 = 0">$c_1 = 0$</span> too since <span class="tex-holder inline-math" data-source-tex="v_1 \neq \vec{0}">$v_1 \neq \vec{0}$</span>, and so <span class="tex-holder inline-math" data-source-tex="v_1">$v_1$</span> and <span class="tex-holder inline-math" data-source-tex="v_2">$v_2$</span> are linearly independent. </p></div><div class="text-buttons nav-buttons"><div class="focus-on-child tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div></section></main>