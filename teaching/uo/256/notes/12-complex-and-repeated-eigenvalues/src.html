### nav-buttons



So far, we know how to handle systems of the form $\mathbf{x}' = \mathbf{Ax}$ when the eigenvalues of $\mathbf{A}$ are real and distinct. Just like with $n$th-order individual DEs, we'll now work on extending that to all matrices $\mathbf{A}$, regardless of the eigenvalues.



## Complex Eigenvalues

Much like we only discussed $n$th-order DEs with real coefficients, we'll only be considering matrices with real entries. That means the characteristic polynomials will have only real coefficients, and so any complex eigenvalues will come in conjugate pairs. If $\mathbf{A}$ has eigenvalues of $\lambda = \alpha \pm \beta i$ corresponding to eigenvectors $\mathbf{v_+}$ and $\mathbf{v_-}$, then

$$
	\mathbf{A}\mathbf{v_+} &= \left(\alpha + \beta i \right) \mathbf{v_+}.
$$

Let's now take the complex conjugate of both sides (flipping the sign of any imaginary number), which is typically indicated by putting a bar above a variable.

$$
	\overline{\mathbf{A}\mathbf{v_+}} &= \overline{\left(\alpha + \beta i \right) \mathbf{v_+}}.
$$

For reasons slightly out of scope of this class, conjugation splits across multiplication, so we have

$$
	\overline{\mathbf{A}} \overline{\mathbf{v_+}} &= \overline{\left(\alpha + \beta i \right)} \overline{\mathbf{v_+}}.
$$

Finally, $\overline{\alpha + \beta i} = \alpha - \beta i$, and $\overline{\mathbf{A}} = \mathbf{A}$, since all the entries of $\mathbf{A}$ are real by assumption. Therefore,

$$
	\mathbf{A} \overline{\mathbf{v_+}} &= \left(\alpha - \beta i \right) \overline{\mathbf{v_+}}.
$$

But that's exactly what it means to be $\mathbf{v_-}$! What we've shown is that if $\mathbf{A}$ has real entries, then it's not just its eigenvalues that come in conjugate pairs --- the corresponding eigenvectors do too.

With individual DEs, we were able to extract two fundamental solutions immediately from a pair of conjugate roots to the characteristic equation by taking the real and imaginary parts of a single one of the two solutions, and we'll do the same here. Let's work through an example to see this all in action.



### ex complex eigenvalues
	
	A species of fish in a pond feeds primarily on a certain strain of algae --- with $p_1(t)$ and $p_2(t)$ giving the total biomass of fish and algae in $\text{kg}$ after $t$ days, respectively, they satisfy the system
	
	$$
		p_1' &= p_1 + p_2
		
		p_2' &= -2p_1 + 3p_2.
	$$
	
	At time $t = 0$, there are $100\,\text{kg}$ of both fish and algae. Find $\mathbf{p}$ as a function of $t$ and sketch a vector field.
	
	This problem proceeds familiarly, but we'll have the added wrinkle of complex eigenvalues. To get started, let's write it as a matrix.
	
	$$
		\mathbf{p}' = \left[ \begin{array}{cc} 1 & 1 \\ -2 & 3 \end{array} \right] \mathbf{p}.
	$$
	
	Calling that matrix $\mathbf{A}$, we first need to solve $\det (\mathbf{A} - \lambda \mathbf{I}) = 0$. That gives us
	
	$$
		\det \left[ \begin{array}{cc} 1 - \lambda & 1 \\ -2 & 3 - \lambda \end{array} \right] &= 0
		
		(1 - \lambda)(3 - \lambda) - (1)(-2) &= 0
		
		\lambda^2 - 4\lambda + 5 &= 0
		
		\lambda &= \frac{4 \pm \sqrt{16 - 20}}{2}
		
		\lambda &= \frac{4 \pm 2i}{2}
		
		\lambda &= 2 \pm i.
	$$
	
	That's the eigenvalues done --- now we need to find the corresponding eigenvectors. Let's start with $\lambda = 2 + i$ --- we haven't had to deal with row reducing a complex-values matrix before, but it takes a little bit of care: in general, we want to avoid dividing by imaginary numbers even more than we want to avoid dividing by real ones.
	
	$$
		\left[ \begin{array}{cc|c} 1 - (2 + i) & 1 & 0 \\ -2 & 3 - (2 + i) & 0 \end{array} \right] &
		
		\left[ \begin{array}{cc|c} -1 - i & 1 & 0 \\ -2 & 1 - i & 0 \end{array} \right] &
		
		\left[ \begin{array}{cc|c} -2 & 1 - i & 0 \\ -1 - i & 1 & 0 \end{array} \right] & \qquad \text{swap } \mathbf{r_1}, \mathbf{r_2}
		
		\left[ \begin{array}{cc|c} -2 & 1 - i & 0 \\ -2 - 2i & 2 & 0 \end{array} \right] & \qquad \mathbf{r_2} \te 2
		
		\left[ \begin{array}{cc|c} -2 & 1 - i & 0 \\ -2i & 1 + i & 0 \end{array} \right] & \qquad \mathbf{r_2} \pe -\mathbf{r_1}
		
		\left[ \begin{array}{cc|c} -2 & 1 - i & 0 \\ 0 & 0 & 0 \end{array} \right] & \qquad \mathbf{r_2} \pe -i\mathbf{r_1}
		
		-2v_1 + (1 - i)v_2 &= 0
		
		v_1 &= \frac{1 - i}{2}v_2.
	$$
	
	With $v_2 = 2$, we have
	
	$$
		\mathbf{v} = \left[ \begin{array}{c} 1 - i \\ 2 \end{array} \right].
	$$
	
	Although row reducing a complex matrix is a little more work, the bright side is that we also know the eigenvector corresponding to $\lambda = 2 - i$: it's the conjugate of the vector $\mathbf{v}$ we found. So the eigenvalues and eigenvectors are
	
	$$
		\lambda = 2 + i: & \quad \left[ \begin{array}{c} 1 - i \\ 2 \end{array} \right]
		
		\lambda = 2 - i: & \quad \left[ \begin{array}{c} 1 + i \\ 2 \end{array} \right].
	$$
	
	However, we won't actually need the other eigenvector --- just like with complex roots of the characteristic equation, all the data we need is contained in the real and imaginary parts of a single eigenvalue and eigenvector. Let's write out the part of the solution that comes from $\lambda = 2 + i$:
	
	$$
		\mathbf{p} &= e^{(2 + i)t} \left[ \begin{array}{c} 1 - i \\ 2 \end{array} \right]
		
		&= e^{2t}(\cos(t) + i\sin(t)) \left( \left[ \begin{array}{c} 1 \\ 2 \end{array} \right] + i\left[ \begin{array}{c} - 1 \\ 0 \end{array} \right] \right).
	$$
	
	The amount of terms can seem a little off-putting, but we actually want to foil this out --- that way, we can completely separate the real and imaginary parts.
	
	$$
		\mathbf{p} &= e^{2t}\cos(t)\left[ \begin{array}{c} 1 \\ 2 \end{array} \right] + ie^{2t}\cos(t)\left[ \begin{array}{c} - 1 \\ 0 \end{array} \right] + ie^{2t}\sin(t)\left[ \begin{array}{c} 1 \\ 2 \end{array} \right] + i^2e^{2t}\sin(t)\left[ \begin{array}{c} - 1 \\ 0 \end{array} \right]
		
		&= e^{2t} \left[ \begin{array}{c} \cos(t) + \sin(t) \\ 2\cos(t) \end{array} \right] + ie^{2t}\left[ \begin{array}{c} \sin(t) - \cos(t) \\ 2\sin(t) \end{array} \right].
	$$
	
	The real and imaginary parts here are fundamental solutions: they're both solutions and linearly independent. Therefore, our general solution is
	
	$$
		\mathbf{p} = c_1 e^{2t} \left[ \begin{array}{c} \cos(t) + \sin(t) \\ 2\cos(t) \end{array} \right] + c_2 e^{2t}\left[ \begin{array}{c} \sin(t) - \cos(t) \\ 2\sin(t) \end{array} \right].
	$$
	
	Now for the rest of the bookkeeping. The initial condition is
	
	$$
		\mathbf{p}(0) &= \left[ \begin{array}{c} 100 \\ 100 \end{array} \right]
		
		c_1 e^{0} \left[ \begin{array}{c} \cos(0) + \sin(0) \\ 2\cos(0) \end{array} \right] + c_2 e^{0}\left[ \begin{array}{c} \sin(0) - \cos(0) \\ 2\sin(0) \end{array} \right] &= \left[ \begin{array}{c} 100 \\ 100 \end{array} \right]
		
		c_1 \left[ \begin{array}{c} 1 \\ 2 \end{array} \right] + c_2 \left[ \begin{array}{c} -1 \\ 0 \end{array} \right] &= \left[ \begin{array}{c} 100 \\ 100 \end{array} \right].
	$$
	
	We can form a matrix and row reduce to solve for $c_1$ and $c_2$, but here it's easier to just solve it manually: the bottom equation reads $2c_1 = 100$, so $c_1 = 50$, and then $c_1 - c_2 = 100$, so $c_2 = -50$. Our particular solution is therefore
	
	$$
		\mathbf{p} &= 50 e^{2t} \left[ \begin{array}{c} \cos(t) + \sin(t) \\ 2\cos(t) \end{array} \right] - 50 e^{2t}\left[ \begin{array}{c} \sin(t) - \cos(t) \\ 2\sin(t) \end{array} \right]
		
		&= 100 e^{2t} \left[ \begin{array}{c} \cos(t) \\ \cos(t) - \sin(t) \end{array} \right].
	$$
	
	This system is exhibiting unbounded exponential growth, so it's probably not the complete story: maybe an ecosystem consisting only of these two species would be mutually beneficial to an extreme degree, but factors like other predators and the carrying capacity of the ecosystem play roles unaccounted for here.
	
	Sketching a direction field, we can see a distinct spiral shape. When a $2 \times 2$ system of DEs has complex conjugate eigenvalues, we see either a spiral into or out from the origin, depending on whether the sign of the real part of the eigenvalues is positive or negative.
	
	### desmos vector-field
	
	### canvas vector-field
	
###

### exc complex eigenvalues
	
	Chemical solutions $A$ and $B$ are mixed together and react. Large amounts of solution $A$ are rapidly converted to small amounts of solution $B$, and both $A$ and $B$ decay into an inert solution at a moderate rate. Specifically, if the masses of solutions $A$ and $B$ are given by $m_1$ and $m_2$ in grams after $t$ seconds, then
	
	$$
		m_1' &= -m_1 - 9m_2
		
		m_2' &= m_1 - m_2.
	$$
	
	At time $t = 0$, $10\,g$ of both solutions are mixed together. How much of each remains after 10 seconds? Sketch a direction field.
	
###



## Repeated Eigenvalues

In individual $n$th-order DEs, repeated roots of the characteristic equation were always a problem: we read off fundamental solutions directly from those roots. When eigenvalues of a matrix are repeated, though, the corresponding eigenvalues don't necessarily repeat. For example, let

$$
	\mathbf{A} = \left[ \begin{array}{cc} 0 & -1 \\ 1 & -2 \end{array} \right].
$$

Solving for the eigenvalues gives us $\lambda^2 + 2\lambda + 1 = 0$, so $\lambda = -1$. When we subtract it from the diagonal, we find

$$
	\left[ \begin{array}{cc|c} 1 & -1 & 0 \\ 1 & -1 & 0 \end{array} \right],
$$

so $v_1 = v_2$. Taking $v_2 = 1$, our single eigenvector is

$$
	\mathbf{v} = \left[ \begin{array}{c} 1 \\ 1 \end{array} \right].
$$

On the other hand,

$$
	B = \left[ \begin{array}{cc} -1 & 0 \\ 0 & -1 \end{array} \right]
$$

has a repeated eigenvalue of $\lambda = -1$, but when we solve for the eigenvectors, we get a matrix of all zeros, so there are two free parameters. When we can choose more than one entry of $\mathbf{v}$, a good strategy is to choose one of them to be one and the rest to be zero, and repeat for all of the entries. Taking $v_1 = 1$ and $v_2 = 0$, then $v_1 = 0$ and $v_2 = 1$, we get

$$
	\mathbf{v}_1 = \left[ \begin{array}{c} 1 \\ 0 \end{array} \right], \qquad \mathbf{v}_2 = \left[ \begin{array}{c} 0 \\ 1 \end{array} \right]. 
$$

So our metaphor of eigenvalues working like roots of the characteristic equation isn't quite one-to-one: repeated eigenvalues aren't always a problem. When there are still $n$ different eigen*vectors*, we don't need to do anything, but when there actually are fewer, we'll need to work to get more solutions. The technical details are similar to reduction of order, but they result in a rather different solution. To get started, we'll need a slight generalization of the notion of an eigenvector. Fittingly enough, these are called



### def generalized eigenvector
	
	Let $\mathbf{A}$ be an $n \times n$ matrix. An eigenvector of $\mathbf{A}$ is a vector $\mathbf{v}$ with $(\mathbf{A} - \lambda\mathbf{I})\mathbf{v} = \mathbf{0}$. In comparison, a **generalized eigenvector** of $\mathbf{A}$ is a vector $\mathbf{w}$ with
	
	$$
		(\mathbf{A} - \lambda\mathbf{I})^k\mathbf{w} = \mathbf{0}
	$$
	
	for some positive integer $k$. Once we've found an eigenvector $\mathbf{v}$, we can find its generalized eigenvectors (if there are any) by solving
	
	$$
		(\mathbf{A} - \lambda\mathbf{I})\mathbf{w} = \mathbf{v}.
	$$
	
	We can find the generalized eigenvectors of $\mathbf{w}$ by solving
	
	$$
		(\mathbf{A} - \lambda\mathbf{I})\mathbf{u} = \mathbf{w},
	$$
	
	and so on. There are always $n$ linearly independent generalized eigenvectors of an $n \times n$ matrix.
	
###



As an opening example, let's return to our previous matrix with a repeated eigenvector:

$$
	\mathbf{A} = \left[ \begin{array}{cc} 0 & -1 \\ 1 & -2 \end{array} \right].
$$

We found that the single eigenvector was

$$
	\mathbf{v} = \left[ \begin{array}{c} 11 \\ 1 \end{array} \right],
$$

and its eigenvalue was $\lambda = -1$. To find its generalized eigenvectors, we solve

$$
	(\mathbf{A} - \lambda\mathbf{I})\mathbf{w} = \mathbf{v},
$$

which means row reducing the matrix

$$
	\left[ \begin{array}{cc|c} 1 & -1 & 1 \\ 1 & -1 & 1 \end{array} \right].
$$

The bottom row reduces to all zeros, so we just have $w_1 - w_2 = 1$. Therefore, $w_1 = w_2 + 1$, so if we take $w_2 = 0$, then our generalized eigenvector is

$$
	\mathbf{w} = \left[ \begin{array}{c} 1 \\ 0 \end{array} \right].
$$

So even though $(\mathbf{A} - (-1)\mathbf{I})\mathbf{w} \neq \mathbf{0}$, it is the case that $(\mathbf{A} - (-1)\mathbf{I})^2\mathbf{w} = \mathbf{0}$. Continuing to try to generalize $\mathbf{w}$ results in no solutions at all, and so we've exhausted the list.

Without a use for them, it doesn't make much sense to find generalized eigenvectors, but you might already see where we're going with this: they're exactly the tool we need to find missing fundamental solutions to a system of DEs.



### thm solving a system with repeated eigenvectors
	
	If the matrix $\mathbf{A}$ has fewer than $n$ different eigenvectors, then the remaining fundamental solutions to $\mathbf{x}' = \mathbf{Ax}$ are of the form
	
	$$
		e^{\lambda t}\mathbf{w},
	$$
	
	where $\mathbf{w}$ is a generalized eigenvector of $\mathbf{A}$.
	
###



So: whenever we have an $n \times n$ matrix with fewer than $n$ eigenvectors, we go through the list of eigenvectors we do have and solve $(\mathbf{A} - \lambda \mathbf{I})\mathbf{w} = \mathbf{v}$ for each $\mathbf{v}$ and $\lambda$. If necessary, we repeat this with all the vectors $\mathbf{w}$ we find, and so on, until we have $n$ vectors total.



### nav-buttons



<script src="/scripts/init.js"></script>