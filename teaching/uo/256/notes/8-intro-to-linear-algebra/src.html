### nav-buttons



So far in the course, we've covered many of the most important techniques for solving DEs. There are a lot more we could discuss, but just like with integration techniques, they become less and less generally applicable the farther we go, both among DEs in general and among those that actually occur in science. Instead, we'll turn to another type of problem that's both solvable and extremely common: systems of DEs. Here's the one-sentence pitch: most physical systems whose rates of change we can express with a differential equation depend not only on their own state and derivatives, but on other systems' states and derivatives too. One of the most famous examples is a **predator-prey system**: if $x(t)$ gives the population of a prey animal at time $t$ and $y(t)$ gives the population of a predator animal, then $x'$ and $y'$ will both depend on $x$ and $y$, since the population growth rate of the prey depends both on its current population and the population of the predators. To write down some concrete equations, let's look at the **Lotka-Volterra model**, which aims to model such a system:

$$
	x' &= ax - bxy
	
	y' &= -cy + dxy,
$$

where $a$, $b$, $c$, and $d$ are constants that describe the particular conditions of the system. Breaking this apart, it makes quite a bit of sense: prey animals (e.g. rabbits) typically grow exponentially in number (ignoring the carrying capacity of the environment itself), so for a variable $a$ determining the growth rate, we'd expect $x' = ax$. That's not the only thing influencing the rabbit population, though --- the more of the predator (e.g. foxes) are present, *and* the more rabbits there are, the more will be eaten: hence the $bxy$ term, which increases with both $x$ and $y$. Foxes' and other predators' populations typically do *not* grow exponentially. When food is scarce or nonexistent, they decay exponentially instead --- and this is reflected by the $-cy$ term. The fox population increases when both it and the rabbit population are high, and this is the point of the $dxy$ term.

The Lotka-Volterra model does a surprisingly great job of modeling real-life predator-prey systems while being simple enough to be solvable. We currently don't have a single technique to help us solve it, but in just a few sections, we'll be able to return to systems like this one and solve them without too much trouble.

The tools we'll need to solve systems of DEs reach far beyond the subject of differential equations, and we'll take this section to get acquainted with many of the key ideas before returning to DEs in the next.



## Vectors and Matrices

Let's return to the world of systems of algebraic equations. A system of two equations and two unknowns, say $x_1$ and $x_2$, might look something like this:

$$
	2x_1 - x_2 &= 3
	
	x_1 + 5x_2 &= -1.
$$

The details that distinguish this system from any other aren't the $x_1$ and $x_2$, but the *coefficients* --- the six numbers specify the exact system we're dealing with. Let's focus on the left sides in particular. By forgetting the $x_i$, we get a $2 \times 2$ grid of numbers:

$$
	\begin{array}{cc} 2 & -1 \\ 1 & 5 \end{array}
$$

The big leap is to treat these four numbers as one large object. To indicate the distinction, we'll wrap it in brackets (parantheses are also commonly used), and to remind ourselves that it's a somewhat more complex variable than a single number, we'll represent it with a capital --- and in printed text, bold --- letter.

$$
	\mathbf{A} = \left[ \begin{array}{cc} 2 & -1 \\ 1 & 5 \end{array} \right].
$$

We call $\mathbf{A}$ a $2 \times 2$ **matrix** --- other sizes, including non-square ones, are also possible, and we'll look at examples momentarily.

In the same way that an ordered pair $(x, y)$ has two degrees of freedom --- both $x$ and $y$ can be changed independently --- $\mathbf{A}$ has four. To reference the individual elements of $\mathbf{A}$, we typically use the corresponding lowercase letter with indices. For example, the elements of $\mathbf{A}$ are $a_{11}$, $a_{12}$, $a_{21}$, and $a_{22}$. The first number indexes the row and the second the column, so

$$
	a_{11} &= 2
	
	a_{12} &= -1
	
	a_{21} &= 1
	
	a_{22} &= 5.
$$

You might find the idea of matrices fascinating in its own right, or you might be waiting for the applications --- either way, we need to actually use these things for something to justify the extra notation and definitions. Getting back to our systems of equations, we want to be able to represent both left sides of the equations at once with some sort of expression involving $\mathbf{A}$. A clue toward the answer lies in the right side of the equation. This time, there aren't any $x_i$ to drop, and we can just assemble the numbers into a matrix directly.

$$
	\mathbf{b} = \left[ \begin{array}{c} 3 \\ -1 \end{array} \right].
$$

When a matrix only has a single column, we call is a **column vector** and denote it with a bold but lowercase letter. In handwriting, we'll typically denote vectors with a little arrow above them, as in $\vec{b}$.

In order for two vectors to be equal, all of their entries must be the same, and this indicates the direction we should go: we can represent the two equations in the system as a single equation involving vectors:

$$
	\left[ \begin{array}{c} 2x_1 - x_2 \\ x_1 + 5x_2 \end{array} \right] = \mathbf{b}.
$$

On the surface, we haven't changed much of anything, but we're actually getting close to something profound. Since $\mathbf{A}$ is providing the coefficients for all the $x_i$, we should be able to just have $\mathbf{A}$ *be a coefficient itself*. To that end, let's define a vector $\mathbf{x}$ that stores both of our variables:

$$
	\mathbf{x} = \left[ \begin{array}{c} x_1 \\ x_2 \end{array} \right].
$$

Now the final part: if $\mathbf{A}$ is going be a single coeffecient, we want to be able to write $\mathbf{Ax}$, multiplying a matrix and a column vector, to mean

$$
	\mathbf{Ax} = \left[ \begin{array}{cc} 2 & -1 \\ 1 & 5 \end{array} \right] \left[ \begin{array}{c} x_1 \\ x_2 \end{array} \right] = \left[ \begin{array}{c} 2x_1 - x_2 \\ x_1 + 5x_2 \end{array} \right].
$$

So, to bring it all together: for matrices to actually represent coefficients in the way we want them to, this is how matrix-vector multiplication should be defined. Let's write this out in a little more generality.



### def $2 \times 2$ matrix-vector multiplication
	
	Let $\mathbf{A}$ and $\mathbf{x}$ be a matrix and vector, respectively, defined as
	
	$$
		\mathbf{A} &= \left[ \begin{array}{cc} a & b \\ c & d \end{array} \right]
		\mathbf{x} &= \left[ \begin{array}{c} x_1 \\ x_2 \end{array} \right].
	$$
	
	Then the product $\mathbf{Ax}$ is a column vector, defined to be
	
	$$
		\mathbf{Ax} = \left[ \begin{array}{cc} a & b \\ c & d \end{array} \right] \left[ \begin{array}{c} x_1 \\ x_2 \end{array} \right] = \left[ \begin{array}{c} ax_1 + bx_2 \\ cx_1 + dx_2 \end{array} \right].
	$$
	
	One way to visualize this is by taking the vector $\mathbf{x}$, rotating it sideways so that it reads $\left[ \begin{array}{cc} x_1 & x_2 \end{array} \right]$, and then for each row of $\mathbf{A}$, multiplying the two together and adding the results.
	
###



As we'll see in the next few examples, matrix-vector multiplication is defined for larger than $2 \times 2$ matrices: for the definition to work, the only restriction is that the length of $\mathbf{x}$ is equal to the row length of $\mathbf{A}$ --- otherwise there'd be a mismatch we turn $\mathbf{x}$ sideways.



### ex matrix-vector multiplication
	
	Evaluate the following products:
	
	1. $$\left[ \begin{array}{cc} 1 & -1 \\ 2 & 3 \end{array} \right] \left[ \begin{array}{c} 3 \\ -2 \end{array} \right].$$
	
	Turning the vector sideways and multiplying through, we have
	
	$$
		\left[ \begin{array}{c} 1(3) + -1(-2) \\ 2(3) + 3(-2) \end{array} \right] = \left[ \begin{array}{c} 5 \\ 0 \end{array} \right].
	$$
	
	2. $$\left[ \begin{array}{ccc} 5 & 0 & 2 \\ 2 & 3 & 0 \end{array} \right] \left[ \begin{array}{c} 1 \\ 0 \\ -2 \end{array} \right].$$
	
	Although the matrix isn't square and larger than $2 \times 2$, the definition still works fine: we just turn the vector sideways and multiply it with every row.
	
	$$
		\left[ \begin{array}{c} 5(1) + 0(0) + 2(-2) \\ 2(1) + 3(0) + 0(-2) \end{array} \right] = \left[ \begin{array}{c} 1 \\ 2 \end{array} \right].
	$$
	
	3. $$\left[ \begin{array}{ccc} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{array} \right] \left[ \begin{array}{c} 5 \\ -4 \\ 1 \end{array} \right].$$
	
	This one works exactly the same way! The resulting vector is
	
	$$
		\left[ \begin{array}{c} 1(5) + 0(-4) + 0(1) \\ 0(5) + 1(-4) + 0(1) \\ 0(5) + 0(-4) + 1(1) \end{array} \right] = \left[ \begin{array}{c} 5 \\ -4 \\ 1 \end{array} \right].
	$$
	
###

### exc matrix-vector multiplication
	
	Evaluate the following products:
	
	1. $$\left[ \begin{array}{cc} 1 & 0 \\ 0 & 1 \end{array} \right] \left[ \begin{array}{c} 5 \\ 7 \end{array} \right].$$
	
	2. $$\left[ \begin{array}{cc} 1 & 1 \\ 2 & 1 \\ -1 & 1 \end{array} \right] \left[ \begin{array}{c} -2 \\ 5 \end{array} \right].$$
	
	3. $$\left[ \begin{array}{cc} 1 & 1 \\ 2 & 1 \\ -1 & 1 \end{array} \right] \left[ \begin{array}{c} -3 \\ 6 \\ 1 \end{array} \right].$$
	
###



From these examples, we can see some general properties of matrix-vector multiplication. If $\mathbf{A}$ is an $n \times k$ matrix, meaning it has $n$ rows and $k$ columns, then $\mathbf{Ax}$ is defined only if $\mathbf{x}$ has length $k$, and the length of $\mathbf{Ax}$ is $n$.

A square matrix with all $0$s with a strip of $1$s down the diagonal from top-left to bottom-right, like those in part 3 of the example or part 1 of the exercise, seem to multiply vectors by not changing them, and it's not too hard to see that the same will happen with every vector: the $i$th row pulls out exactly the $i$th component. This matrix is useful enough that it deserves its own name.



### def identity matrix
	
	The $n \times n$ **identity matrix** consists of all $0$s, except for $1$s down the top-left to bottom-right diagonal (called the **main diagonal**). We write it as $\mathbf{I}_n$, or just $\mathbf{I}$ if the dimension is clear from context.
	
###



Now that we're used to the basic idea of matrices, let's look at some of their basic properties. The guiding principle to keep in mind is that **matrices are functions** that operate by multiplying: in other words, every matrix $\mathbf{A}$ corresponds to a linear function $T$ so that $\mathbf{Ax} = T(\mathbf{x})$. To that end, we want to define matrix addition so that it acts just like function addition. In other words, matrices $\mathbf{A}$ and $\mathbf{B}$ should have a sum so that

$$
	(\mathbf{A} + \mathbf{B})\mathbf{x} = \mathbf{Ax} + \mathbf{Bx},
$$

just like functions. Therefore, we need $\mathbf{A}$ and $\mathbf{B}$ to have the same number of columns so they can both multiply $\mathbf{x}$, and the same number of rows so that they can add together after multiplying. For vector multiplication to distribute like this, we just need the $i$th row of $\mathbf{A} + \mathbf{B}$ to be the $i$th row of $\mathbf{A}$ plus the $i$th row of $\mathbf{B}$. All of this is to say that matrix addition works exactly like we'd hope: as long as they're exactly the same shape, we can just add each component together. For example,

$$
	\left[ \begin{array}{ccc} 5 & 0 & 2 \\ 2 & 3 & 0 \end{array} \right] + \left[ \begin{array}{ccc} -1 & 2 & 0 \\ -5 & 0 & 2 \end{array} \right] &= \left[ \begin{array}{ccc} 5 - 1 & 0 + 2 & 2 + 0 \\ 2 - 5 & 3 + 0 & 0 + 2 \end{array} \right]
	
	&= \left[ \begin{array}{ccc} 5 & 2 & 2 \\ -3 & 3 & 2 \end{array} \right].
$$



### nav-buttons



<script src="/scripts/init.js"></script>