<!DOCTYPE html><html lang="en"><head><title>Section 10: Eigenthings</title><meta property="og:title" content="Section 10: Eigenthings"/><meta property="og:type" content="website"/><meta property="og:url" content="https://cruzgodar.com/teaching/uo/256/notes/10-eigenthings/"/><meta property="og:image" content="https://cruzgodar.com/teaching/uo/256/notes/10-eigenthings/cover.webp"/><meta property="og:locale" content="en_US"/><meta property="og:site_name" content="Cruz Godar"/><style>body {opacity: 0;}</style></head><body><noscript><p class="body-text" style="text-align: center">JavaScript is required to use this site and many others. Consider enabling it.</p></noscript><header><div id="logo"><a href="/home/" tabindex="-1"><img src="/graphics/general-icons/logo.png" alt="Logo" tabindex="1"></img></a></div><div style="height: 20px"></div><h1 class="heading-text">Section 10: Eigenthings</h1></header><main><section><div class="text-buttons nav-buttons"><div class="focus-on-child tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div><p class="body-text">When a matrix failts to be invertible, it&#x2019;s because at some point in the row reduction process, a row became completely zero. All that row reduction can do is turn rows into sums of multiples of all the rows. Specifically, if some row becomes all zero, then</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]c_1\mathbf{r_1} + c_2\mathbf{r_2} + \cdots + c_n\mathbf{r_n} = \mathbf{0}[NEWLINE]$$">$$\begin{align*}c_1\mathbf{r_1} + c_2\mathbf{r_2} + \cdots + c_n\mathbf{r_n} = \mathbf{0}\end{align*}$$</span></p><p class="body-text">for some constants <span class="tex-holder inline-math" data-source-tex="c_i">$c_i$</span>, where <span class="tex-holder inline-math" data-source-tex="\mathbf{0}">$\mathbf{0}$</span> is the row vector of all zeros. Some of the <span class="tex-holder inline-math" data-source-tex="c_i">$c_i$</span> may be zero and some negative, but the point is that this is a much more concise description of when a matrix is invertible. In fact, this is such an important concept in linear algebra that it deserves its own name.</p><div class="notes-def notes-environment"><p class="body-text"</p><span class="notes-def-title">Definition: linear independence</span></p><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="\mathbf{v_1}">$\mathbf{v_1}$</span>, <span class="tex-holder inline-math" data-source-tex="\mathbf{v_2}">$\mathbf{v_2}$</span>, ..., <span class="tex-holder inline-math" data-source-tex="\mathbf{v_k}">$\mathbf{v_k}$</span> be <span class="tex-holder inline-math" data-source-tex="n">$n$</span>-dimensional vectors. The <span class="tex-holder inline-math" data-source-tex="\mathbf{v_i}">$\mathbf{v_i}$</span> are <strong>linearly dependent</strong> if</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]c_1\mathbf{v_1} + c_2\mathbf{v_2} + \cdots + c_k\mathbf{v_k} = \mathbf{0}[NEWLINE]$$">$$\begin{align*}c_1\mathbf{v_1} + c_2\mathbf{v_2} + \cdots + c_k\mathbf{v_k} = \mathbf{0}\end{align*}$$</span></p><p class="body-text">for some choice of <span class="tex-holder inline-math" data-source-tex="c_1, c_2, ..., c_k">$c_1, c_2, ..., c_k$</span>, and <strong>linearly independent</strong> if no such <span class="tex-holder inline-math" data-source-tex="c_i">$c_i$</span> exist.</p></div><p class="body-text">For example, <span class="tex-holder inline-math" data-source-tex="\mathbf{v_1} = \left[ \begin{array}{ccc} 2 & -5 & 1 \end{array} \right]">$\mathbf{v_1} = \left[ \begin{array}{ccc} 2 & -5 & 1 \end{array} \right]$</span> and <span class="tex-holder inline-math" data-source-tex="\mathbf{v_2} = \left[ \begin{array}{ccc} -4 & 10 & -2 \end{array} \right]">$\mathbf{v_2} = \left[ \begin{array}{ccc} -4 & 10 & -2 \end{array} \right]$</span> are linearly dependent, since <span class="tex-holder inline-math" data-source-tex="-2\mathbf{v_1} + \mathbf{v_2} = \mathbf{0}">$-2\mathbf{v_1} + \mathbf{v_2} = \mathbf{0}$</span>. To determine if a more complicated set of vectors is linearly dependent, just throw them into a matrix as row vectors and tr to row reduce the matrix. If a row becomes the zero vector, they&#x2019;re linearly dependent, and if not, they&#x2019;re linearly independent.</p><p class="body-text">If we&#x2019;re looking at <span class="tex-holder inline-math" data-source-tex="n">$n$</span>-dimensional vectors, any collection of more than <span class="tex-holder inline-math" data-source-tex="n">$n$</span> of them is <em>guaranteed</em> to be linearly dependent. That&#x2019;s because putting the into a matrix results in one that is taller than it is wide, and so even if the first <span class="tex-holder inline-math" data-source-tex="n">$n$</span> rows are linearly independent, meaning they reduce to the identity matrix, the rows after it will reduce to zero: once we have the identity matrix, we can destroy any other row, meaning the matrix has the form</p><p class="body-text" style="text-align: center"><span class="tex-holder" style="padding: 8px" data-source-tex="$$[NEWLINE][TAB]\left[ \begin{array}{c} \mathbf{I} \\ \hline \mathbf{0} \end{array} \right].[NEWLINE]$$">$$\begin{align*}\left[ \begin{array}{c} \mathbf{I} \\ \hline \mathbf{0} \end{array} \right].\end{align*}$$</span></p><p class="body-text">Since linear dependence of the rows is the only problem that can arise when row reducing, we have a clean result relating it to matrix inversion.</p><div class="notes-prop notes-environment"><p class="body-text"</p><span class="notes-prop-title">Proposition: linear independence and matrix inversion</span></p><p class="body-text">Let <span class="tex-holder inline-math" data-source-tex="\mathbf{A}">$\mathbf{A}$</span> be an <span class="tex-holder inline-math" data-source-tex="n \times n">$n \times n$</span> matrix. Then <span class="tex-holder inline-math" data-source-tex="\mathbf{A}">$\mathbf{A}$</span> is invertible if and only if the rows of <span class="tex-holder inline-math" data-source-tex="\mathbf{A}">$\mathbf{A}$</span> are linearly independent.</p></div><div class="text-buttons nav-buttons"><div class="focus-on-child tabindex="1"><button class="text-button linked-text-button previous-nav-button" type="button" tabindex="-1">Previous</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button home-nav-button" type="button" tabindex="-1">Home</button></div><div class="focus-on-child" tabindex="1"><button class="text-button linked-text-button next-nav-button" type="button" tabindex="-1">Next</button></div></div></section></main><script src="/scripts/init.min.js"></script></body></html>