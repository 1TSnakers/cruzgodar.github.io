### nav-buttons



Welcome to Math 341! You're viewing the interactive lecture notes --- reading these is required for the class, since we'll have a short reading quiz on Canvas due before most lectures. To get started, let's make sure your browser handles equations and graphs correctly. You should see an equation on its own line below and a graph below that.

$$
	x = \dfrac{-b \pm \sqrt{b^2 - 4ac}}{2a}
$$

### desmos test-graph

If anything doesn't appear correctly, update your browser and then reach out if that doesn't work.

It's hard to overstate just how pervasive linear algebra is in science. It underpins nearly every math course that comes after it in one way or another, and it's so important in other STEM fields that when they talk about math, they often just mean linear algebra. With such an impressive resume, it might be surprising to hear that linear algebra is no more than a formalized, organized way to solve systems of linear equations --- at least, that's how it gets started. As we'll quickly see, formalizing that process will result in an unreasonably useful set of tools and definitions with applications far beyond what it was designed for.

Unlike most math classes that have come before, this one is a clean break from its prerequisites: there isn't much to review that will be useful. Instead, we'll be building our theory of linear algebra more or less from scratch --- to that end, let's take a look at our central object of study: systems of linear equations. *Linear* means that variables aren't multiplied or raised to powers, just added together, so when there are only two variables, the graph is a line.

### ex a system of linear equations
	
	Solve the system
	
	$$
		x_1 + 3x_2 &= -5
		
		2x_1 - 3x_2 &= 8.
	$$
	
	We could use substitution by solving the first equation for $x_1$ and substituting it into the second, but let's think critically about that. Substitution doesn't scale well --- if we had $n$ equations and $n$ unknowns, solving by substitution means isolating and plugging in $n - 1$ equations until we finally reach the end, and then we have to do a large amount of back-substitution. That's not to mention the complexity that arises when not every equation contains every variable, and we have to carefully pick and choose our order. We're trying to build a general solving method here, and so we owe it to ourselves to make a better foundation.
	
	It may have been a while since you learned about solving systems of linear equations, but you probably learned another solving method, different from substitution. It's usually called the method of **addition**, and critically for us, it's much more scalable. We begin by looking at the top-left term in the system (i.e. $x_1$) and adding copies of the entire first equation down to the others to clear their $x_1$ terms. Here, that means adding $-2$ times the first equation down to the second, resulting in
	
	$$
		x_1 + 3x_2 &= -5
		
		- 7x_2 &= 14.
	$$
	
	At this point, we can divide the second equation through by $-7$ to find that $x_2 = -2$:
	
	$$
		x_1 + 3x_2 &= -5
		
		x_2 &= -2.
	$$

	Instead of substituting that into the first equation, we'll add $-3$ times the entire second equation up to the first.
	
	$$
		x_1 &= 1
		
		x_2 &= -2.
	$$
	
	So --- why does this supposedly scale better? The key is in the organization. The variables stayed on the left and the constants on the right, and we only had to look at two coefficients at a time to figure out how many of one equation to add to another. As the number of variables and equations increases, the number of operations required to do both substitution and addition increases, but the *complexity* of those operations doesn't increase with addition.
	
###

### exc a system of linear equations
	
	Solve the system
	
	$$
		2x_1 - x_2 &= -6
		
		-4x_1 - 3x_2 &= 12.
	$$
	
###

Now that we've worked through an example, we're prepared to define linear systems in general.

### def linear system

	A general linear equation in the variables $x_1, x_2, ..., x_n$ has the form
	
	$$
		a_1x_1 + a_2x_2 + \cdots + a_nx_n = b
	$$
	
	for some constants $a_i$ and $b$. A **linear system** is a collection of linear equations. The coefficients usually pick up an extra subscript:
	
	$$
		a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &= b_1
		
		a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n &= b_2
		
		& \ \ \vdots
		
		a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n &= b_m.
	$$
	
###

Much of the intuition we have for small linear systems helps us understand the behavior of large ones. Depending on the particular equations, a system can have a single solution, no solutions, or infinitely many. Geometrically, linear equations form straight lines (or planes in higher dimensions), and the points where they all intersect are the solutions. In the example below, setting $a = 1$ results in the two lines being parallel, so there are no intersection points and therefore no solutions. When $a = 0$, the lines are the same (we sometimes say they **coincide**), and so there are infinitely many solutions. For all other values of $a$, the lines intersect at exactly one point, meaning there is a unique solution.

### desmos linear-system

We'll develop vocabulary to describe all of these situations shortly, but the most pressing issue is that it's unwieldy to deal with $m$ different equations at once. If we could consolidate all $m$ of them into a single equation that still contained all the data, it would make the discussion of linear systems quite a bit simpler.

Let's start with the right sides --- the $b_i$. There are quite a few ways we could try to combine them all into one single object, but let's just follow our nose and try the simplest thing we can. When we write the equations in a row like we did above, the $b_i$ form a column. To that end, let's create an object that contains all of the $b_i$ at once.

### def column vector

	A **column vector** is a vertical rectangle of numbers of the form

	$$
		\left[ \begin{array}{c} b_1 \\ b_2 \\ \cdots \\ b_m \end{array} \right].
	$$

	We say that a column vector has length $m$, or that its dimensions are $m \times 1$ (since there's only one column).

###

Just like ordered pairs, two column vectors are equal only if they're the same length and all of their entries are equal. We can leverage this by assembling all of the equations into vectors. Going back to our original system,

$$
	x_1 + 3x_2 &= -5
	
	2x_1 - 3x_2 &= 8.
$$

becomes

$$
	\left[ \begin{array}{c} x_1 + 3x_2 \\ 2x_1 - 3x_2 \end{array} \right] = \left[ \begin{array}{c} -5 \\ 8 \end{array} \right].
$$

This is a good step, but it's not particularly useful yet: all we've done is drawn a few more symbols. To really cut through the noise and get to a good generalization, we need to go one step further. In a linear system with two unknowns, there will always be a single copy of $x_1$ and $x_2$ in each equation. What changes is the coefficients --- the only part of the left side that matters are the four numbers $1$, $3$, $2$, and $-3$. As before, let's try the simplest possible organization. When we write out the equations, they form a square, so let's use vector notation and put them into a $2 \times 2$ rectangle:

$$
	\left[ \begin{array}{cc} 1 & 3 \\ 2 & -3 \end{array} \right].
$$

We call this a $2 \times 2$ **matrix**. All we need is a way to use this matrix as a coefficient on both $x_1$ and $x_2$ at the same time --- that sounds like a column vector, which means we want

$$
	\left[ \begin{array}{cc} 1 & 3 \\ 2 & -3 \end{array} \right] \left[ \begin{array}{c} x_1 \\ x_2 \end{array} \right] = \left[ \begin{array}{c} x_1 + 3x_2 \\ 2x_1 - 3x_2 \end{array} \right].
$$

There are two valuable lessons to learn here that are much wider-reaching than linear systems: first, consolidating and abstracting a concept often leads to a much more powerful theory in the end, even if it's often more difficult to work with at first. Second, it's almost always worth trying the simplest idea first in math --- quite often, everything will fall in place as if it was meant to be, and it will feel as though you're discovering the theory instead of inventing it.



### nav-buttons