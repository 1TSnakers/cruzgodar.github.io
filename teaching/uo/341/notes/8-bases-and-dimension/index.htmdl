### nav-buttons



So far, generalizing all of our results to different vector spaces hasn't been too terrible. The objects in our new spaces differ quite a bit from those in $#R#^n$ --- the only vector space we studied in the first six sections --- but the tools we've used to study them are nearly identical. Vectors can be added and multiplied by scalars, and linear transformations are defined in exactly the same way. However, so much of what we've learned relies on *matrices*, and at the moment, we just don't have those. To recap, the reason we can express a linear transformation $T : #R#^n \to #R#^m$ as multiplication by an $m \times n$ matrix $A$ is because any vector $\vec{v} \in #R#^n$ can be expressed as a linear combination of the $\vec{e_i}$:

$$
	\vec{v} = [[ c_1 ; c_2 ; \vdots ; c_n ]] = c_1\vec{e_1} + c_2\vec{e_2} + \cdots + c_n\vec{e_n}.
$$

Then applying $T$ to both sides results in

$$
	T(\vec{v}) = T\left( [[ c_1 ; c_2 ; \vdots ; c_n ]]\right) = c_1T(\vec{e_1}) + c_2T(\vec{e_2}) + \cdots + c_nT(\vec{e_n}),
$$

and that linear combination can be expressed as the matrix-vector product

$$
	T(\vec{v}) = [[ \mid, \mid, , \mid ; T(\vec{e_1}), T(\vec{e_2}), \cdots, T(\vec{e_n}) ; \mid, \mid, , \mid ]][[ c_1 ; c_2 ; \vdots ; c_n ]].
$$

So how can we bring this idea to a general vector space $V$? If we could find a collection of fundamental vectors like $\vec{e_1}, ..., \vec{e_n}$ so that we could express any $\vec{v} \in V$ as a linear combination of them --- ideally in a unique way --- then we'd be able to reap the same benefits of matrices: by computing a linear transformation's effect on just those fundamental vectors, we'd then be able to find its effect on any vector at all.

What conditions will we need to make that a reality? If we want a collection of vectors $\vec{v_1}, \vec{v_2}, ..., \vec{v_n} \in V$ so that any $\vec{v} \in V$ is expressible as a unique linear combination of the $\vec{v_i}$, then an immediate first requirement is that the $\vec{v_i}$ need to span $V$ --- otherwise, some vectors wouldn't be expressible as a linear combination of them at all, let alone a unique one. To guarantee the uniqueness condition, a rather technical argument can show that we need the $\vec{v_i}$ to be linearly independent (it's very similar to the one that shows a linear transformation from $#R#^n$ to $#R#^m$ is one-to-one if it sends only $\vec{0}$ to $\vec{0}$). When we say linearly independent and spanning, we mean exactly the same thing as we did in $#R#^n$, but let's take the time to write down the formal definitions in a general vector space $V$, and then name the collection of vectors whose linear combinations express all others uniquely.

### def linear independence and span

	Let $V$ be a vector space. A collection of vectors $\vec{v_1}, \vec{v_2}, ..., \vec{v_n} \in V$ is ** linearly independent** if the only linear combination

	$$
		c_1\vec{v_1} + c_2\vec{v_2} + \cdots + c_n\vec{v_n} = \vec{0}
	$$

	is when $c_1 = c_2 = \cdots = c_n = 0$. The **span** of the $\vec{v_i}$ is the set of all their possible linear combinations, and we say the collection **spans** $V$ if

	$$
		\span \{ \vec{v_1}, \vec{v_2}, ..., \vec{v_n} \} = V.
	$$

###

### def basis

	Let $V$ be a vector space. A **basis** for $V$ is a collection of vectors $\vec{v_1}, \vec{v_2}, ..., \vec{v_n} \in V$ that is linearly independent and spans $V$. Given a basis and any vector $\vec{v} \in V$, there is a **unique** linear combination

	$$
		c_1\vec{v_1} + c_2\vec{v_2} + \cdots + c_n\vec{v_n} = \vec{v}.
	$$

###

Just like in $#R#^n$, if a collection of vectors $\vec{v_i}$ spans a vector space $V$ but one of the $\vec{v_i}$ is a linear combination of the others, then we can remove that one without affecting the fact that the collection spans $V$. In this sense, a basis is just the right size: add any new vector to it and it'll stop being linearly independent, and remove any and it won't span $V$.

### ex a basis for $#R#^n$

	The primordial example of a basis is the vectors

	$$
		\vec{e_1} = [[ 1 ; 0 ; 0 ; \vdots ; 0 ]], \vec{e_2} = [[ 0 ; 1 ; 0 ; \vdots ; 0 ]], ..., \vec{e_n} = [[ 0 ; 0 ; \vdots ; 0 ; 1 ]] \in #R#^n.
	$$

	The $\vec{e_i}$ are linearly independent since they're already the rows of the identity matrix, and they certainly span $#R#^n$: given a vector $\vec{v} \in #R#^n$, we can immediately express $\vec{v}$ as a linear combination of the $\vec{e_i}$ (like we did at the start of the section).

###

This basis is so common that we'll want a name for it --- it's called the **standard basis for $#R#^n$**. The name implies it's not the only one, though, and that's definitely the case. Let's take a look at some more unusual ones.

### ex another basis for $#R#^n$

	Show that the vectors

	$$
		\vec{v_1} = [[ 3 ; 1 ; 2 ]], \qquad \vec{v_2} = [[ 1 ; 0 ; 1 ]], \qquad \vec{v_3} = [[ 0 ; 2 ; 1 ]]
	$$

	form a basis for $#R#^3$ and express the vector

	$$
		\vec{v} = [[ 4 ; 1 ; 0 ]]
	$$

	in this basis.
	
	While this is nominally a new kind of task, we've been showing that vectors are linearly independent and spanning for a few sections now: showing that a linear transformation is one-to-one and onto amounts to showing that its columns are linearly independent and span the transformation's codomain. We do that by transposing the matrix and row reducing without swapping rows, so now we might as well jump straight to that step now by placing our vectors as rows in a matrix. We have
	
	$$
		[[ 3, 1, 2 ; 1, 0, 1 ; 0, 2, 1 ]] &

		[[ 0, 1, -1 ; 1, 0, 1 ; 0, 2, 1 ]] & \qquad \vec{r_1} \me 3\vec{r_2}

		[[ 0, 1, -1 ; 1, 0, 1 ; 0, 0, 3 ]] & \qquad \vec{r_3} \me 2\vec{r_1}

		[[ 0, 1, -1 ; 1, 0, 1 ; 0, 0, 1 ]] & \qquad \vec{r_3} \te \frac{1}{3}

		[[ 0, 1, 0 ; 1, 0, 1 ; 0, 0, 1 ]] & \qquad :: \vec{r_2} \me \vec{r_3} ; \vec{r_1} \pe \vec{r_3} ::,
	$$

	and so all of the vectors are linearly independent and therefore span $#R#^3$.

	To express $\vec{v}$ in the basis, we're just trying to find a linear combination of the basis vectors that equals $\vec{v}$. We've already solved that type of problem: it's just row reduction once again, this time with the $\vec{v_i}$ as columns.

	$$
		[[ 3, 1, 0 | 4 ; 1, 0, 2 | 1 ; 2, 1, 1 | 0 ]] & 

		[[ 1, 0, 2 | 1 ; 3, 1, 0 | 4 ; 2, 1, 1 | 0 ]] & \qquad \swap \vec{r_1}, \vec{r_2}

		[[ 1, 0, 2 | 1 ; 0, 1, -4 | 1 ; 0, 1, -3 | -2 ]] & \qquad :: \vec{r_2} \me 3\vec{r_1} ; \vec{r_3} \me 2\vec{r_1} ::

		[[ 1, 0, 2 | 1 ; 0, 1, -4 | 1 ; 0, 0, 1 | -3 ]] & \qquad \vec{r_3} \me \vec{r_2}

		[[ 1, 0, 2 | 1 ; 0, 1, 0 | -11 ; 0, 0, 1 | -3 ]] & \qquad \vec{r_2} \pe 4\vec{r_3}

		[[ 1, 0, 0 | 7 ; 0, 1, 0 | -11 ; 0, 0, 1 | -3 ]] & \qquad \vec{r_1} \me 2\vec{r_3}.
	$$

	In total, our linear combination is $\vec{v} = 7\vec{v_1} - 11\vec{v_2} - 3\vec{v_3}$. Geometrically, we can think of this expansion as meaning that the coordinates of $\vec{v}$ are $(7, -11, -3)$ in a strange coordinate system where the axes are parallel to $\vec{v_1}$, $\vec{v_2}$, and $\vec{v_3}$.

###

Let's dig into that coordinate system idea a little more. In $#R#^2$, the standard basis produces the typical coordinate system we're used to, while other bases produce coordinate systems that are stretched and scaled, but still perfectly functional at assigning a unique coordinate pair to every point. 

### desmos coordinate-systems

Here, the purple grid shows the coordinates assigned by the standard basis for $#R#^2$, while the blue one shows the coordinates assigned by the basis $\left\{ [[ -1 ; 1 ]], [[ 1 ; -2 ]] \right\}$. The red point is called $(-2, 1)$ in the standard basis, but $(3, 1)$ in this alternate one.

### exc another basis for $#R#^n$
	
	Show that the vectors
	
	$$
		\vec{v_1} = [[ 1 ; 2 ]], \qquad \vec{v_2} = [[ 0 ; -1 ]]
	$$
	
	form a basis for $#R#^2$, and express the vector
	
	$$
		\vec{v} = [[ 3 ; 8 ]]
	$$

	in this basis.
	
###

Much like the key idea that matrix multiplication is function composition, let's state another critical notion that we'll return to throughout this section and future ones. The only reason $#R#^n$ seems easier to work with than all of these new vector spaces is **because it came equipped with a convenient basis** --- the standard one. If we want to bring the convenience of matrices and vectors to spaces that don't support them natrually --- like $#R#[x]$ --- then we need to choose our own basis.



### nav-buttons