### nav-buttons



So far, generalizing all of our results to different vector spaces hasn't been too terrible. The objects in our new spaces differ quite a bit from those in $#R#^n$ --- the only vector space we studied in the first six sections --- but the tools we've used to study them are nearly identical. Vectors can be added and multiplied by scalars, and linear transformations are defined in exactly the same way. However, so much of what we've learned relies on *matrices*, and at the moment, we just don't have those. To recap, the reason we can express a linear transformation $T : #R#^n \to #R#^m$ as multiplication by an $m \times n$ matrix $A$ is because any vector $\vec{v} \in #R#^n$ can be expressed as a linear combination of the $\vec{e_i}$:

$$
	\vec{v} = [[ c_1 ; c_2 ; \vdots ; c_n ]] = c_1\vec{e_1} + c_2\vec{e_2} + \cdots + c_n\vec{e_n}.
$$

Then applying $T$ to both sides results in

$$
	T(\vec{v}) = T\left( [[ c_1 ; c_2 ; \vdots ; c_n ]]\right) = c_1T(\vec{e_1}) + c_2T(\vec{e_2}) + \cdots + c_nT(\vec{e_n}),
$$

and that linear combination can be expressed as the matrix-vector product

$$
	T(\vec{v}) = [[ \mid, \mid, , \mid ; T(\vec{e_1}), T(\vec{e_2}), \cdots, T(\vec{e_n}) ; \mid, \mid, , \mid ]][[ c_1 ; c_2 ; \vdots ; c_n ]].
$$

So how can we bring this idea to a general vector space $V$? If we could find a collection of fundamental vectors like $\vec{e_1}, ..., \vec{e_n}$ so that we could express any $\vec{v} \in V$ as a linear combination of them --- ideally in a unique way --- then we'd be able to reap the same benefits of matrices: by computing a linear transformation's effect on just those fundamental vectors, we'd then be able to find its effect on any vector at all.

What conditions will we need to make that a reality? If we want a collection of vectors $\vec{v_1}, \vec{v_2}, ..., \vec{v_n} \in V$ so that any $\vec{v} \in V$ is expressible as a unique linear combination of the $\vec{v_i}$, then an immediate first requirement is that the $\vec{v_i}$ need to span $V$ --- otherwise, some vectors wouldn't be expressible as a linear combination of them at all, let alone a unique one. To guarantee the uniqueness condition, a rather technical argument can show that we need the $\vec{v_i}$ to be linearly independent (it's very similar to the one that shows a linear transformation from $#R#^n$ to $#R#^m$ is one-to-one if it sends only $\vec{0}$ to $\vec{0}$). When we say linearly independent and spanning, we mean exactly the same thing as we did in $#R#^n$, but let's take the time to write down the formal definitions in a general vector space $V$, and then name the collection of vectors whose linear combinations express all others uniquely.

### def linear independence and span

	Let $V$ be a vector space. A collection of vectors $\vec{v_1}, \vec{v_2}, ..., \vec{v_n} \in V$ is ** linearly independent** if the only linear combination

	$$
		c_1\vec{v_1} + c_2\vec{v_2} + \cdots + c_n\vec{v_n} = \vec{0}
	$$

	is when $c_1 = c_2 = \cdots = c_n = 0$. The **span** of the $\vec{v_i}$ is the set of all their possible linear combinations, and we say the collection **spans** $V$ if

	$$
		\span \{ \vec{v_1}, \vec{v_2}, ..., \vec{v_n} \} = V.
	$$

###

### def basis

	Let $V$ be a vector space. A **basis** for $V$ is a collection of vectors $\vec{v_1}, \vec{v_2}, ..., \vec{v_n} \in V$ that is linearly independent and spans $V$. Given a basis and any vector $\vec{v} \in V$, there is a **unique** linear combination

	$$
		c_1\vec{v_1} + c_2\vec{v_2} + \cdots + c_n\vec{v_n} = \vec{v}.
	$$

###

Just like in $#R#^n$, if a collection of vectors $\vec{v_i}$ spans a vector space $V$ but one of the $\vec{v_i}$ is a linear combination of the others, then we can remove that one without affecting the fact that the collection spans $V$. In this sense, a basis is just the right size: add any new vector to it and it'll stop being linearly independent, and remove any and it won't span $V$.

### ex a basis for $#R#^n$

	The primordial example of a basis is the vectors

	$$
		\vec{e_1} = [[ 1 ; 0 ; 0 ; \vdots ; 0 ]], \vec{e_2} = [[ 0 ; 1 ; 0 ; \vdots ; 0 ]], ..., \vec{e_n} = [[ 0 ; 0 ; \vdots ; 0 ; 1 ]] \in #R#^n.
	$$

	The $\vec{e_i}$ are linearly independent since they're already the rows of the identity matrix, and they certainly span $#R#^n$: given a vector $\vec{v} \in #R#^n$, we can immediately express $\vec{v}$ as a linear combination of the $\vec{e_i}$ (like we did at the start of the section).

###

This basis is so common that we'll want a name for it --- it's called the **standard basis for $#R#^n$**. The name implies it's not the only one, though, and that's definitely the case. Let's take a look at some more unusual ones.

### ex more bases for $#R#^n$

	Show that the vectors

	$$
		\vec{v_1} = [[ 3 ; 1 ; 2 ]], \qquad \vec{v_2} = [[ 1 ; 0 ; 1 ]], \qquad \vec{v_3} = [[ 0 ; 2 ; 1 ]]
	$$

	form a basis for $#R#^3$ and express the vector

	$$
		\vec{v} = [[ 4 ; 1 ; 0 ]]
	$$

	in this basis.

###

Much like the key idea that matrix multiplication is function composition, let's state another critical notion that we'll return to throughout this section and future ones. 



### nav-buttons